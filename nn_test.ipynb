{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "30acb086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, ConcatDataset\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "7b68a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data_standardized(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks_standardized = data['dom_landmarks_standardized']\n",
    "    non_dom_landmarks_standardized = data['non_dom_landmarks_standardized']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores_standardized = data['blendshape_scores_standardized']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist_standardized = data['nose_to_wrist_dist_standardized']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    dom_velocity_small_standardized = data['dom_velocity_small_standardized']\n",
    "    dom_velocity_large_standardized = data['dom_velocity_large_standardized']\n",
    "    non_dom_velocity_small_standardized = data['non_dom_velocity_small_standardized']\n",
    "    non_dom_velocity_large_standardized = data['non_dom_velocity_large_standardized']\n",
    "    velocity_confidence = data['velocity_confidence']\n",
    "    velocity_calculation_confidence = data['velocity_calculation_confidence']\n",
    "    nose_to_wrist_velocity_small_standardized = data['wrist_velocity_small_standardized']\n",
    "    nose_to_wrist_velocity_large_standardized = data['wrist_velocity_large_standardized']\n",
    "    \n",
    "    return (dom_landmarks_standardized, non_dom_landmarks_standardized, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores_standardized, face_detected, \n",
    "            nose_to_wrist_dist_standardized, frame_idx, timestamp_ms, dom_velocity_small_standardized, dom_velocity_large_standardized, non_dom_velocity_small_standardized, non_dom_velocity_large_standardized, velocity_confidence, velocity_calculation_confidence, nose_to_wrist_velocity_small_standardized, nose_to_wrist_velocity_large_standardized)\n",
    "\n",
    "def sorted_npz_files_checked_label(directory_path):\n",
    "    if os.path.exists(directory_path) and os.path.isdir(directory_path):\n",
    "        # List all NPZ files in the directory\n",
    "        npz_files = sorted(glob.glob(os.path.join(directory_path, \"*.npz\")))\n",
    "    else:\n",
    "        print(f\"Directory path {directory_path} doesn't exist or it isn't a directory\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    \n",
    "    # Skip if no files found\n",
    "    if not npz_files:\n",
    "        print(f\"No NPZ files found in {directory_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    \n",
    "    with open(os.path.join(directory_path, 'detection_statistics.json')) as f:\n",
    "        statistics_file = json.load(f)\n",
    "    \n",
    "    if statistics_file['video_info']['total_frames'] != (len(npz_files)-1):\n",
    "        print(\"npz filepath list contain different amount of items than total frames\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    frame_to_file = {}\n",
    "    for file_path in npz_files:\n",
    "        if os.path.basename(file_path) == 'smooth_labels.npz':\n",
    "            label_path = file_path\n",
    "            continue\n",
    "        try:\n",
    "            frame_data = load_frame_data_standardized(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading frame with path: {file_path}: {e}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        frame_idx = frame_data[8]  # Index for frame_idx\n",
    "        frame_to_file[frame_idx] = file_path\n",
    "\n",
    "    \n",
    "    frame_indices = sorted(frame_to_file.keys())\n",
    "    if not all(frame_indices[i+1] - frame_indices[i] == 1 for i in range(len(frame_indices) - 1)):\n",
    "        print(\"Consecutive frames are not different by one frame\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    \n",
    "\n",
    "    return frame_to_file, frame_indices, label_path\n",
    "\n",
    "def load_label(label_path):\n",
    "    label_data = np.load(label_path)\n",
    "    L_index = label_data['L_index']\n",
    "    L_values = label_data['L_values']\n",
    "    return L_index, L_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "3b6f998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ASLFrameDataset(Dataset):\n",
    "    \"\"\"Dataset for ASL frame data from video clips with feature extraction.\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataframe: Pandas DataFrame containing 'landmarks_file_path' column\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.video_paths = list(dataframe['landmarks_file_path'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of videos in the dataset.\"\"\"\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get data for a complete video with all features.\"\"\"\n",
    "        directory_path = self.video_paths[idx]\n",
    "        \n",
    "        # Get paths to all frame files in this video\n",
    "        frame_to_file, frame_indices, label_path = sorted_npz_files_checked_label(directory_path)\n",
    "        \n",
    "        # Initialize dictionaries to store all data\n",
    "        all_data = {\n",
    "            # Primary features for model input\n",
    "            'dom_landmarks': [],\n",
    "            'non_dom_landmarks': [],\n",
    "            'blendshape_scores': [],\n",
    "            'nose_to_wrist_dist': [],\n",
    "            'dom_velocity_small': [],\n",
    "            'dom_velocity_large': [],\n",
    "            'non_dom_velocity_small': [],\n",
    "            'non_dom_velocity_large': [],\n",
    "            'nose_to_wrist_velocity_small': [],\n",
    "            'nose_to_wrist_velocity_large': [],\n",
    "            \n",
    "            # Additional data for later use\n",
    "            'confidence_scores': [],\n",
    "            'interpolation_scores': [],\n",
    "            'detection_status': [],\n",
    "            'face_detected': [],\n",
    "            'frame_idx': [],\n",
    "            'velocity_confidence': [],\n",
    "            'velocity_calculation_confidence': []\n",
    "        }\n",
    "        \n",
    "        # Load data from each frame\n",
    "        for frame_idx in frame_indices:\n",
    "            file_path = frame_to_file[frame_idx]\n",
    "            frame_data = load_frame_data_standardized(file_path)\n",
    "            \n",
    "            # Unpack frame data\n",
    "            (dom_landmarks_standardized,\n",
    "             non_dom_landmarks_standardized,\n",
    "             confidence_scores,\n",
    "             interpolation_scores,\n",
    "             detection_status,\n",
    "             blendshape_scores_standardized,\n",
    "             face_detected,\n",
    "             nose_to_wrist_dist_standardized,\n",
    "             frame_idx_val,\n",
    "             timestamp_ms,  # We'll skip this one\n",
    "             dom_velocity_small_standardized,\n",
    "             dom_velocity_large_standardized,\n",
    "             non_dom_velocity_small_standardized,\n",
    "             non_dom_velocity_large_standardized,\n",
    "             velocity_confidence,\n",
    "             velocity_calculation_confidence,\n",
    "             nose_to_wrist_velocity_small_standardized,\n",
    "             nose_to_wrist_velocity_large_standardized) = frame_data\n",
    "            \n",
    "            # Store primary features for model input\n",
    "            all_data['dom_landmarks'].append(dom_landmarks_standardized)\n",
    "            all_data['non_dom_landmarks'].append(non_dom_landmarks_standardized)\n",
    "            all_data['blendshape_scores'].append(blendshape_scores_standardized)\n",
    "            all_data['nose_to_wrist_dist'].append(nose_to_wrist_dist_standardized)\n",
    "            all_data['dom_velocity_small'].append(dom_velocity_small_standardized)\n",
    "            all_data['dom_velocity_large'].append(dom_velocity_large_standardized)\n",
    "            all_data['non_dom_velocity_small'].append(non_dom_velocity_small_standardized)\n",
    "            all_data['non_dom_velocity_large'].append(non_dom_velocity_large_standardized)\n",
    "            all_data['nose_to_wrist_velocity_small'].append(nose_to_wrist_velocity_small_standardized)\n",
    "            all_data['nose_to_wrist_velocity_large'].append(nose_to_wrist_velocity_large_standardized)\n",
    "            \n",
    "            # Store additional data for later use\n",
    "            all_data['confidence_scores'].append(confidence_scores)\n",
    "            all_data['interpolation_scores'].append(interpolation_scores)\n",
    "            all_data['detection_status'].append(detection_status)\n",
    "            all_data['face_detected'].append(face_detected)\n",
    "            all_data['frame_idx'].append(frame_idx_val)\n",
    "            all_data['velocity_confidence'].append(velocity_confidence)\n",
    "            all_data['velocity_calculation_confidence'].append(velocity_calculation_confidence)\n",
    "        \n",
    "        # Convert lists to numpy arrays\n",
    "        for key in all_data:\n",
    "            all_data[key] = np.array(all_data[key])\n",
    "        \n",
    "        # Load label data\n",
    "        L_index, L_values = load_label(label_path)\n",
    "        all_data['L_index'] = L_index\n",
    "        all_data['L_values'] = L_values\n",
    "        \n",
    "        # Store sequence length and directory path\n",
    "        all_data['seq_length'] = len(frame_indices)\n",
    "        all_data['directory_path'] = directory_path\n",
    "        \n",
    "        return all_data\n",
    "\n",
    "\n",
    "class SingleDataFrameBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Custom batch sampler that ensures each batch contains samples \n",
    "    from only one dataframe.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_sizes: List[int], batch_size: int, drop_last: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the batch sampler.\n",
    "        \n",
    "        Args:\n",
    "            dataset_sizes: List of sizes for each dataset\n",
    "            batch_size: Batch size\n",
    "            drop_last: Whether to drop the last batch if incomplete\n",
    "        \"\"\"\n",
    "        self.dataset_sizes = dataset_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        \n",
    "        # Calculate offsets for indexing into the combined dataset\n",
    "        self.offsets = [0]\n",
    "        for size in dataset_sizes[:-1]:\n",
    "            self.offsets.append(self.offsets[-1] + size)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate batches of indices, ensuring each batch comes from one dataset.\"\"\"\n",
    "        # Create index lists for each dataset\n",
    "        all_indices = []\n",
    "        for dataset_idx, size in enumerate(self.dataset_sizes):\n",
    "            offset = self.offsets[dataset_idx]\n",
    "            indices = list(range(offset, offset + size))\n",
    "            random.shuffle(indices)\n",
    "            all_indices.append(indices)\n",
    "            \n",
    "        # Create batches for each dataset\n",
    "        all_batches = []\n",
    "        for dataset_idx, indices in enumerate(all_indices):\n",
    "            for i in range(0, len(indices), self.batch_size):\n",
    "                batch = indices[i:min(i + self.batch_size, len(indices))]\n",
    "                \n",
    "                # Skip last incomplete batch if drop_last is True\n",
    "                if self.drop_last and len(batch) < self.batch_size:\n",
    "                    continue\n",
    "                \n",
    "                all_batches.append(batch)\n",
    "        \n",
    "        # Shuffle the order of batches\n",
    "        random.shuffle(all_batches)\n",
    "        \n",
    "        # Yield batches one at a time\n",
    "        for batch in all_batches:\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches.\"\"\"\n",
    "        if self.drop_last:\n",
    "            return sum(size // self.batch_size for size in self.dataset_sizes)\n",
    "        else:\n",
    "            return sum((size + self.batch_size - 1) // self.batch_size for size in self.dataset_sizes)\n",
    "\n",
    "def collate_with_dynamic_padding(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that handles variable-length sequences and label data.\n",
    "    \"\"\"\n",
    "    # Find the maximum sequence length in this batch\n",
    "    max_seq_length = max(sample['seq_length'] for sample in batch)\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # Initialize the result dictionary\n",
    "    result = {\n",
    "        'directory_paths': [],\n",
    "        'seq_lengths': []\n",
    "    }\n",
    "    \n",
    "    # Store directory paths and sequence lengths\n",
    "    for sample in batch:\n",
    "        result['directory_paths'].append(sample['directory_path'])\n",
    "        result['seq_lengths'].append(sample['seq_length'])\n",
    "    \n",
    "    result['seq_lengths'] = torch.tensor(result['seq_lengths'], dtype=torch.long)\n",
    "    \n",
    "    # Create mask tensor for frames [batch_size, max_seq_length]\n",
    "    frame_mask = torch.zeros((batch_size, max_seq_length), dtype=torch.bool)\n",
    "    \n",
    "    # Handle variable-sized label data\n",
    "    # Find maximum dimensions for L_index and L_values\n",
    "    max_tokens = max(sample['L_index'].shape[0] for sample in batch)\n",
    "    token_width = batch[0]['L_index'].shape[1]  # Assuming all have same width (6)\n",
    "    \n",
    "    # Create padded tensors for labels - using appropriate dtypes\n",
    "    L_index_padded = torch.zeros((batch_size, max_tokens, token_width), dtype=torch.long)\n",
    "    L_values_padded = torch.zeros((batch_size, max_tokens, token_width), dtype=torch.float32)\n",
    "    label_mask = torch.zeros((batch_size, max_tokens), dtype=torch.bool)\n",
    "    \n",
    "    # Fill in label data\n",
    "    for i, sample in enumerate(batch):\n",
    "        num_tokens = sample['L_index'].shape[0]\n",
    "        L_index_padded[i, :num_tokens] = torch.tensor(sample['L_index'], dtype=torch.long)\n",
    "        L_values_padded[i, :num_tokens] = torch.tensor(sample['L_values'], dtype=torch.float32)\n",
    "        label_mask[i, :num_tokens] = True\n",
    "    \n",
    "    result['L_index'] = L_index_padded\n",
    "    result['L_values'] = L_values_padded\n",
    "    result['label_mask'] = label_mask\n",
    "    \n",
    "    # Process feature data with consistent dimensions\n",
    "    feature_keys = [\n",
    "        # Primary features for model input\n",
    "        'dom_landmarks', 'non_dom_landmarks', 'blendshape_scores',\n",
    "        'nose_to_wrist_dist', 'dom_velocity_small', 'dom_velocity_large',\n",
    "        'non_dom_velocity_small', 'non_dom_velocity_large',\n",
    "        'nose_to_wrist_velocity_small', 'nose_to_wrist_velocity_large',\n",
    "        \n",
    "        # Additional data for later use\n",
    "        'confidence_scores', 'interpolation_scores', 'detection_status',\n",
    "        'frame_idx', 'velocity_confidence',\n",
    "        'velocity_calculation_confidence'\n",
    "    ]\n",
    "    \n",
    "    # Process all standard features\n",
    "    for key in feature_keys:\n",
    "        try:\n",
    "            # Get the sample feature\n",
    "            sample_feature = batch[0][key]\n",
    "            feature_shape = sample_feature.shape[1:] if len(sample_feature.shape) > 1 else ()\n",
    "            \n",
    "            # Create padded tensor [batch_size, max_seq_length, *feature_shape]\n",
    "            padded_tensor = torch.zeros((batch_size, max_seq_length) + feature_shape, dtype=torch.float32)\n",
    "            \n",
    "            # Fill in the actual data and update the mask\n",
    "            for i, sample in enumerate(batch):\n",
    "                seq_length = sample['seq_length']\n",
    "                feature_data = sample[key]\n",
    "                padded_tensor[i, :seq_length] = torch.tensor(feature_data, dtype=torch.float32)\n",
    "                frame_mask[i, :seq_length] = True\n",
    "                \n",
    "            # Add to result\n",
    "            result[key] = padded_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing feature '{key}': {e}\")\n",
    "            print(f\"  Shape in first sample: {np.array(batch[0][key]).shape}\")\n",
    "            if i > 0:\n",
    "                print(f\"  Shape in problematic sample {i}: {np.array(sample[key]).shape}\")\n",
    "    \n",
    "    # Process face_detected separately with proper reshaping\n",
    "    try:\n",
    "        # Create a tensor specifically for face_detected (which needs special handling)\n",
    "        face_detected_tensor = torch.zeros((batch_size, max_seq_length), dtype=torch.float32)\n",
    "        \n",
    "        for i, sample in enumerate(batch):\n",
    "            seq_length = sample['seq_length']\n",
    "            face_data = sample['face_detected']\n",
    "            \n",
    "            # Convert to tensor and ensure it's 1D\n",
    "            face_tensor = torch.tensor(face_data, dtype=torch.float32)\n",
    "            \n",
    "            # Assign directly without reshaping\n",
    "            face_detected_tensor[i, :seq_length] = face_tensor\n",
    "            \n",
    "        result['face_detected'] = face_detected_tensor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing face_detected: {e}\")\n",
    "        print(f\"  Shape: {np.array(batch[0]['face_detected']).shape}\")\n",
    "    \n",
    "    # Add the frame mask\n",
    "    result['mask'] = frame_mask\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def create_asl_dataloader(low_df, mid_df, high_df, batch_size=16, num_workers=4, drop_last=False):\n",
    "    \"\"\"\n",
    "    Create a data loader for ASL data that ensures batches only contain samples from one dataframe.\n",
    "    \n",
    "    Args:\n",
    "        low_df: DataFrame with low frame count videos\n",
    "        mid_df: DataFrame with medium frame count videos\n",
    "        high_df: DataFrame with high frame count videos\n",
    "        batch_size: Batch size\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        drop_last: Whether to drop the last batch if incomplete\n",
    "        \n",
    "    Returns:\n",
    "        A DataLoader that yields batches from the three dataframes\n",
    "    \"\"\"\n",
    "    # Create datasets for each dataframe\n",
    "    low_dataset = ASLFrameDataset(low_df)\n",
    "    mid_dataset = ASLFrameDataset(mid_df)\n",
    "    high_dataset = ASLFrameDataset(high_df)\n",
    "    \n",
    "    # Get dataset sizes\n",
    "    dataset_sizes = [len(low_dataset), len(mid_dataset), len(high_dataset)]\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_dataset = ConcatDataset([low_dataset, mid_dataset, high_dataset])\n",
    "    \n",
    "    # Create a batch sampler that ensures batches only contain samples from one dataframe\n",
    "    batch_sampler = SingleDataFrameBatchSampler(dataset_sizes, batch_size, drop_last)\n",
    "    \n",
    "    # Create the data loader\n",
    "    data_loader = DataLoader(\n",
    "        combined_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_with_dynamic_padding\n",
    "    )\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "73f77c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_df = pd.read_csv(\"./low_df_only_path.csv\")\n",
    "mid_df = pd.read_csv(\"./mid_df_only_path.csv\")\n",
    "high_df = pd.read_csv(\"./high_df_only_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "b727312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded batch 0\n",
      "tensor([50, 45])\n",
      "Successfully loaded batch 1\n",
      "tensor([42, 48])\n",
      "Successfully loaded batch 2\n",
      "tensor([67, 91])\n",
      "Successfully loaded batch 3\n",
      "tensor([40, 42])\n",
      "Successfully loaded batch 4\n",
      "tensor([104, 105])\n",
      "Successfully loaded batch 5\n",
      "tensor([36, 26])\n"
     ]
    }
   ],
   "source": [
    "loader = create_asl_dataloader(\n",
    "    low_df=low_df, \n",
    "    mid_df=mid_df, \n",
    "    high_df=high_df,\n",
    "    batch_size=2,\n",
    "    num_workers=0\n",
    ")\n",
    "for i, batch in enumerate(loader):\n",
    "    print(f\"Successfully loaded batch {i}\")\n",
    "    print(batch['seq_lengths'])\n",
    "    if i >= 5:  # Check a few batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "e34bc35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch contains data from 2 videos\n",
      "Video paths: ['./OpenASL-main/Open_asl_all_data/clips/aWYgWpsTAjo-00:00:11.929-00:00:13.910_fps15_landmarks/', './ASL_Citizen/videos/3837343931767261-OPINION 1_fps15_L_landmarks/']\n",
      "Sequence lengths: tensor([36, 26])\n",
      "Maximum sequence length in this batch: 36\n",
      "\n",
      "Frame mask:\n",
      "  Shape: torch.Size([2, 36])\n",
      "  Type: torch.bool\n",
      "  Sample values:\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "         False, False, False, False, False, False]])\n",
      "\n",
      "Label mask:\n",
      "  Shape: torch.Size([2, 8])\n",
      "  Type: torch.bool\n",
      "  Sample values:\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False, False, False, False, False]])\n",
      "\n",
      "Label indices:\n",
      "  Shape: torch.Size([2, 8, 6])\n",
      "  Type: torch.int64\n",
      "  First slice sample:\n",
      "tensor([   72,    64,    40,    68,  4178, 31599])\n",
      "\n",
      "Label values:\n",
      "  Shape: torch.Size([2, 8, 6])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([0.7000, 0.0646, 0.0611, 0.0587, 0.0579, 0.0577])\n",
      "\n",
      "dom_landmarks:\n",
      "  Shape: torch.Size([2, 36, 20, 3])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([[-0.7655,  1.1525,  0.5936],\n",
      "        [-0.9083,  1.1929,  0.8082],\n",
      "        [-0.9576,  1.1283,  0.9809],\n",
      "        [-0.9333,  1.0618,  1.0494],\n",
      "        [-0.8397,  1.2820,  0.6919],\n",
      "        [-0.9346,  1.1326,  1.0145],\n",
      "        [-0.9608,  1.0294,  1.1616],\n",
      "        [-0.9513,  0.9474,  1.2168],\n",
      "        [-0.7935,  1.0902,  0.9469],\n",
      "        [-0.9251,  0.9297,  1.1300],\n",
      "        [-0.9522,  0.8179,  1.2189],\n",
      "        [-0.9238,  0.7351,  1.2475],\n",
      "        [-0.6826,  0.8452,  1.1798],\n",
      "        [-0.8492,  0.7297,  1.2779],\n",
      "        [-0.8854,  0.6395,  1.2742],\n",
      "        [-0.8601,  0.5763,  1.2226],\n",
      "        [-0.5361,  0.5611,  1.3321],\n",
      "        [-0.7026,  0.5196,  1.3471],\n",
      "        [-0.7438,  0.4699,  1.2834],\n",
      "        [-0.7348,  0.4358,  1.1980]])\n",
      "\n",
      "non_dom_landmarks:\n",
      "  Shape: torch.Size([2, 36, 20, 3])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([[ 1.4680,  0.9021, -1.2578],\n",
      "        [ 1.5324,  1.0997, -1.9434],\n",
      "        [ 1.5294,  1.2528, -2.2087],\n",
      "        [ 1.4949,  1.3548, -2.4100],\n",
      "        [ 1.4998,  1.1208, -2.4025],\n",
      "        [ 1.4355,  1.2793, -2.2462],\n",
      "        [ 1.4650,  1.2205, -2.2472],\n",
      "        [ 1.4997,  1.1680, -2.2466],\n",
      "        [ 1.3864,  1.0442, -2.2449],\n",
      "        [ 1.3322,  1.1830, -1.8131],\n",
      "        [ 1.3920,  1.0980, -1.5942],\n",
      "        [ 1.4373,  1.0389, -1.5637],\n",
      "        [ 1.2682,  0.9576, -1.9028],\n",
      "        [ 1.2341,  1.0691, -1.3718],\n",
      "        [ 1.2931,  0.9992, -1.0228],\n",
      "        [ 1.3446,  0.9445, -0.8797],\n",
      "        [ 1.1531,  0.8712, -1.5938],\n",
      "        [ 1.1313,  0.9502, -1.2412],\n",
      "        [ 1.1831,  0.9041, -1.0231],\n",
      "        [ 1.2369,  0.8582, -0.9216]])\n",
      "\n",
      "blendshape_scores:\n",
      "  Shape: torch.Size([2, 36, 52])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([ 2.5650e-03, -6.3482e-01, -6.7593e-01,  2.2066e+00,  1.9574e+00,\n",
      "        -7.1628e-02, -2.0743e-01,  2.5747e-01,  7.2133e-02, -7.8099e-01,\n",
      "        -7.0117e-01,  6.2544e-01,  5.1895e-01,  2.3728e-01, -5.3508e-01,\n",
      "        -7.7941e-01, -6.8257e-03, -4.1995e-01, -4.4766e-01, -4.0455e-01,\n",
      "        -2.6606e-01,  2.5589e-01, -4.9049e-03, -5.9824e-02, -2.4580e-01,\n",
      "        -5.6342e-01, -1.0501e-01, -2.7758e-01, -3.7254e-01, -2.8005e-01,\n",
      "         8.1142e-01,  5.8632e-01, -2.7378e-01, -3.4752e-01, -2.9913e-01,\n",
      "        -3.0923e-01, -3.5953e-01, -2.7912e-01, -4.8161e-01,  4.3280e-01,\n",
      "        -5.1679e-01, -5.9788e-01,  3.0831e-01, -1.6944e-01,  3.2343e+00,\n",
      "         3.7156e+00,  1.2520e+00,  3.5350e+00, -8.1142e-03,  8.4907e-02,\n",
      "         3.4001e-01,  1.9260e-01])\n",
      "\n",
      "nose_to_wrist_dist:\n",
      "  Shape: torch.Size([2, 36, 2, 2])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([[ 0.6922, -1.4198],\n",
      "        [-1.6845, -1.4302]])\n",
      "\n",
      "dom_velocity_small:\n",
      "  Shape: torch.Size([2, 36, 20, 5])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
      "        [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
      "        [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
      "        [-6.3390e-01, -1.2001e-02, -1.6251e-02, -1.4160e+00,  2.3259e-03],\n",
      "        [-6.1212e-01, -5.9781e-03, -1.2865e-02, -1.4158e+00,  8.3111e-03],\n",
      "        [-5.9569e-01, -1.5343e-02, -2.1330e-02, -1.4158e+00,  6.5718e-03],\n",
      "        [-6.1056e-01, -2.4008e-02, -2.2362e-02, -1.4158e+00,  5.8891e-03],\n",
      "        [-6.2919e-01, -2.8595e-02, -1.9942e-02, -1.4159e+00,  3.7739e-03],\n",
      "        [-6.1127e-01, -1.2797e-02, -1.1197e-02, -1.4159e+00,  8.2197e-03],\n",
      "        [-6.0042e-01, -2.0855e-02, -2.2079e-02, -1.4158e+00,  5.4550e-03],\n",
      "        [-6.1116e-01, -2.6921e-02, -2.1978e-02, -1.4159e+00,  2.5359e-03],\n",
      "        [-6.1820e-01, -2.9067e-02, -1.8126e-02, -1.4160e+00,  1.1019e-03],\n",
      "        [-6.1508e-01, -2.0269e-02, -1.1890e-02, -1.4159e+00,  7.4416e-03],\n",
      "        [-6.0877e-01, -2.6380e-02, -2.2669e-02, -1.4159e+00,  2.6408e-03],\n",
      "        [-6.1197e-01, -3.1247e-02, -2.1728e-02, -1.4160e+00, -8.1908e-04],\n",
      "        [-6.1015e-01, -3.2168e-02, -1.7380e-02, -1.4160e+00, -2.8086e-03],\n",
      "        [-6.2161e-01, -2.6101e-02, -1.3377e-02, -1.4159e+00,  5.9179e-03],\n",
      "        [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
      "        [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
      "        [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]])\n",
      "\n",
      "dom_velocity_large:\n",
      "  Shape: torch.Size([2, 36, 20, 5])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
      "        [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
      "        [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
      "        [-7.5414e-01, -1.7585e-02, -1.3272e-02, -1.2904e+00, -9.7386e-04],\n",
      "        [-7.2834e-01, -1.0552e-02, -6.1860e-03, -1.2902e+00,  1.3839e-02],\n",
      "        [-7.1428e-01, -2.2887e-02, -1.5396e-02, -1.2902e+00,  1.1654e-02],\n",
      "        [-7.3133e-01, -3.2295e-02, -1.7673e-02, -1.2903e+00,  7.1067e-03],\n",
      "        [-7.5221e-01, -3.6377e-02, -1.7344e-02, -1.2903e+00,  3.2925e-03],\n",
      "        [-7.3187e-01, -2.0132e-02, -3.0537e-03, -1.2902e+00,  1.6645e-02],\n",
      "        [-7.2819e-01, -2.9101e-02, -1.8283e-02, -1.2902e+00,  1.2179e-02],\n",
      "        [-7.4168e-01, -3.4365e-02, -1.9708e-02, -1.2903e+00,  6.2284e-03],\n",
      "        [-7.5060e-01, -3.5868e-02, -1.6503e-02, -1.2904e+00,  1.2599e-03],\n",
      "        [-7.4917e-01, -2.9210e-02, -5.4647e-03, -1.2903e+00,  1.6719e-02],\n",
      "        [-7.4919e-01, -3.5781e-02, -1.9881e-02, -1.2903e+00,  9.9028e-03],\n",
      "        [-7.5371e-01, -3.9156e-02, -1.9622e-02, -1.2904e+00,  2.7148e-03],\n",
      "        [-7.5121e-01, -3.8879e-02, -1.5817e-02, -1.2904e+00, -1.1765e-03],\n",
      "        [-7.6964e-01, -3.4093e-02, -8.8835e-03, -1.2903e+00,  1.4433e-02],\n",
      "        [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
      "        [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
      "        [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]])\n",
      "\n",
      "non_dom_velocity_small:\n",
      "  Shape: torch.Size([2, 36, 20, 5])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
      "        [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
      "        [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
      "        [-3.3980e-01, -1.0318e-02,  8.8433e-03, -1.0977e+00, -2.8096e-03],\n",
      "        [-3.2139e-01, -7.4050e-03,  1.1434e-02, -1.0977e+00,  1.0670e-03],\n",
      "        [-3.3562e-01, -1.2428e-02,  1.3445e-02, -1.0976e+00, -4.4511e-05],\n",
      "        [-3.4430e-01, -1.6446e-02,  1.4663e-02, -1.0977e+00, -1.5298e-03],\n",
      "        [-3.5344e-01, -1.8392e-02,  1.3544e-02, -1.0977e+00, -2.8935e-03],\n",
      "        [-3.2134e-01, -1.1503e-02,  1.1548e-02, -1.0977e+00,  1.0276e-03],\n",
      "        [-3.3537e-01, -1.6011e-02,  1.4940e-02, -1.0977e+00, -1.2668e-03],\n",
      "        [-3.4259e-01, -1.9409e-02,  1.5243e-02, -1.0977e+00, -3.6743e-03],\n",
      "        [-3.5056e-01, -2.0677e-02,  1.3546e-02, -1.0977e+00, -5.1784e-03],\n",
      "        [-3.2146e-01, -1.5626e-02,  1.2828e-02, -1.0977e+00, -1.2908e-03],\n",
      "        [-3.3433e-01, -1.8548e-02,  1.6583e-02, -1.0977e+00, -3.3323e-03],\n",
      "        [-3.3978e-01, -2.1143e-02,  1.6397e-02, -1.0977e+00, -5.4543e-03],\n",
      "        [-3.4593e-01, -2.1707e-02,  1.4206e-02, -1.0978e+00, -6.2811e-03],\n",
      "        [-3.2226e-01, -1.8014e-02,  1.4098e-02, -1.0977e+00, -4.1233e-03],\n",
      "        [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
      "        [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
      "        [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]])\n",
      "\n",
      "non_dom_velocity_large:\n",
      "  Shape: torch.Size([2, 36, 20, 5])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([[-3.9297e-01,  2.0130e-02,  1.4542e-02, -1.0331e+00,  1.3694e-03],\n",
      "        [-4.1717e-01,  1.1397e-02,  1.5145e-02, -1.0332e+00,  1.5606e-03],\n",
      "        [-4.3783e-01,  5.9299e-03,  1.5309e-02, -1.0332e+00,  1.8212e-03],\n",
      "        [-4.5638e-01,  2.1029e-03,  1.5086e-02, -1.0332e+00,  3.0693e-04],\n",
      "        [-4.2668e-01,  4.6055e-03,  1.5452e-02, -1.0332e+00,  3.6217e-03],\n",
      "        [-4.5038e-01, -1.0720e-03,  1.7832e-02, -1.0332e+00,  9.8735e-04],\n",
      "        [-4.6357e-01, -5.3590e-03,  1.8594e-02, -1.0332e+00, -2.4241e-03],\n",
      "        [-4.7622e-01, -6.6478e-03,  1.8260e-02, -1.0332e+00, -4.4807e-03],\n",
      "        [-4.2815e-01, -6.4285e-05,  1.5166e-02, -1.0332e+00,  2.2186e-03],\n",
      "        [-4.5150e-01, -5.2151e-03,  1.9180e-02, -1.0332e+00,  2.5060e-05],\n",
      "        [-4.6221e-01, -8.2964e-03,  1.9014e-02, -1.0332e+00, -3.8460e-03],\n",
      "        [-4.7311e-01, -9.0039e-03,  1.7448e-02, -1.0332e+00, -5.6112e-03],\n",
      "        [-4.3024e-01, -3.6586e-03,  1.6898e-02, -1.0332e+00,  4.2750e-07],\n",
      "        [-4.5152e-01, -7.5857e-03,  2.1139e-02, -1.0332e+00, -1.9287e-03],\n",
      "        [-4.5968e-01, -9.9201e-03,  2.0445e-02, -1.0332e+00, -5.6341e-03],\n",
      "        [-4.6816e-01, -1.0104e-02,  1.8171e-02, -1.0332e+00, -6.1697e-03],\n",
      "        [-4.3307e-01, -4.9964e-03,  1.8316e-02, -1.0332e+00, -4.1702e-03],\n",
      "        [-4.5078e-01, -7.0583e-03,  2.1566e-02, -1.0332e+00, -5.2447e-03],\n",
      "        [-4.5801e-01, -8.7969e-03,  2.1025e-02, -1.0332e+00, -7.2302e-03],\n",
      "        [-4.6547e-01, -8.7936e-03,  1.9168e-02, -1.0332e+00, -6.8863e-03]])\n",
      "\n",
      "nose_to_wrist_velocity_small:\n",
      "  Shape: torch.Size([2, 36, 2, 3])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([[-0.3644, -0.0383, -0.0022],\n",
      "        [-0.3113, -0.0307,  0.0108]])\n",
      "\n",
      "nose_to_wrist_velocity_large:\n",
      "  Shape: torch.Size([2, 36, 2, 3])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([[-0.5147, -0.0215, -0.0032],\n",
      "        [-0.4816, -0.0126,  0.0123]])\n",
      "\n",
      "confidence_scores:\n",
      "  Shape: torch.Size([2, 36, 2])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([0.0000, 0.5662])\n",
      "\n",
      "face_detected:\n",
      "  Shape: torch.Size([2, 36])\n",
      "  Type: torch.float32\n",
      "  Sample values:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "velocity_confidence:\n",
      "  Shape: torch.Size([2, 36, 2])\n",
      "  Type: torch.float32\n",
      "  First slice sample:\n",
      "tensor([0., 0.])\n",
      "\n",
      "Frame mask visualization (1=real data, 0=padding):\n",
      "Sample 0: 111111111111111111111111111111111111 (36 real frames, 0 padding)\n",
      "Sample 1: 111111111111111111111111110000000000 (26 real frames, 10 padding)\n",
      "\n",
      "Checking consistency across batch dimension:\n",
      "seq_lengths: First dimension size = 2\n",
      "L_index: First dimension size = 2\n",
      "L_values: First dimension size = 2\n",
      "label_mask: First dimension size = 2\n",
      "dom_landmarks: First dimension size = 2\n",
      "non_dom_landmarks: First dimension size = 2\n",
      "blendshape_scores: First dimension size = 2\n",
      "nose_to_wrist_dist: First dimension size = 2\n",
      "dom_velocity_small: First dimension size = 2\n",
      "dom_velocity_large: First dimension size = 2\n",
      "non_dom_velocity_small: First dimension size = 2\n",
      "non_dom_velocity_large: First dimension size = 2\n",
      "nose_to_wrist_velocity_small: First dimension size = 2\n",
      "nose_to_wrist_velocity_large: First dimension size = 2\n",
      "confidence_scores: First dimension size = 2\n",
      "interpolation_scores: First dimension size = 2\n",
      "detection_status: First dimension size = 2\n",
      "frame_idx: First dimension size = 2\n",
      "velocity_confidence: First dimension size = 2\n",
      "velocity_calculation_confidence: First dimension size = 2\n",
      "face_detected: First dimension size = 2\n",
      "mask: First dimension size = 2\n"
     ]
    }
   ],
   "source": [
    "def inspect_tensor(tensor, name, max_items=3):\n",
    "    \"\"\"Print shape and sample values from a tensor.\"\"\"\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Shape: {tensor.shape}\")\n",
    "    print(f\"  Type: {tensor.dtype}\")\n",
    "    \n",
    "    # Print a few values if it's not too large\n",
    "    if tensor.numel() > 0:\n",
    "        if tensor.dim() <= 2:\n",
    "            print(f\"  Sample values:\\n{tensor[:max_items]}\")\n",
    "        else:\n",
    "            # For higher dimensional tensors, show the first slice\n",
    "            print(f\"  First slice sample:\\n{tensor[0, 0]}\")\n",
    "    \n",
    "    # Check for NaN or infinity values\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(\"  WARNING: Contains NaN values!\")\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(\"  WARNING: Contains infinity values!\")\n",
    "\n",
    "# Print basic batch information\n",
    "print(f\"Batch contains data from {len(batch['directory_paths'])} videos\")\n",
    "print(f\"Video paths: {batch['directory_paths']}\")\n",
    "print(f\"Sequence lengths: {batch['seq_lengths']}\")\n",
    "print(f\"Maximum sequence length in this batch: {batch['mask'].shape[1]}\")\n",
    "\n",
    "# Inspect key tensors\n",
    "inspect_tensor(batch['mask'], \"Frame mask\")\n",
    "inspect_tensor(batch['label_mask'], \"Label mask\")\n",
    "inspect_tensor(batch['L_index'], \"Label indices\")\n",
    "inspect_tensor(batch['L_values'], \"Label values\")\n",
    "\n",
    "# Check primary feature tensors\n",
    "primary_features = [\n",
    "    'dom_landmarks', 'non_dom_landmarks', 'blendshape_scores',\n",
    "    'nose_to_wrist_dist', 'dom_velocity_small', 'dom_velocity_large', \n",
    "    'non_dom_velocity_small', 'non_dom_velocity_large',\n",
    "    'nose_to_wrist_velocity_small', 'nose_to_wrist_velocity_large'\n",
    "]\n",
    "\n",
    "for feature in primary_features:\n",
    "    inspect_tensor(batch[feature], feature)\n",
    "\n",
    "# Check a few additional data tensors\n",
    "additional_features = [\n",
    "    'confidence_scores', 'face_detected', 'velocity_confidence'\n",
    "]\n",
    "\n",
    "for feature in additional_features:\n",
    "    inspect_tensor(batch[feature], feature)\n",
    "\n",
    "# Visualize the padding with the mask\n",
    "print(\"\\nFrame mask visualization (1=real data, 0=padding):\")\n",
    "for i in range(len(batch['mask'])):\n",
    "    seq_len = batch['seq_lengths'][i].item()\n",
    "    max_len = batch['mask'].shape[1]\n",
    "    padding = max_len - seq_len\n",
    "    print(f\"Sample {i}: {'1'*seq_len}{'0'*padding} ({seq_len} real frames, {padding} padding)\")\n",
    "\n",
    "# Optional: Check for consistent shapes across batch dimension\n",
    "print(\"\\nChecking consistency across batch dimension:\")\n",
    "for key in batch:\n",
    "    if isinstance(batch[key], torch.Tensor) and batch[key].dim() > 0:\n",
    "        print(f\"{key}: First dimension size = {batch[key].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8c7ed946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LandmarkEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates learnable embeddings for hand landmarks.\n",
    "    \n",
    "    This module maps each landmark (across both hands) to a unique \n",
    "    embedding vector that encodes its semantic meaning.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_landmarks_per_hand=21):\n",
    "        \"\"\"\n",
    "        Initialize the landmark embedding module.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of the embedding vectors\n",
    "            num_landmarks_per_hand: Number of landmarks per hand (default: 21)\n",
    "        \"\"\"\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_landmarks_per_hand = num_landmarks_per_hand\n",
    "        self.total_landmarks = 2 * num_landmarks_per_hand  # Both hands\n",
    "        \n",
    "        # Create the embedding table: [total_landmarks, embedding_dim]\n",
    "        self.embedding_table = nn.Embedding(\n",
    "            num_embeddings=self.total_landmarks,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        \n",
    "        # Initialize the embeddings with a normal distribution\n",
    "        nn.init.normal_(self.embedding_table.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, landmark_indices=None):\n",
    "        \"\"\"\n",
    "        Get embeddings for landmarks.\n",
    "        \n",
    "        Args:\n",
    "            landmark_indices: Optional tensor of landmark indices to retrieve.\n",
    "                             If None, returns all landmark embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of landmark embeddings\n",
    "        \"\"\"\n",
    "        if landmark_indices is None:\n",
    "            # Return all landmark embeddings\n",
    "            # Create indices for all landmarks: 0 to total_landmarks-1\n",
    "            landmark_indices = torch.arange(self.total_landmarks, device=self.embedding_table.weight.device)\n",
    "        \n",
    "        # Get the embeddings for the specified indices\n",
    "        embeddings = self.embedding_table(landmark_indices)\n",
    "        return embeddings\n",
    "    \n",
    "    def get_dominant_hand_embeddings(self):\n",
    "        \"\"\"\n",
    "        Get embeddings for landmarks in the dominant hand.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [num_landmarks_per_hand, embedding_dim]\n",
    "        \"\"\"\n",
    "        indices = torch.arange(self.num_landmarks_per_hand, \n",
    "                              device=self.embedding_table.weight.device)\n",
    "        return self.embedding_table(indices)\n",
    "    \n",
    "    def get_non_dominant_hand_embeddings(self):\n",
    "        \"\"\"\n",
    "        Get embeddings for landmarks in the non-dominant hand.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [num_landmarks_per_hand, embedding_dim]\n",
    "        \"\"\"\n",
    "        indices = torch.arange(self.num_landmarks_per_hand, self.total_landmarks, \n",
    "                              device=self.embedding_table.weight.device)\n",
    "        return self.embedding_table(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "b18ab69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 36\n",
    "embedding_table = LandmarkEmbedding(embedding_dim=embedding_dim, num_landmarks_per_hand=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "d3e9d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_table.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b428bd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 36])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "88c95467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkSpatialEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the spatial information (x,y,z coordinates) of individual hand landmarks.\n",
    "    \n",
    "    This module transforms the 3D coordinates of each landmark into a higher-dimensional\n",
    "    representation that captures the 'where' aspect of the landmark.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the spatial encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Base dimension for the model\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*embedding_dim] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(LandmarkSpatialEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension will be 2*embedding_dim as requested\n",
    "        self.output_dim = 2 * embedding_dim\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * embedding_dim] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(3, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.spatial_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, landmarks):\n",
    "        \"\"\"\n",
    "        Encode the spatial coordinates of landmarks.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Tensor of shape [..., 3] containing x,y,z coordinates\n",
    "                       The leading dimensions can be anything (batch, sequence, landmark)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., output_dim] with the spatial encodings\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = landmarks.shape\n",
    "        \n",
    "        # Reshape to [-1, 3] to process all landmarks in parallel\n",
    "        flat_landmarks = landmarks.reshape(-1, 3)\n",
    "        \n",
    "        # Apply the spatial encoder\n",
    "        encoded = self.spatial_encoder(flat_landmarks)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        reshaped_encoded = encoded.reshape(*original_shape[:-1], self.output_dim)\n",
    "        \n",
    "        return reshaped_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "076147f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ladmark_encoder = LandmarkSpatialEncoder(embedding_dim, hidden_dims=[30, 60, 30],activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "183fdcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 20, 3])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['dom_landmarks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c166e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_landmarks_where = ladmark_encoder.forward(batch['dom_landmarks'])\n",
    "non_dom_landmarks_where = ladmark_encoder.forward(batch['non_dom_landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "af243f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_spatial_and_semantic_features(spatial_features, semantic_features):\n",
    "    \"\"\"\n",
    "    Combines the spatial encoder output with the semantic embedding features.\n",
    "    \n",
    "    This function concatenates the \"where\" (spatial) information with the \"what\" \n",
    "    (semantic) information to create a comprehensive landmark representation.\n",
    "    \n",
    "    Args:\n",
    "        spatial_features: Tensor of shape [..., n_spatial_encode] where\n",
    "                         n_spatial_encode = 2*embedding_dim\n",
    "        semantic_features: Tensor of shape [..., embedding_dim]\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [..., 3*embedding_dim] containing the combined representation\n",
    "    \"\"\"\n",
    "\n",
    "    batch_dims = spatial_features.shape[:-2]\n",
    "    expanded_embeddings = semantic_features.expand(*batch_dims, -1, -1)\n",
    "    # Verify that the batch dimensions match\n",
    "    assert spatial_features.shape[:-1] == expanded_embeddings.shape[:-1], \\\n",
    "        \"Batch dimensions of spatial and semantic features must match\"\n",
    "    \n",
    "    # Concatenate along the last dimension\n",
    "    combined_features = torch.cat([expanded_embeddings, spatial_features], dim=-1)\n",
    "    \n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "b8fcf783",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_landmarks_conc = combine_spatial_and_semantic_features(spatial_features=dom_landmarks_where, semantic_features=embeddings[:20])\n",
    "non_dom_landmarks_conc = combine_spatial_and_semantic_features(spatial_features=non_dom_landmarks_where, semantic_features=embeddings[21:41])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f6b2b722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WristSpatialEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the spatial information of wrist landmarks relative to the nose.\n",
    "    \n",
    "    This module processes the 2D coordinates (x,y) of each wrist independently\n",
    "    but in parallel, using shared weights across both wrists.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the wrist spatial encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Base dimension for the model\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*embedding_dim] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(WristSpatialEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension will be 2*embedding_dim as requested\n",
    "        self.output_dim = 2 * embedding_dim\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * embedding_dim] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer (2D coordinates instead of 3D)\n",
    "        layers.append(nn.Linear(2, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.wrist_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, wrist_coordinates):\n",
    "        \"\"\"\n",
    "        Encode the spatial coordinates of wrist landmarks.\n",
    "        \n",
    "        Args:\n",
    "            wrist_coordinates: Tensor of shape [..., 2, 2] containing x,y coordinates\n",
    "                              for both wrists. Leading dimensions can be anything\n",
    "                              (batch, sequence), and the last two dimensions are:\n",
    "                              - Dimension -2: Wrist index (0=dominant, 1=non-dominant)\n",
    "                              - Dimension -1: Coordinates (x,y)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., 2, output_dim] with the spatial encodings for each wrist\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = wrist_coordinates.shape\n",
    "        \n",
    "        # Reshape to [-1, 2] to process all wrist coordinates in parallel\n",
    "        # This flattens all leading dimensions and processes each (x,y) pair independently\n",
    "        flat_wrists = wrist_coordinates.reshape(-1, 2)\n",
    "        \n",
    "        # Apply the wrist encoder\n",
    "        encoded = self.wrist_encoder(flat_wrists)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        # Replace the coordinate dimension (2) with output_dim\n",
    "        new_shape = original_shape[:-1] + (self.output_dim,)\n",
    "        reshaped_encoded = encoded.reshape(new_shape)\n",
    "        \n",
    "        return reshaped_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "55733dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrist_encoder = WristSpatialEncoder(embedding_dim, hidden_dims=[30, 60, 30],activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "cbd6cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrists_where = wrist_encoder.forward(wrist_coordinates=batch['nose_to_wrist_dist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "d289b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_wrist_embedding_and_spatial(wrist_embeddings, wrist_spatial_features):\n",
    "    \"\"\"\n",
    "    Combines wrist semantic embeddings with their spatial features.\n",
    "    \n",
    "    This function integrates:\n",
    "    1. The semantic meaning of each wrist (from embeddings)\n",
    "    2. The spatial position of each wrist (from the WristSpatialEncoder)\n",
    "    \n",
    "    Args:\n",
    "        wrist_embeddings: Tensor of shape [2, embedding_dim] with wrist embeddings\n",
    "                         where [0] is dom wrist and [1] is non-dom wrist\n",
    "        wrist_spatial_features: Tensor of shape [..., 2, 2*embedding_dim] \n",
    "                               from the WristSpatialEncoder\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [..., 2, 3*embedding_dim] with the combined representation\n",
    "    \"\"\"\n",
    "    # Get the batch dimensions from the spatial features tensor\n",
    "    batch_dims = wrist_spatial_features.shape[:-2]\n",
    "    \n",
    "    # Expand wrist embeddings to match the batch dimensions\n",
    "    # From [2, embedding_dim] to [..., 2, embedding_dim]\n",
    "    expanded_embeddings = wrist_embeddings.expand(*batch_dims, -1, -1)\n",
    "    \n",
    "    # Verify that the shapes are compatible for concatenation\n",
    "    assert expanded_embeddings.shape[:-1] == wrist_spatial_features.shape[:-1], \\\n",
    "        \"Batch dimensions of embeddings and spatial features must match\"\n",
    "    \n",
    "    # Concatenate along the last dimension\n",
    "    combined_features = torch.cat([\n",
    "        expanded_embeddings,     # Wrist identity (what)\n",
    "        wrist_spatial_features   # Wrist position (where)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "ecd011cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "e91f28dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 2, 72])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrists_where.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "f7ffac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrists_conc = combine_wrist_embedding_and_spatial(wrist_embeddings=torch.cat([embeddings[20], embeddings[41]], dim=-1).reshape((2,-1)), wrist_spatial_features=wrists_where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "42807a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendshapeEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes facial blendshape scores into a higher-dimensional representation.\n",
    "    \n",
    "    This network processes the 52 facial blendshape parameters that capture\n",
    "    expressions and face movements relevant to ASL interpretation.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the blendshape encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Base dimension for the model\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*embedding_dim] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(BlendshapeEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension will be 2*embedding_dim as requested\n",
    "        self.output_dim = 2 * embedding_dim\n",
    "        \n",
    "        # Input dimension for blendshape scores\n",
    "        self.input_dim = 52\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * embedding_dim] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer (52 blendshape scores)\n",
    "        layers.append(nn.Linear(self.input_dim, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.blendshape_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, blendshape_scores):\n",
    "        \"\"\"\n",
    "        Encode the facial blendshape scores.\n",
    "        \n",
    "        Args:\n",
    "            blendshape_scores: Tensor of shape [..., 52] containing facial expression parameters.\n",
    "                              Leading dimensions can be anything (batch, sequence).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., output_dim] with the encoded facial features\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = blendshape_scores.shape\n",
    "        \n",
    "        # Reshape to [-1, 52] to process all blendshape scores in parallel\n",
    "        flat_blendshapes = blendshape_scores.reshape(-1, self.input_dim)\n",
    "        \n",
    "        # Apply the blendshape encoder\n",
    "        encoded = self.blendshape_encoder(flat_blendshapes)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        # Replace the blendshape dimension (52) with output_dim\n",
    "        new_shape = original_shape[:-1] + (self.output_dim,)\n",
    "        reshaped_encoded = encoded.reshape(new_shape)\n",
    "        \n",
    "        return reshaped_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "ea78753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blendshapes_feedforward = BlendshapeEncoder(embedding_dim, hidden_dims=None, num_layers=2,activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "e7eeb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "blendshapes_encoded = blendshapes_feedforward(batch['blendshape_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "486fa1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelocityEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes velocity features of hand landmarks into a higher-dimensional representation.\n",
    "    \n",
    "    This network processes the 5 spherical coordinate velocity features for each landmark\n",
    "    independently but in parallel, using the same weights across all landmarks, hands,\n",
    "    and velocity windows (small and large).\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_velocity_encoding, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the velocity encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            n_velocity_encoding: Output dimension for each landmark's velocity encoding\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*n_velocity_encoding] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(VelocityEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension as specified\n",
    "        self.output_dim = n_velocity_encoding\n",
    "        \n",
    "        # Input dimension for velocity features (spherical coordinates)\n",
    "        self.input_dim = 5\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * n_velocity_encoding] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer (5 velocity features in spherical coordinates)\n",
    "        layers.append(nn.Linear(self.input_dim, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.velocity_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, velocity_features):\n",
    "        \"\"\"\n",
    "        Encode the velocity features for hand landmarks.\n",
    "        \n",
    "        Args:\n",
    "            velocity_features: Tensor of shape [..., 5] containing velocity features\n",
    "                              in spherical coordinates. Leading dimensions can be anything\n",
    "                              (batch, sequence, landmark).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., output_dim] with the encoded velocity features\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = velocity_features.shape\n",
    "        \n",
    "        # Reshape to [-1, 5] to process all velocity features in parallel\n",
    "        flat_velocities = velocity_features.reshape(-1, self.input_dim)\n",
    "        \n",
    "        # Apply the velocity encoder\n",
    "        encoded = self.velocity_encoder(flat_velocities)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        # Replace the velocity dimension (5) with output_dim\n",
    "        new_shape = original_shape[:-1] + (self.output_dim,)\n",
    "        reshaped_encoded = encoded.reshape(new_shape)\n",
    "        \n",
    "        return reshaped_encoded\n",
    "    \n",
    "    def encode_all_velocity_windows(self, dom_vel_small, dom_vel_large, non_dom_vel_small, non_dom_vel_large):\n",
    "        \"\"\"\n",
    "        Encode all four velocity window tensors using the same encoder.\n",
    "        \n",
    "        Args:\n",
    "            dom_vel_small: Dominant hand small window velocities [batch_size, seq_len, 20, 5]\n",
    "            dom_vel_large: Dominant hand large window velocities [batch_size, seq_len, 20, 5]\n",
    "            non_dom_vel_small: Non-dominant hand small window velocities [batch_size, seq_len, 20, 5]\n",
    "            non_dom_vel_large: Non-dominant hand large window velocities [batch_size, seq_len, 20, 5]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing encoded velocity features for all windows\n",
    "        \"\"\"\n",
    "        # Process each velocity window\n",
    "        dom_small_encoded = self.forward(dom_vel_small)  # [batch_size, seq_len, 20, output_dim]\n",
    "        dom_large_encoded = self.forward(dom_vel_large)  # [batch_size, seq_len, 20, output_dim]\n",
    "        non_dom_small_encoded = self.forward(non_dom_vel_small)  # [batch_size, seq_len, 20, output_dim]\n",
    "        non_dom_large_encoded = self.forward(non_dom_vel_large)  # [batch_size, seq_len, 20, output_dim]\n",
    "        \n",
    "        return {\n",
    "            'dom_velocity_small_encoded': dom_small_encoded,\n",
    "            'dom_velocity_large_encoded': dom_large_encoded,\n",
    "            'non_dom_velocity_small_encoded': non_dom_small_encoded,\n",
    "            'non_dom_velocity_large_encoded': non_dom_large_encoded\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6db1a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_feedforward = VelocityEncoder(n_velocity_encoding=2*embedding_dim, hidden_dims=None, num_layers=2,activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "da5bac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_small_vel_encoded = velocity_feedforward.forward(batch['dom_velocity_small']) \n",
    "dom_large_vel_encoded = velocity_feedforward.forward(batch['dom_velocity_large']) \n",
    "non_dom_small_vel_encoded = velocity_feedforward.forward(batch['non_dom_velocity_small']) \n",
    "non_dom_large_vel_encoded = velocity_feedforward.forward(batch['non_dom_velocity_large']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "4a882111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_semantic_and_velocity_features(semantic_features, velocity_small_features, velocity_large_features):\n",
    "    \"\"\"\n",
    "    Combines landmark semantic embeddings with velocity features from both time windows.\n",
    "    \n",
    "    This function concatenates:\n",
    "    1. The \"what\" (semantic embedding) of each landmark\n",
    "    2. The \"how fast small window\" (small window velocity encoding)\n",
    "    3. The \"how fast large window\" (large window velocity encoding)\n",
    "    \n",
    "    Args:\n",
    "        semantic_features: Tensor of shape [..., embedding_dim] containing landmark embeddings\n",
    "        velocity_small_features: Tensor of shape [..., n_velocity_encoding] from small window\n",
    "        velocity_large_features: Tensor of shape [..., n_velocity_encoding] from large window\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [..., embedding_dim + 2*n_velocity_encoding] with the combined representation\n",
    "    \"\"\"\n",
    "    batch_shape = velocity_small_features.shape[:-2]\n",
    "    semantic_features_expanded = semantic_features.expand(*batch_shape, -1, -1)\n",
    "    # Verify that the batch dimensions match\n",
    "    assert semantic_features_expanded.shape[:-1] == velocity_small_features.shape[:-1] == velocity_large_features.shape[:-1], \\\n",
    "        \"Batch dimensions of semantic and velocity features must match\"\n",
    "    \n",
    "    # Concatenate all three feature types along the last dimension\n",
    "    combined_features = torch.cat([\n",
    "        semantic_features_expanded,        # Landmark identity (what)\n",
    "        velocity_small_features,  # Short-term movement (how fast recently)\n",
    "        velocity_large_features   # Long-term movement (how fast overall)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "bf079189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_landmarks_velocity_conc = combine_semantic_and_velocity_features(semantic_features=embeddings[:20], velocity_small_features=dom_small_vel_encoded, velocity_large_features=dom_large_vel_encoded)\n",
    "non_dom_landmarks_velocity_conc = combine_semantic_and_velocity_features(semantic_features=embeddings[21:41], velocity_small_features=non_dom_small_vel_encoded, velocity_large_features=non_dom_large_vel_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "b078bce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 20, 180])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_landmarks_velocity_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "62b43f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WristVelocityEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes velocity features of wrist landmarks relative to the nose.\n",
    "    \n",
    "    This network processes the 3 polar coordinate velocity features for each wrist\n",
    "    independently but in parallel, using the same weights across both wrists\n",
    "    and both velocity windows (small and large).\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_velocity_encoding, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the wrist velocity encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            n_velocity_encoding: Output dimension for each wrist's velocity encoding\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*n_velocity_encoding] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(WristVelocityEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension as specified\n",
    "        self.output_dim = n_velocity_encoding\n",
    "        \n",
    "        # Input dimension for wrist velocity features (polar coordinates)\n",
    "        self.input_dim = 3\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * n_velocity_encoding] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer (3 velocity features in polar coordinates)\n",
    "        layers.append(nn.Linear(self.input_dim, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.wrist_velocity_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, wrist_velocity_features):\n",
    "        \"\"\"\n",
    "        Encode the velocity features for wrist landmarks.\n",
    "        \n",
    "        Args:\n",
    "            wrist_velocity_features: Tensor of shape [..., 3] containing velocity features\n",
    "                                    in polar coordinates. Leading dimensions can be anything\n",
    "                                    (batch, sequence, wrist).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., output_dim] with the encoded velocity features\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = wrist_velocity_features.shape\n",
    "        \n",
    "        # Reshape to [-1, 3] to process all velocity features in parallel\n",
    "        flat_velocities = wrist_velocity_features.reshape(-1, self.input_dim)\n",
    "        \n",
    "        # Apply the wrist velocity encoder\n",
    "        encoded = self.wrist_velocity_encoder(flat_velocities)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        # Replace the velocity dimension (3) with output_dim\n",
    "        new_shape = original_shape[:-1] + (self.output_dim,)\n",
    "        reshaped_encoded = encoded.reshape(new_shape)\n",
    "        \n",
    "        return reshaped_encoded\n",
    "    \n",
    "    def encode_both_velocity_windows(self, wrist_vel_small, wrist_vel_large):\n",
    "        \"\"\"\n",
    "        Encode both velocity window tensors for wrists using the same encoder.\n",
    "        \n",
    "        Args:\n",
    "            wrist_vel_small: Wrist small window velocities [batch_size, seq_len, 2, 3]\n",
    "            wrist_vel_large: Wrist large window velocities [batch_size, seq_len, 2, 3]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing encoded velocity features for both windows\n",
    "        \"\"\"\n",
    "        # Process each velocity window\n",
    "        small_window_encoded = self.forward(wrist_vel_small)  # [batch_size, seq_len, 2, output_dim]\n",
    "        large_window_encoded = self.forward(wrist_vel_large)  # [batch_size, seq_len, 2, output_dim]\n",
    "        \n",
    "        return {\n",
    "            'wrist_velocity_small_encoded': small_window_encoded,\n",
    "            'wrist_velocity_large_encoded': large_window_encoded\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "b6756154",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrist_vel_feedforward = WristVelocityEncoder(n_velocity_encoding=2*embedding_dim, hidden_dims=None, num_layers=2,activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "ae7b4cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrist_vel_small_encoded = wrist_vel_feedforward.forward(batch['nose_to_wrist_velocity_small'])\n",
    "wrist_vel_large_encoded = wrist_vel_feedforward.forward(batch['nose_to_wrist_velocity_large'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "b75c60b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 2, 72])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrist_vel_small_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "a7132abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_wrist_embedding_and_velocity(wrist_embeddings, wrist_velocity_small, wrist_velocity_large):\n",
    "    \"\"\"\n",
    "    Combines wrist semantic embeddings with velocity features from both time windows.\n",
    "    \n",
    "    This function handles the specific arrangement of wrist data in your model:\n",
    "    - In embeddings: Wrists are at indices 20 (dom) and 41 (non-dom) in the embedding table\n",
    "    - In velocity tensors: Wrists are at indices 0 (dom) and 1 (non-dom)\n",
    "    \n",
    "    Args:\n",
    "        wrist_embeddings: Tensor of shape [2, embedding_dim] with wrist embeddings\n",
    "                         where [0] is dom wrist and [1] is non-dom wrist\n",
    "        wrist_velocity_small: Tensor of shape [..., 2, n_velocity_encoding] \n",
    "                             from small window velocity encoder\n",
    "        wrist_velocity_large: Tensor of shape [..., 2, n_velocity_encoding] \n",
    "                             from large window velocity encoder\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [..., 2, embedding_dim + 2*n_velocity_encoding] \n",
    "        with the combined representation for both wrists\n",
    "    \"\"\"\n",
    "    # Get the batch dimensions from the velocity tensors\n",
    "    batch_dims = wrist_velocity_small.shape[:-2]\n",
    "    \n",
    "    # Expand wrist embeddings to match the batch dimensions\n",
    "    # From [2, embedding_dim] to [..., 2, embedding_dim]\n",
    "    expanded_embeddings = wrist_embeddings.expand(*batch_dims, -1, -1)\n",
    "    \n",
    "    # Verify that the shapes are compatible for concatenation\n",
    "    assert expanded_embeddings.shape[:-1] == wrist_velocity_small.shape[:-1] == wrist_velocity_large.shape[:-1], \\\n",
    "        \"Batch dimensions of embeddings and velocity features must match\"\n",
    "    \n",
    "    # Concatenate along the last dimension\n",
    "    combined_features = torch.cat([\n",
    "        expanded_embeddings,      # Wrist identity (what)\n",
    "        wrist_velocity_small,     # Short-term movement (how fast recently)\n",
    "        wrist_velocity_large      # Long-term movement (how fast overall)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "37ef1759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([embeddings[20], embeddings[41]], dim=-1).reshape((2,-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "540c985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrists_vel_conc = combine_wrist_embedding_and_velocity(wrist_embeddings=torch.cat([embeddings[20], embeddings[41]], dim=-1).reshape((2,-1)), wrist_velocity_small=wrist_vel_small_encoded, wrist_velocity_large=wrist_vel_large_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "51a44c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder for processing hand landmarks and learning contextual relationships.\n",
    "    \n",
    "    This module treats the set of landmarks as a sequence and applies self-attention\n",
    "    to learn the relationships between different parts of the hand.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 num_layers=2,\n",
    "                 num_heads=8,\n",
    "                 hidden_dim=None,\n",
    "                 ff_dim=None,\n",
    "                 prenorm=True,\n",
    "                 activation='gelu',\n",
    "                 init_method='xavier_uniform',\n",
    "                 init_gain=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the landmark transformer encoder.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input features per landmark (3*embedding_dim)\n",
    "            num_layers: Number of transformer encoder layers\n",
    "            num_heads: Number of attention heads\n",
    "            hidden_dim: Hidden dimension size (if None, uses input_dim)\n",
    "            ff_dim: Feed-forward dimension (if None, uses 4*hidden_dim)\n",
    "            prenorm: Whether to use pre-norm (True) or post-norm (False) architecture\n",
    "            activation: Activation function in feed-forward network\n",
    "            init_method: Weight initialization method\n",
    "            init_gain: Gain parameter for initialization\n",
    "        \"\"\"\n",
    "        super(LandmarkTransformerEncoder, self).__init__()\n",
    "        \n",
    "        # Set dimensions\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim if hidden_dim is not None else input_dim\n",
    "        self.ff_dim = ff_dim if ff_dim is not None else 4 * self.hidden_dim\n",
    "        \n",
    "        # Input projection if needed\n",
    "        self.input_projection = None\n",
    "        if self.input_dim != self.hidden_dim:\n",
    "            self.input_projection = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        \n",
    "        # Create transformer encoder layers\n",
    "        encoder_layer = LandmarkTransformerLayer(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=self.ff_dim,\n",
    "            prenorm=prenorm,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        \n",
    "        # Final normalization\n",
    "        self.norm = nn.LayerNorm(self.hidden_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights(init_method, init_gain)\n",
    "    \n",
    "    def _init_weights(self, init_method, gain):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if init_method == 'xavier_uniform':\n",
    "                    nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "                elif init_method == 'xavier_normal':\n",
    "                    nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "                elif init_method == 'kaiming_uniform':\n",
    "                    nn.init.kaiming_uniform_(module.weight, a=0, mode='fan_in')\n",
    "                elif init_method == 'kaiming_normal':\n",
    "                    nn.init.kaiming_normal_(module.weight, a=0, mode='fan_in')\n",
    "                else:\n",
    "                    raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "                \n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process hand landmarks through the transformer.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, seq_len, 20, input_dim]\n",
    "               where 20 is the number of landmarks and input_dim is 3*embedding_dim\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, 20, hidden_dim]\n",
    "            with contextually enriched landmark representations\n",
    "        \"\"\"\n",
    "        # Get original shape\n",
    "        batch_size, seq_len, num_landmarks, _ = x.shape\n",
    "        \n",
    "        # Reshape to process each frame separately\n",
    "        # [batch_size * seq_len, 20, input_dim]\n",
    "        x_reshaped = x.reshape(-1, num_landmarks, self.input_dim)\n",
    "        \n",
    "        # Apply input projection if needed\n",
    "        if self.input_projection is not None:\n",
    "            x_reshaped = self.input_projection(x_reshaped)\n",
    "        \n",
    "        # Process through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x_reshaped = layer(x_reshaped)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        x_reshaped = self.norm(x_reshaped)\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        # [batch_size, seq_len, 20, hidden_dim]\n",
    "        output = x_reshaped.reshape(batch_size, seq_len, num_landmarks, self.hidden_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class LandmarkTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer encoder layer for landmark processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, prenorm=True, activation='gelu'):\n",
    "        \"\"\"\n",
    "        Initialize a transformer encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            hidden_dim: Hidden dimension size\n",
    "            num_heads: Number of attention heads\n",
    "            ff_dim: Feed-forward dimension\n",
    "            prenorm: Whether to use pre-norm (True) or post-norm (False)\n",
    "            activation: Activation function in feed-forward network\n",
    "        \"\"\"\n",
    "        super(LandmarkTransformerLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prenorm = prenorm\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, ff_dim),\n",
    "            self._get_activation(activation),\n",
    "            nn.Linear(ff_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalizations\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def _get_activation(self, name):\n",
    "        \"\"\"Get activation function by name.\"\"\"\n",
    "        if name.lower() == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif name.lower() == 'gelu':\n",
    "            return nn.GELU()\n",
    "        elif name.lower() == 'silu' or name.lower() == 'swish':\n",
    "            return nn.SiLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{name}' not supported.\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process landmarks through a transformer layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size*seq_len, 20, hidden_dim]\n",
    "               representing landmarks in a single frame\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of same shape with contextualized representations\n",
    "        \"\"\"\n",
    "        # Pre-norm or post-norm architecture\n",
    "        if self.prenorm:\n",
    "            # Pre-norm: Apply normalization before attention\n",
    "            norm_x = self.norm1(x)\n",
    "            attn_output, _ = self.self_attention(norm_x, norm_x, norm_x)\n",
    "            x = x + attn_output  # Residual connection\n",
    "            \n",
    "            # Feed-forward with normalization\n",
    "            norm_x = self.norm2(x)\n",
    "            ff_output = self.ff_network(norm_x)\n",
    "            x = x + ff_output  # Residual connection\n",
    "        else:\n",
    "            # Post-norm: Apply attention then normalization\n",
    "            attn_output, _ = self.self_attention(x, x, x)\n",
    "            x = self.norm1(x + attn_output)  # Residual connection and norm\n",
    "            \n",
    "            # Feed-forward and normalization\n",
    "            ff_output = self.ff_network(x)\n",
    "            x = self.norm2(x + ff_output)  # Residual connection and norm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "709d6f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 20, 108])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_landmarks_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "763db5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_transformer = LandmarkTransformerEncoder(input_dim=3 * embedding_dim,num_layers=4,num_heads=8,hidden_dim=256,activation='gelu',prenorm=True)\n",
    "non_dom_transformer = LandmarkTransformerEncoder(input_dim=3 * embedding_dim,num_layers=4,num_heads=8,hidden_dim=256,activation='gelu',prenorm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e9df3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_contextualized = dom_transformer(dom_landmarks_conc)\n",
    "non_dom_contextualized=non_dom_transformer(non_dom_landmarks_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "bf1456be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 20, 256])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_contextualized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ad1f4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LandmarkAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies attention pooling over landmarks using PyTorch's MultiheadAttention.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the attention pooling module.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input features per landmark\n",
    "            output_dim: Dimension of the output representation\n",
    "        \"\"\"\n",
    "        super(LandmarkAttentionPooling, self).__init__()\n",
    "        \n",
    "        # Using PyTorch's built-in attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=input_dim,\n",
    "            num_heads=1,  # Single head is sufficient for pooling\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Learnable query vector\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        # Layer normalization for stability\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply attention pooling over landmarks.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, seq_len, num_landmarks, input_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, output_dim]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, num_landmarks, input_dim = x.shape\n",
    "        \n",
    "        # Reshape to process each sequence element separately\n",
    "        x_reshaped = x.reshape(batch_size * seq_len, num_landmarks, input_dim)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x_norm = self.layer_norm(x_reshaped)\n",
    "        \n",
    "        # Expand query to match the batch size\n",
    "        query = self.query.expand(batch_size * seq_len, -1, -1)\n",
    "        \n",
    "        # Apply attention\n",
    "        # The query attends to all landmarks (keys and values are the same: x_norm)\n",
    "        pooled, _ = self.attention(query, x_norm, x_norm)\n",
    "        \n",
    "        # Remove the sequence dimension (which is 1 for the query)\n",
    "        pooled = pooled.squeeze(1)  # [batch_size * seq_len, input_dim]\n",
    "        \n",
    "        # Project to output dimension\n",
    "        output = self.output_projection(pooled)  # [batch_size * seq_len, output_dim]\n",
    "        \n",
    "        # Reshape back to [batch_size, seq_len, output_dim]\n",
    "        output = output.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "95313c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_pooling = LandmarkAttentionPooling(input_dim=dom_contextualized.shape[-1],output_dim=256)\n",
    "non_dom_pooling = LandmarkAttentionPooling(input_dim=non_dom_contextualized.shape[-1],output_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "5b229e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_pooled = dom_pooling(dom_contextualized)\n",
    "non_dom_pooled = non_dom_pooling(non_dom_contextualized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "6e260fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 2, 108])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrists_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "07b67df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 108])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrists_conc[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "b3668234",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_wrist_conc = wrists_conc[:,:,0]\n",
    "non_dom_wrist_conc = wrists_conc[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "b7b9c17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 256])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "b6d29278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_pooled_wrists(pooled, wrist):\n",
    "# Verify that the shapes are compatible for concatenation\n",
    "    assert pooled.shape[:-1] == wrist.shape[:-1], \\\n",
    "        \"Batch dimensions of embeddings and spatial features must match\"\n",
    "    \n",
    "    # Concatenate along the last dimension\n",
    "    combined_features = torch.cat([\n",
    "        pooled,     # Wrist identity (what)\n",
    "        wrist   # Wrist position (where)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "701c84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_spatial_combined = concat_pooled_wrists(pooled=dom_pooled, wrist=dom_wrist_conc)\n",
    "non_dom_spatial_combined = concat_pooled_wrists(pooled=non_dom_pooled, wrist=non_dom_wrist_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "5b06e434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 364])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_spatial_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "84b168bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 20, 180])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_landmarks_velocity_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "041758a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_vel_transformer = LandmarkTransformerEncoder(input_dim=dom_landmarks_velocity_conc.shape[-1],num_layers=4,num_heads=8,hidden_dim=256,activation='gelu',prenorm=True)\n",
    "non_dom_vel_transformer = LandmarkTransformerEncoder(input_dim=dom_landmarks_velocity_conc.shape[-1],num_layers=4,num_heads=8,hidden_dim=256,activation='gelu',prenorm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "75cc2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_vel_contextualized = dom_vel_transformer(dom_landmarks_velocity_conc)\n",
    "non_dom_vel_contextualized=non_dom_vel_transformer(non_dom_landmarks_velocity_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "e5f54c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_vel_pooling = LandmarkAttentionPooling(input_dim=dom_vel_contextualized.shape[-1],output_dim=256)\n",
    "non_dom_vel_pooling = LandmarkAttentionPooling(input_dim=non_dom_vel_contextualized.shape[-1],output_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "a0ed4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_vel_pooled = dom_vel_pooling(dom_vel_contextualized)\n",
    "non_dom_vel_pooled = non_dom_vel_pooling(non_dom_vel_contextualized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "6b267525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 256])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_vel_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "9136301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_wrist_vel_conc = wrists_vel_conc[:,:,0]\n",
    "non_dom_wrist_vel_conc = wrists_vel_conc[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "438ac900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 180])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_wrist_vel_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "06629e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_velocity_combined = concat_pooled_wrists(pooled=dom_vel_pooled, wrist=dom_wrist_vel_conc)\n",
    "non_dom_velocity_combined = concat_pooled_wrists(pooled=non_dom_vel_pooled, wrist=non_dom_wrist_vel_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "902f4193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 436])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_velocity_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "5e40cb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 364])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_spatial_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "db7f12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_combined = concat_pooled_wrists(dom_spatial_combined, dom_velocity_combined)\n",
    "non_dom_combined = concat_pooled_wrists(non_dom_spatial_combined, non_dom_velocity_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "a502bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 800])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "ccc0778b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 2, 800])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hands_combined = torch.stack([dom_combined, non_dom_combined], dim=2)\n",
    "hands_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "5b35ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfidenceWeightedTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder that incorporates confidence scores into attention calculations.\n",
    "    \n",
    "    This second-stage transformer learns relationships between the two hands while\n",
    "    taking into account confidence and interpolation scores from both spatial and\n",
    "    velocity features.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 num_layers=2,\n",
    "                 num_heads=8,\n",
    "                 hidden_dim=None,\n",
    "                 ff_dim=None,\n",
    "                 prenorm=True,\n",
    "                 activation='gelu',\n",
    "                 init_method='xavier_uniform',\n",
    "                 init_gain=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the confidence-weighted transformer encoder.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input features per hand\n",
    "            num_layers: Number of transformer encoder layers\n",
    "            num_heads: Number of attention heads\n",
    "            hidden_dim: Hidden dimension size (if None, uses input_dim)\n",
    "            ff_dim: Feed-forward dimension (if None, uses 4*hidden_dim)\n",
    "            prenorm: Whether to use pre-norm (True) or post-norm (False) architecture\n",
    "            activation: Activation function in feed-forward network\n",
    "            init_method: Weight initialization method\n",
    "            init_gain: Gain parameter for initialization\n",
    "        \"\"\"\n",
    "        super(ConfidenceWeightedTransformerEncoder, self).__init__()\n",
    "        \n",
    "        # Set dimensions\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim if hidden_dim is not None else input_dim\n",
    "        self.ff_dim = ff_dim if ff_dim is not None else 4 * self.hidden_dim\n",
    "        \n",
    "        # Input projection if needed\n",
    "        self.input_projection = None\n",
    "        if self.input_dim != self.hidden_dim:\n",
    "            self.input_projection = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        \n",
    "        # Create transformer encoder layers with confidence weighting\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(\n",
    "                ConfidenceWeightedTransformerLayer(\n",
    "                    hidden_dim=self.hidden_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    ff_dim=self.ff_dim,\n",
    "                    prenorm=prenorm,\n",
    "                    activation=activation\n",
    "                )\n",
    "            )\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "        # Final normalization\n",
    "        self.norm = nn.LayerNorm(self.hidden_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights(init_method, init_gain)\n",
    "    \n",
    "    def _init_weights(self, init_method, gain):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if init_method == 'xavier_uniform':\n",
    "                    nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "                elif init_method == 'xavier_normal':\n",
    "                    nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "                elif init_method == 'kaiming_uniform':\n",
    "                    nn.init.kaiming_uniform_(module.weight, a=0, mode='fan_in')\n",
    "                elif init_method == 'kaiming_normal':\n",
    "                    nn.init.kaiming_normal_(module.weight, a=0, mode='fan_in')\n",
    "                else:\n",
    "                    raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "                \n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x, confidence_scores):\n",
    "        \"\"\"\n",
    "        Process hand features through the transformer with confidence weighting.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, seq_len, 2, input_dim]\n",
    "               where 2 represents the dom and non-dom hands\n",
    "            confidence_scores: Dictionary containing:\n",
    "                - Cd_spatial: [batch_size, seq_len, 2] confidence scores\n",
    "                - Ci_spatial: [batch_size, seq_len, 2] interpolation scores\n",
    "                - Cd_velocity: [batch_size, seq_len, 2] velocity calculation confidence\n",
    "                - Ci_velocity: [batch_size, seq_len, 2] velocity confidence\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, 2, hidden_dim]\n",
    "            with confidence-weighted contextual representations\n",
    "        \"\"\"\n",
    "        # Get original shape\n",
    "        batch_size, seq_len, num_hands, _ = x.shape\n",
    "        \n",
    "        # Reshape to process each frame separately\n",
    "        # [batch_size * seq_len, 2, input_dim]\n",
    "        x_reshaped = x.reshape(-1, num_hands, self.input_dim)\n",
    "        \n",
    "        # Apply input projection if needed\n",
    "        if self.input_projection is not None:\n",
    "            x_reshaped = self.input_projection(x_reshaped)\n",
    "        \n",
    "        # Reshape confidence scores for per-frame processing\n",
    "        conf_scores_reshaped = {}\n",
    "        for key, tensor in confidence_scores.items():\n",
    "            conf_scores_reshaped[key] = tensor.reshape(-1, num_hands)\n",
    "        \n",
    "        # Process through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x_reshaped = layer(x_reshaped, conf_scores_reshaped)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        x_reshaped = self.norm(x_reshaped)\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        # [batch_size, seq_len, 2, hidden_dim]\n",
    "        output = x_reshaped.reshape(batch_size, seq_len, num_hands, self.hidden_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class ConfidenceWeightedTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder layer with confidence-weighted attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, prenorm=True, activation='gelu'):\n",
    "        \"\"\"\n",
    "        Initialize a confidence-weighted transformer encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            hidden_dim: Hidden dimension size\n",
    "            num_heads: Number of attention heads\n",
    "            ff_dim: Feed-forward dimension\n",
    "            prenorm: Whether to use pre-norm (True) or post-norm (False)\n",
    "            activation: Activation function in feed-forward network\n",
    "        \"\"\"\n",
    "        super(ConfidenceWeightedTransformerLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prenorm = prenorm\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Custom attention with confidence weighting\n",
    "        self.self_attention = ConfidenceWeightedAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, ff_dim),\n",
    "            self._get_activation(activation),\n",
    "            nn.Linear(ff_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalizations\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def _get_activation(self, name):\n",
    "        \"\"\"Get activation function by name.\"\"\"\n",
    "        if name.lower() == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif name.lower() == 'gelu':\n",
    "            return nn.GELU()\n",
    "        elif name.lower() == 'silu' or name.lower() == 'swish':\n",
    "            return nn.SiLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{name}' not supported.\")\n",
    "    \n",
    "    def forward(self, x, confidence_scores):\n",
    "        \"\"\"\n",
    "        Process through a transformer layer with confidence-weighted attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size*seq_len, 2, hidden_dim]\n",
    "            confidence_scores: Dictionary of confidence scores\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of same shape with contextualized representations\n",
    "        \"\"\"\n",
    "        # Pre-norm or post-norm architecture\n",
    "        if self.prenorm:\n",
    "            # Pre-norm: Apply normalization before attention\n",
    "            norm_x = self.norm1(x)\n",
    "            attn_output = self.self_attention(norm_x, norm_x, norm_x, confidence_scores)\n",
    "            x = x + attn_output  # Residual connection\n",
    "            \n",
    "            # Feed-forward with normalization\n",
    "            norm_x = self.norm2(x)\n",
    "            ff_output = self.ff_network(norm_x)\n",
    "            x = x + ff_output  # Residual connection\n",
    "        else:\n",
    "            # Post-norm: Apply attention then normalization\n",
    "            attn_output = self.self_attention(x, x, x, confidence_scores)\n",
    "            x = self.norm1(x + attn_output)  # Residual connection and norm\n",
    "            \n",
    "            # Feed-forward and normalization\n",
    "            ff_output = self.ff_network(x)\n",
    "            x = self.norm2(x + ff_output)  # Residual connection and norm\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ConfidenceWeightedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention with confidence weighting.\n",
    "    \n",
    "    This applies the formula:\n",
    "    Attention(Q,K,V,Cd_spatial,Ci_spatial,Cd_velocity,Ci_velocity) = \n",
    "        softmax(QK^T/sqrt(dk) + f(Cd_spatial,Ci_spatial,Cd_velocity,Ci_velocity))V\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(ConfidenceWeightedAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Learnable parameters for confidence weighting\n",
    "        self.a = nn.Parameter(torch.zeros(1))  # For Cd_spatial\n",
    "        self.b = nn.Parameter(torch.zeros(1))  # For Cd_velocity\n",
    "        self.c = nn.Parameter(torch.zeros(1))  # For Ci_spatial\n",
    "        self.d = nn.Parameter(torch.zeros(1))  # For Ci_velocity\n",
    "        \n",
    "        # Small epsilon to avoid log(0)\n",
    "        self.epsilon = 0.01\n",
    "    \n",
    "  \n",
    "    def compute_confidence_weights(self, confidence_scores):\n",
    "        \"\"\"\n",
    "        Compute the confidence weighting matrix f(Cd_spatial, Ci_spatial, Cd_velocity, Ci_velocity).\n",
    "\n",
    "        Args:\n",
    "            confidence_scores: Dictionary with confidence score tensors of shape [flattened_batch_size, 2]\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [flattened_batch_size, 2, 2] for weighting attention scores\n",
    "        \"\"\"\n",
    "        Cd_spatial = confidence_scores['Cd_spatial']\n",
    "        Ci_spatial = confidence_scores['Ci_spatial']\n",
    "        Cd_velocity = confidence_scores['Cd_velocity']\n",
    "        Ci_velocity = confidence_scores['Ci_velocity']\n",
    "\n",
    "        # These tensors have shape [flattened_batch_size, 2]\n",
    "        flattened_batch_size, num_hands = Cd_spatial.shape\n",
    "\n",
    "        # Apply the confidence weighting formula\n",
    "        f_values = (\n",
    "            torch.log2(self.epsilon + Cd_spatial) * torch.sigmoid(self.a) * 0.25 +\n",
    "            torch.log2(self.epsilon + Cd_velocity) * torch.sigmoid(self.b) * 0.25 +\n",
    "            torch.log2(self.epsilon + Ci_spatial) * torch.sigmoid(self.c) * 0.5 +\n",
    "            torch.log2(self.epsilon + Ci_velocity) * torch.sigmoid(self.d) * 0.5\n",
    "        )\n",
    "    \n",
    "        \n",
    "        # Create the 2x2 matrix for each batch item where columns have same values\n",
    "        confidence_matrix = f_values.unsqueeze(1).expand(-1, num_hands, -1)\n",
    "        \n",
    "        \n",
    "        return confidence_matrix\n",
    "    \n",
    "    def forward(self, query, key, value, confidence_scores):\n",
    "        \"\"\"\n",
    "        Apply confidence-weighted attention.\n",
    "        \n",
    "        Args:\n",
    "            query, key, value: Tensors of shape [batch_size*seq_len, num_hands, embed_dim]\n",
    "                              where batch_size*seq_len represents flattened batch and sequence dimensions\n",
    "            confidence_scores: Dictionary of confidence scores\n",
    "            \n",
    "        Returns:\n",
    "            Attention output tensor of same shape\n",
    "        \"\"\"\n",
    "        # Get the shape components - note there's no separate sequence dimension here!\n",
    "        flattened_batch_size, num_hands, embed_dim = query.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.q_proj(query)  # [flattened_batch_size, num_hands, embed_dim]\n",
    "        k = self.k_proj(key)    # [flattened_batch_size, num_hands, embed_dim]\n",
    "        v = self.v_proj(value)  # [flattened_batch_size, num_hands, embed_dim]\n",
    "        \n",
    "        # Compute confidence weights\n",
    "        # This should return: [flattened_batch_size, num_hands, num_hands]\n",
    "        confidence_weights = self.compute_confidence_weights(confidence_scores)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # Split embed_dim into num_heads × head_dim\n",
    "        q = q.reshape(flattened_batch_size, num_hands, self.num_heads, self.head_dim)\n",
    "        q = q.permute(0, 2, 1, 3)  # [flattened_batch_size, num_heads, num_hands, head_dim]\n",
    "        \n",
    "        k = k.reshape(flattened_batch_size, num_hands, self.num_heads, self.head_dim)\n",
    "        k = k.permute(0, 2, 1, 3)  # [flattened_batch_size, num_heads, num_hands, head_dim]\n",
    "        \n",
    "        v = v.reshape(flattened_batch_size, num_hands, self.num_heads, self.head_dim)\n",
    "        v = v.permute(0, 2, 1, 3)  # [flattened_batch_size, num_heads, num_hands, head_dim]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        # [flattened_batch_size, num_heads, num_hands, num_hands]\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Add confidence weights to attention scores\n",
    "        # Expand confidence_weights for all heads\n",
    "        # [flattened_batch_size, 1, num_hands, num_hands]\n",
    "        confidence_weights = confidence_weights.unsqueeze(1)\n",
    "        \n",
    "        # Add confidence weights to attention scores\n",
    "        attention_scores = attention_scores + confidence_weights\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # [flattened_batch_size, num_heads, num_hands, head_dim]\n",
    "        context = torch.matmul(attention_probs, v)\n",
    "        \n",
    "        # Reshape back\n",
    "        context = context.permute(0, 2, 1, 3)  # [flattened_batch_size, num_hands, num_heads, head_dim]\n",
    "        context = context.reshape(flattened_batch_size, num_hands, embed_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.out_proj(context)  # [flattened_batch_size, num_hands, embed_dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "503f4ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_hand_transformer = ConfidenceWeightedTransformerEncoder(\n",
    "    input_dim=hands_combined.shape[-1],\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    hidden_dim=hands_combined.shape[-1],\n",
    "    prenorm=True,\n",
    "    activation='gelu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "873fd66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_scores = {\n",
    "    'Cd_spatial': batch['confidence_scores'],\n",
    "    'Ci_spatial': batch['interpolation_scores'],\n",
    "    'Cd_velocity': batch['velocity_calculation_confidence'],\n",
    "    'Ci_velocity': batch['velocity_confidence']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "12fe1185",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_hands = cross_hand_transformer(hands_combined, confidence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f4569913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 59, 2, 800])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_hands.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "36a370b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pooling = LandmarkAttentionPooling(\n",
    "    input_dim=enhanced_hands.shape[-1],\n",
    "    output_dim=enhanced_hands.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "43b51d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 800])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pooling.query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "b1321e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hands_representation = final_pooling(enhanced_hands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "c3b1a84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 800])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hands_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "7a109864",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_representation = concat_pooled_wrists(final_hands_representation, blendshapes_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "8faf56ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36, 872])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "1791d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDownsampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduces frame count using 1D convolution with configurable parameters.\n",
    "    \n",
    "    This module applies a 1D convolution across the temporal dimension,\n",
    "    effectively reducing the number of frames while preserving important\n",
    "    temporal information through learned filters.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_channels=None, \n",
    "                 kernel_size=3, \n",
    "                 stride=2,\n",
    "                 activation='relu',\n",
    "                 norm_layer=True):\n",
    "        \"\"\"\n",
    "        Initialize the temporal downsampler.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input dimension (d) - feature size per frame\n",
    "            output_channels: Number of convolutional filters (C), defaults to input_dim\n",
    "            kernel_size: Size of the convolutional kernel (k)\n",
    "            stride: Stride of the convolution, controls downsampling factor\n",
    "            activation: Activation function ('relu', 'gelu', None)\n",
    "            norm_layer: Whether to include layer normalization after convolution\n",
    "        \"\"\"\n",
    "        super(TemporalDownsampler, self).__init__()\n",
    "        \n",
    "        # Default output channels to input dimension if not specified\n",
    "        self.output_channels = input_dim if output_channels is None else output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Calculate padding to maintain temporal alignment\n",
    "        # For even kernel sizes, we'll use asymmetric padding later\n",
    "        self.padding = (kernel_size - 1) // 2\n",
    "        self.is_even_kernel = (kernel_size % 2 == 0)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=self.output_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=self.padding,  # This will be adjusted for even kernels\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(self.output_channels) if norm_layer else None\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation is None:\n",
    "            self.activation = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply temporal downsampling to the input sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, n_frames, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, n_frames/stride, output_channels]\n",
    "        \"\"\"\n",
    "        batch_size, n_frames, input_dim = x.shape\n",
    "        \n",
    "        # Reshape for conv1d which expects [batch_size, channels, length]\n",
    "        x = x.permute(0, 2, 1)  # -> [batch_size, input_dim, n_frames]\n",
    "        \n",
    "        # Handle even-sized kernels with asymmetric padding if needed\n",
    "        if self.is_even_kernel:\n",
    "            # For even kernels, PyTorch padding is not symmetric\n",
    "            # We'll pad manually to handle this\n",
    "            pad_size = (self.kernel_size - 1) // 2\n",
    "            x = nn.functional.pad(x, (pad_size, pad_size+1), mode='constant', value=0)\n",
    "            \n",
    "        # Apply convolution\n",
    "        x = self.conv(x)  # -> [batch_size, output_channels, n_frames/stride]\n",
    "        \n",
    "        # Reshape back to [batch_size, n_frames/stride, output_channels]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply normalization if specified\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        # Apply activation if specified\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_length):\n",
    "        \"\"\"\n",
    "        Calculate the output sequence length given the input length.\n",
    "        \n",
    "        Args:\n",
    "            input_length: Length of the input sequence (n_frames)\n",
    "            \n",
    "        Returns:\n",
    "            Length of the output sequence\n",
    "        \"\"\"\n",
    "        # For even kernels with our manual padding\n",
    "        if self.is_even_kernel:\n",
    "            padding = self.padding + 1\n",
    "        else:\n",
    "            padding = self.padding\n",
    "        \n",
    "        # Standard formula for conv output shape\n",
    "        return math.floor((input_length + 2 * padding - self.kernel_size) / self.stride + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "18c7810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d = TemporalDownsampler(\n",
    "    input_dim=frame_representation.shape[-1],          # Feature dimension (d)\n",
    "    output_channels=768,    # Number of filters (C), same as input to preserve dimension\n",
    "    kernel_size=5,          # Kernel size (k)\n",
    "    stride=2,               # Stride for downsampling\n",
    "    activation='relu',      # Activation function\n",
    "    norm_layer=True         # Include layer normalization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "26387256",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_representation = conv1d(frame_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "2fa87200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 768])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "ccbcb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create fixed positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x, scale=1.0):\n",
    "        \"\"\"\n",
    "        Add positional encodings to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "            scale: Scaling factor for the positional encodings\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with added positional encodings\n",
    "        \"\"\"\n",
    "        x = x + (self.pe[:, :x.size(1), :] * scale)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "a2fbb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoder = PositionalEncoding(\n",
    "    d_model=downsampled_representation.shape[-1],  # Feature dimension\n",
    "    max_len=120  # Add some buffer for sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "f07cd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_with_positional_encoding = positional_encoder(downsampled_representation, scale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e4b6d481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 768])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_with_positional_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "a986b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleTemporalTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer that processes sequences with multi-scale temporal attention.\n",
    "    \n",
    "    Uses exactly three attention heads:\n",
    "    - Short-term head: Attends to frames within ±5 frames\n",
    "    - Medium-term head: Attends to frames within ±15 frames\n",
    "    - Long-term head: Attends to frames within ±45 frames\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 num_layers=4,\n",
    "                 short_range=5,\n",
    "                 medium_range=15,\n",
    "                 long_range=45,\n",
    "                 dim_feedforward=2048,\n",
    "                 activation='gelu',\n",
    "                 stride=2):  # Add stride parameter\n",
    "        \"\"\"\n",
    "        Initialize the multi-scale temporal transformer.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension / feature size\n",
    "            num_layers: Number of transformer encoder layers\n",
    "            short_range: Range for short-term attention (±frames)\n",
    "            medium_range: Range for medium-term attention (±frames)\n",
    "            long_range: Range for long-term attention (±frames)\n",
    "            dim_feedforward: Dimension of feedforward network\n",
    "            activation: Activation function type\n",
    "            stride: Stride used in downsampling (needed for mask adjustment)\n",
    "        \"\"\"\n",
    "        super(MultiScaleTemporalTransformer, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.d_model = d_model\n",
    "        self.total_heads = 3  # Exactly 3 heads\n",
    "        self.head_ranges = {\n",
    "            'short': short_range,\n",
    "            'medium': medium_range,\n",
    "            'long': long_range\n",
    "        }\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Create transformer layers\n",
    "        encoder_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            encoder_layers.append(\n",
    "                MultiScaleTransformerEncoderLayer(\n",
    "                    d_model=d_model,\n",
    "                    head_ranges=self.head_ranges,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    activation=activation\n",
    "                )\n",
    "            )\n",
    "        self.layers = nn.ModuleList(encoder_layers)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, src, mask=None):\n",
    "        \"\"\"\n",
    "        Process the input sequence through the transformer.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor [batch_size, seq_len_downsampled, d_model]\n",
    "            mask: Boolean mask [batch_size, seq_len_original] where True indicates valid frames\n",
    "                 and False indicates padding frames\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of same shape as input with multi-scale temporal context\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        \n",
    "        # Adjust mask for downsampled sequence length\n",
    "        if mask is not None:\n",
    "            # Subsample the mask to match downsampled sequence\n",
    "            # Take every stride-th element, starting from 0\n",
    "            # This accounts for how conv1d downsampling affects the sequence length\n",
    "            downsample_mask = mask[:, ::self.stride]\n",
    "            \n",
    "            # Make sure downsampled mask matches sequence length\n",
    "            # It might be off by 1 due to padding in conv1d\n",
    "            if downsample_mask.shape[1] > src.shape[1]:\n",
    "                downsample_mask = downsample_mask[:, :src.shape[1]]\n",
    "            elif downsample_mask.shape[1] < src.shape[1]:\n",
    "                # This shouldn't normally happen, but just in case\n",
    "                pad_size = src.shape[1] - downsample_mask.shape[1]\n",
    "                pad = torch.zeros((downsample_mask.shape[0], pad_size), dtype=torch.bool, device=mask.device)\n",
    "                downsample_mask = torch.cat([downsample_mask, pad], dim=1)\n",
    "            \n",
    "            # Convert from True=valid to True=padding format used by transformer\n",
    "            padding_mask = ~downsample_mask\n",
    "        else:\n",
    "            padding_mask = None\n",
    "        \n",
    "        # Pass through each transformer layer\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, padding_mask=padding_mask)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiScaleTransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder layer with multi-scale temporal attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 head_ranges,\n",
    "                 dim_feedforward=2048, \n",
    "                 activation=\"gelu\"):\n",
    "        super(MultiScaleTransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-scale attention\n",
    "        self.self_attn = MultiScaleAttention(\n",
    "            embed_dim=d_model,\n",
    "            head_ranges=head_ranges\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = F.gelu\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    \n",
    "    def forward(self, src, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor [batch_size, seq_len_downsampled, d_model]\n",
    "            padding_mask: Boolean mask [batch_size, seq_len_downsampled] \n",
    "                         where True indicates padding\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of the same shape\n",
    "        \"\"\"\n",
    "        # Multi-scale attention with residual connection\n",
    "        src2 = self.norm1(src)\n",
    "        src2 = self.self_attn(src2, src2, src2, padding_mask=padding_mask)\n",
    "        src = src + src2\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.activation(self.linear1(src2)))\n",
    "        src = src + src2\n",
    "        \n",
    "        return src\n",
    "\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention where different heads attend to different temporal ranges.\n",
    "    Uses exactly 3 heads: short, medium, and long-term.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, head_ranges):\n",
    "        super(MultiScaleAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_ranges = head_ranges\n",
    "        self.total_heads = 3  # Fixed: one head per range\n",
    "        \n",
    "        assert embed_dim % self.total_heads == 0, \"embed_dim must be divisible by 3\"\n",
    "        self.head_dim = embed_dim // self.total_heads\n",
    "        \n",
    "        # Create linear projections\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Head indices (fixed for 3 heads)\n",
    "        self.head_indices = {\n",
    "            'short': (0, 1),\n",
    "            'medium': (1, 2),\n",
    "            'long': (2, 3)\n",
    "        }\n",
    "    \n",
    "    def forward(self, query, key, value, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Apply multi-scale attention.\n",
    "        \n",
    "        Args:\n",
    "            query, key, value: Input tensors [batch_size, seq_len, embed_dim]\n",
    "            padding_mask: Boolean mask [batch_size, seq_len] where True indicates padding frames\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len, _ = query.shape\n",
    "        src_len = key.shape[1]\n",
    "        \n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        q = self.q_proj(query).view(batch_size, tgt_len, self.total_heads, self.head_dim)\n",
    "        k = self.k_proj(key).view(batch_size, src_len, self.total_heads, self.head_dim)\n",
    "        v = self.v_proj(value).view(batch_size, src_len, self.total_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q = q.transpose(1, 2)  # [batch_size, total_heads, tgt_len, head_dim]\n",
    "        k = k.transpose(1, 2)  # [batch_size, total_heads, src_len, head_dim]\n",
    "        v = v.transpose(1, 2)  # [batch_size, total_heads, src_len, head_dim]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_output = self._multi_scale_attention(q, k, v, tgt_len, src_len, padding_mask)\n",
    "        \n",
    "        # Reshape and apply final projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, tgt_len, self.embed_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _multi_scale_attention(self, q, k, v, tgt_len, src_len, padding_mask):\n",
    "        \"\"\"\n",
    "        Apply attention with different temporal ranges for different heads.\n",
    "        \"\"\"\n",
    "        # Compute scaled dot-product attention\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Create temporal range masks for each head type\n",
    "        temporal_masks = self._create_temporal_masks(tgt_len, src_len, device=q.device)\n",
    "        \n",
    "        # Apply padding mask if provided\n",
    "        if padding_mask is not None:\n",
    "            # Convert mask from [batch_size, seq_len] to [batch_size, 1, 1, seq_len]\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            # True values are masked positions (set to -inf)\n",
    "            attn_weights = attn_weights.masked_fill(padding_mask, float('-inf'))\n",
    "        \n",
    "        # Apply the temporal masks\n",
    "        for scale, (start_idx, end_idx) in self.head_indices.items():\n",
    "            mask = temporal_masks[scale]\n",
    "            attn_weights[:, start_idx:end_idx] = attn_weights[:, start_idx:end_idx].masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax (no dropout)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _create_temporal_masks(self, tgt_len, src_len, device):\n",
    "        \"\"\"\n",
    "        Create masks to restrict attention to specific temporal ranges.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of masks for each head type\n",
    "        \"\"\"\n",
    "        temporal_masks = {}\n",
    "        \n",
    "        # Create position indices\n",
    "        pos_i = torch.arange(tgt_len, device=device).unsqueeze(1)\n",
    "        pos_j = torch.arange(src_len, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Calculate distance between positions\n",
    "        dist = torch.abs(pos_i - pos_j)  # [tgt_len, src_len]\n",
    "        \n",
    "        # Create masks for each temporal range\n",
    "        for scale, range_val in self.head_ranges.items():\n",
    "            # True where attention should be blocked (outside of the range)\n",
    "            mask = dist > range_val\n",
    "            # Expand for batch dimension and appropriate number of heads\n",
    "            # Shape: [1, 1, tgt_len, src_len]\n",
    "            temporal_masks[scale] = mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        return temporal_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "0dd46234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_with_positional_encoding.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "fdcb9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "temporal_transformer = MultiScaleTemporalTransformer(\n",
    "    d_model=downsampled_with_positional_encoding.shape[-1],\n",
    "    num_layers=4,\n",
    "    short_range=5,\n",
    "    medium_range=15,\n",
    "    long_range=45,\n",
    "    dim_feedforward=2 * downsampled_with_positional_encoding.shape[-1],\n",
    "    activation='gelu',\n",
    "    stride=2  \n",
    ")\n",
    "\n",
    "\n",
    "multi_scale_representation = temporal_transformer(downsampled_with_positional_encoding, mask=batch['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "b2d2478b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 768])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_scale_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "b74388be",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_scale_representation_reinforced = positional_encoder(multi_scale_representation, scale=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "b92d5fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 768])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_scale_representation_reinforced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "899a1e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_contextualized.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "1708ec35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_vel_contextualized.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "f1c05f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dom_vel_transformer.hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd879d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (float, int), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 49\u001b[0m\n\u001b[1;32m     35\u001b[0m final_pooling \u001b[38;5;241m=\u001b[39m LandmarkAttentionPooling(\n\u001b[1;32m     36\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39menhanced_hands\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     37\u001b[0m     output_dim\u001b[38;5;241m=\u001b[39menhanced_hands\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     39\u001b[0m conv1d \u001b[38;5;241m=\u001b[39m TemporalDownsampler(\n\u001b[1;32m     40\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39mframe_representation\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],          \u001b[38;5;66;03m# Feature dimension (d)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m,    \u001b[38;5;66;03m# Number of filters (C)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     norm_layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m         \u001b[38;5;66;03m# Include layer normalization\u001b[39;00m\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 49\u001b[0m positional_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownsampled_representation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Feature dimension\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# max_frames\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m temporal_transformer \u001b[38;5;241m=\u001b[39m MultiScaleTemporalTransformer(\n\u001b[1;32m     57\u001b[0m     d_model\u001b[38;5;241m=\u001b[39mdownsampled_with_positional_encoding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     58\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m  \n\u001b[1;32m     65\u001b[0m )\n",
      "Cell \u001b[0;32mIn[84], line 6\u001b[0m, in \u001b[0;36mPositionalEncoding.__init__\u001b[0;34m(self, d_model, max_len)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create fixed positional encodings\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m pe \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, max_len, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m div_term \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, d_model, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m10000.0\u001b[39m) \u001b[38;5;241m/\u001b[39m d_model))\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (float, int), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 36\n",
    "embedding_table = LandmarkEmbedding(embedding_dim=embedding_dim, num_landmarks_per_hand=21)\n",
    "ladmark_encoder = LandmarkSpatialEncoder(embedding_dim, hidden_dims=[30, 60, 30],activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')\n",
    "\n",
    "wrist_encoder = WristSpatialEncoder(embedding_dim, hidden_dims=[30, 60, 30],activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')\n",
    "\n",
    "blendshapes_feedforward = BlendshapeEncoder(embedding_dim, hidden_dims=None, num_layers=2,activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')\n",
    "\n",
    "velocity_feedforward = VelocityEncoder(n_velocity_encoding=2*embedding_dim, hidden_dims=None, num_layers=2,activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')\n",
    "\n",
    "wrist_vel_feedforward = WristVelocityEncoder(n_velocity_encoding=2*embedding_dim, hidden_dims=None, num_layers=2,activation='relu',init_method='kaiming_normal',init_gain=1.0,init_nonlinearity='relu')\n",
    "\n",
    "dom_transformer = LandmarkTransformerEncoder(input_dim=3 * embedding_dim,num_layers=4,num_heads=8,hidden_dim=256,activation='gelu',prenorm=True)\n",
    "non_dom_transformer = LandmarkTransformerEncoder(input_dim=3 * embedding_dim,num_layers=4,num_heads=8,hidden_dim=256,activation='gelu',prenorm=True)\n",
    "\n",
    "dom_pooling = LandmarkAttentionPooling(input_dim=dom_transformer.hidden_dim,output_dim=256)\n",
    "non_dom_pooling = LandmarkAttentionPooling(input_dim=non_dom_transformer.hidden_dim,output_dim=256)\n",
    "\n",
    "dom_vel_transformer = LandmarkTransformerEncoder(input_dim=velocity_feedforward.output_dim*2+embedding_dim,num_layers=4,num_heads=8,hidden_dim=256,activation='gelu',prenorm=True)\n",
    "non_dom_vel_transformer = LandmarkTransformerEncoder(input_dim=velocity_feedforward.output_dim*2+embedding_dim,num_layers=4,num_heads=8,hidden_dim=256,activation='gelu',prenorm=True)\n",
    "\n",
    "dom_vel_pooling = LandmarkAttentionPooling(input_dim=dom_vel_transformer.hidden_dim,output_dim=256)\n",
    "non_dom_vel_pooling = LandmarkAttentionPooling(input_dim=non_dom_vel_transformer.hidden_dim,output_dim=256)\n",
    "\n",
    "cross_hand_transformer = ConfidenceWeightedTransformerEncoder(\n",
    "    input_dim=hands_combined.shape[-1],\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    hidden_dim=hands_combined.shape[-1],\n",
    "    prenorm=True,\n",
    "    activation='gelu'\n",
    ")\n",
    "\n",
    "\n",
    "final_pooling = LandmarkAttentionPooling(\n",
    "    input_dim=enhanced_hands.shape[-1],\n",
    "    output_dim=enhanced_hands.shape[-1])\n",
    "\n",
    "conv1d = TemporalDownsampler(\n",
    "    input_dim=frame_representation.shape[-1],          # Feature dimension (d)\n",
    "    output_channels=768,    # Number of filters (C)\n",
    "    kernel_size=5,          # Kernel size (k)\n",
    "    stride=2,               # Stride for downsampling\n",
    "    activation='relu',      \n",
    "    norm_layer=True         # Include layer normalization\n",
    ")\n",
    "\n",
    "\n",
    "positional_encoder = PositionalEncoding(\n",
    "    d_model=downsampled_representation.shape[-1],  # Feature dimension\n",
    "    max_len=120/2  # max_frames\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "temporal_transformer = MultiScaleTemporalTransformer(\n",
    "    d_model=downsampled_with_positional_encoding.shape[-1],\n",
    "    num_layers=4,\n",
    "    short_range=5,\n",
    "    medium_range=15,\n",
    "    long_range=45,\n",
    "    dim_feedforward=2 * downsampled_with_positional_encoding.shape[-1],\n",
    "    activation='gelu',\n",
    "    stride=2  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4fcb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding_table.forward()\n",
    "\n",
    "dom_landmarks_where = ladmark_encoder.forward(batch['dom_landmarks'])\n",
    "non_dom_landmarks_where = ladmark_encoder.forward(batch['non_dom_landmarks'])\n",
    "\n",
    "dom_landmarks_conc = combine_spatial_and_semantic_features(spatial_features=dom_landmarks_where, semantic_features=embeddings[:20])\n",
    "non_dom_landmarks_conc = combine_spatial_and_semantic_features(spatial_features=non_dom_landmarks_where, semantic_features=embeddings[21:41])\n",
    "\n",
    "wrists_where = wrist_encoder.forward(wrist_coordinates=batch['nose_to_wrist_dist'])\n",
    "\n",
    "wrists_conc = combine_wrist_embedding_and_spatial(wrist_embeddings=torch.cat([embeddings[20], embeddings[41]], dim=-1).reshape((2,-1)), wrist_spatial_features=wrists_where)\n",
    "\n",
    "blendshapes_encoded = blendshapes_feedforward(batch['blendshape_scores'])\n",
    "\n",
    "dom_small_vel_encoded = velocity_feedforward.forward(batch['dom_velocity_small']) \n",
    "dom_large_vel_encoded = velocity_feedforward.forward(batch['dom_velocity_large']) \n",
    "non_dom_small_vel_encoded = velocity_feedforward.forward(batch['non_dom_velocity_small']) \n",
    "non_dom_large_vel_encoded = velocity_feedforward.forward(batch['non_dom_velocity_large']) \n",
    "\n",
    "dom_landmarks_velocity_conc = combine_semantic_and_velocity_features(semantic_features=embeddings[:20], velocity_small_features=dom_small_vel_encoded, velocity_large_features=dom_large_vel_encoded)\n",
    "non_dom_landmarks_velocity_conc = combine_semantic_and_velocity_features(semantic_features=embeddings[21:41], velocity_small_features=non_dom_small_vel_encoded, velocity_large_features=non_dom_large_vel_encoded)\n",
    "\n",
    "wrist_vel_small_encoded = wrist_vel_feedforward.forward(batch['nose_to_wrist_velocity_small'])\n",
    "wrist_vel_large_encoded = wrist_vel_feedforward.forward(batch['nose_to_wrist_velocity_large'])\n",
    "\n",
    "wrists_vel_conc = combine_wrist_embedding_and_velocity(wrist_embeddings=torch.cat([embeddings[20], embeddings[41]], dim=-1).reshape((2,-1)), wrist_velocity_small=wrist_vel_small_encoded, wrist_velocity_large=wrist_vel_large_encoded)\n",
    "\n",
    "dom_contextualized = dom_transformer(dom_landmarks_conc)\n",
    "non_dom_contextualized=non_dom_transformer(non_dom_landmarks_conc)\n",
    "\n",
    "dom_pooled = dom_pooling(dom_contextualized)\n",
    "non_dom_pooled = non_dom_pooling(non_dom_contextualized)\n",
    "\n",
    "dom_wrist_conc = wrists_conc[:,:,0]\n",
    "non_dom_wrist_conc = wrists_conc[:,:,1]\n",
    "\n",
    "dom_spatial_combined = concat_pooled_wrists(pooled=dom_pooled, wrist=dom_wrist_conc)\n",
    "non_dom_spatial_combined = concat_pooled_wrists(pooled=non_dom_pooled, wrist=non_dom_wrist_conc)\n",
    "\n",
    "dom_vel_contextualized = dom_vel_transformer(dom_landmarks_velocity_conc)\n",
    "non_dom_vel_contextualized=non_dom_vel_transformer(non_dom_landmarks_velocity_conc)\n",
    "\n",
    "dom_vel_pooled = dom_vel_pooling(dom_vel_contextualized)\n",
    "non_dom_vel_pooled = non_dom_vel_pooling(non_dom_vel_contextualized)\n",
    "\n",
    "dom_wrist_vel_conc = wrists_vel_conc[:,:,0]\n",
    "non_dom_wrist_vel_conc = wrists_vel_conc[:,:,1]\n",
    "\n",
    "dom_velocity_combined = concat_pooled_wrists(pooled=dom_vel_pooled, wrist=dom_wrist_vel_conc)\n",
    "non_dom_velocity_combined = concat_pooled_wrists(pooled=non_dom_vel_pooled, wrist=non_dom_wrist_vel_conc)\n",
    "\n",
    "dom_combined = concat_pooled_wrists(dom_spatial_combined, dom_velocity_combined)\n",
    "non_dom_combined = concat_pooled_wrists(non_dom_spatial_combined, non_dom_velocity_combined)\n",
    "\n",
    "hands_combined = torch.stack([dom_combined, non_dom_combined], dim=2)\n",
    "\n",
    "confidence_scores = {\n",
    "    'Cd_spatial': batch['confidence_scores'],\n",
    "    'Ci_spatial': batch['interpolation_scores'],\n",
    "    'Cd_velocity': batch['velocity_calculation_confidence'],\n",
    "    'Ci_velocity': batch['velocity_confidence']\n",
    "}\n",
    "\n",
    "enhanced_hands = cross_hand_transformer(hands_combined, confidence_scores)\n",
    "\n",
    "frame_representation = concat_pooled_wrists(final_hands_representation, blendshapes_encoded)\n",
    "\n",
    "\n",
    "downsampled_representation = conv1d(frame_representation)\n",
    "\n",
    "downsampled_with_positional_encoding = positional_encoder(downsampled_representation, scale=1.0)\n",
    "\n",
    "multi_scale_representation = temporal_transformer(downsampled_with_positional_encoding, mask=batch['mask'])\n",
    "\n",
    "video_representation = positional_encoder(multi_scale_representation, scale=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ffe7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 26, 768])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "83d4fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_smoothing_loss(logits, L_index, L_values, label_mask=None):\n",
    "    \"\"\"\n",
    "    Custom loss function that supports semantic label smoothing with proper\n",
    "    handling of first token prediction.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model output logits [batch_size, seq_len, vocab_size]\n",
    "        L_index: Token indices [batch_size, max_n_tokens, 6] where each row contains \n",
    "                 the original label index and 5 semantically similar tokens\n",
    "        L_values: Token values [batch_size, max_n_tokens, 6] containing smoothed probabilities\n",
    "        label_mask: Boolean mask [batch_size, max_n_tokens] with True for valid tokens\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (total_loss, first_token_loss, next_token_loss)\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    _, max_n_tokens, k = L_index.shape\n",
    "    \n",
    "    # For first token prediction: use first position logits to predict first token\n",
    "    first_token_logits = logits[:, 0, :]  # [batch_size, vocab_size]\n",
    "    first_token_targets = L_index[:, 0, :]  # [batch_size, 6]\n",
    "    first_token_values = L_values[:, 0, :]  # [batch_size, 6]\n",
    "    \n",
    "    # For subsequent tokens: use shifted logits to predict shifted targets\n",
    "    next_token_logits = logits[:, :-1, :]  # [batch_size, seq_len-1, vocab_size]\n",
    "    next_token_targets = L_index[:, 1:, :]  # [batch_size, max_n_tokens-1, 6]\n",
    "    next_token_values = L_values[:, 1:, :]  # [batch_size, max_n_tokens-1, 6]\n",
    "    \n",
    "    if label_mask is not None:\n",
    "        first_token_mask = label_mask[:, 0]  # [batch_size]\n",
    "        next_token_mask = label_mask[:, 1:]  # [batch_size, max_n_tokens-1]\n",
    "    else:\n",
    "        first_token_mask = (first_token_targets[:, 0] != 0)\n",
    "        next_token_mask = (next_token_targets[:, :, 0] != 0)\n",
    "    \n",
    "    # Initialize loss components\n",
    "    first_token_loss = torch.zeros(batch_size, device=logits.device)\n",
    "    next_token_loss = torch.zeros(batch_size, min(next_token_targets.shape[1], next_token_logits.shape[1]), device=logits.device)\n",
    "    \n",
    "    # Calculate first token loss\n",
    "    for b in range(batch_size):\n",
    "        if first_token_mask[b]:\n",
    "            pos_loss = 0\n",
    "            for i in range(k):\n",
    "                token_idx = first_token_targets[b, i]\n",
    "                token_value = first_token_values[b, i]\n",
    "                \n",
    "                if token_idx == 0:  # Skip padding\n",
    "                    continue\n",
    "                \n",
    "                log_prob = F.log_softmax(first_token_logits[b], dim=-1)[token_idx]\n",
    "                pos_loss -= token_value * log_prob\n",
    "            \n",
    "            first_token_loss[b] = pos_loss\n",
    "    \n",
    "    # Calculate subsequent token losses\n",
    "    for b in range(batch_size):\n",
    "        for pos in range(min(next_token_targets.shape[1], next_token_logits.shape[1])):\n",
    "            if pos < next_token_mask.shape[1] and next_token_mask[b, pos]:\n",
    "                pos_loss = 0\n",
    "                for i in range(k):\n",
    "                    token_idx = next_token_targets[b, pos, i]\n",
    "                    token_value = next_token_values[b, pos, i]\n",
    "                    \n",
    "                    if token_idx == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    log_prob = F.log_softmax(next_token_logits[b, pos], dim=-1)[token_idx]\n",
    "                    pos_loss -= token_value * log_prob\n",
    "                \n",
    "                next_token_loss[b, pos] = pos_loss\n",
    "    \n",
    "    # Combine losses\n",
    "    # Count valid tokens \n",
    "    valid_first_tokens = first_token_mask.sum().clamp(min=1)\n",
    "    valid_next_tokens = 0\n",
    "    for b in range(batch_size):\n",
    "        valid_next_tokens += next_token_mask[b, :next_token_loss.shape[1]].sum()\n",
    "    valid_next_tokens = valid_next_tokens.clamp(min=1)\n",
    "    \n",
    "    # Average first token loss\n",
    "    avg_first_token_loss = first_token_loss.sum() / valid_first_tokens\n",
    "    \n",
    "    # Average next token losses\n",
    "    avg_next_token_loss = next_token_loss.sum() / valid_next_tokens\n",
    "    \n",
    "    # Combine both losses\n",
    "    total_loss = (avg_first_token_loss + avg_next_token_loss) / 2\n",
    "    \n",
    "    return total_loss, avg_first_token_loss, avg_next_token_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f531c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_minimum_loss(target_labels, label_mask=None):\n",
    "    \"\"\"\n",
    "    Compute the theoretical minimum loss (entropy of target distribution)\n",
    "    \"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    # Calculate entropy for each position: -∑(p_i * log(p_i))\n",
    "    position_entropy = -(target_labels * torch.log(target_labels + epsilon)).sum(dim=2)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if label_mask is not None:\n",
    "        position_entropy = position_entropy * label_mask.float()\n",
    "        # Average entropy over valid tokens\n",
    "        min_loss = position_entropy.sum() / label_mask.sum().clamp(min=1)\n",
    "    else:\n",
    "        # If no mask, use all tokens\n",
    "        min_loss = position_entropy.mean()\n",
    "    \n",
    "    return min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "ff18ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_semantic_smoothing_loss(logits, L_index, L_values, label_mask=None):\n",
    "    \"\"\"\n",
    "    Highly optimized semantic smoothing loss using a single scatter operation.\n",
    "    No loops needed!\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    \n",
    "    # Create target label distributions all at once\n",
    "    target_labels = torch.zeros(batch_size, seq_len, vocab_size, device=logits.device)\n",
    "    target_labels.scatter_(2, L_index, L_values)\n",
    "    \n",
    "    # Apply log_softmax to get log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Compute loss (batch_size, seq_len)\n",
    "    token_losses = -(target_labels * log_probs).sum(dim=2)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if label_mask is not None:\n",
    "        token_losses = token_losses * label_mask.float()\n",
    "        # Average loss over valid tokens\n",
    "        total_loss = token_losses.sum() / label_mask.sum().clamp(min=1)\n",
    "    else:\n",
    "        # If no mask, use all tokens\n",
    "        total_loss = token_losses.mean()\n",
    "    \n",
    "    return total_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "c3024084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 18, 768])"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_scale_representation_reinforced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "31aa2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention using PyTorch's optimized MultiheadAttention implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=768, num_heads=12):\n",
    "        super(OptimizedCrossAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # PyTorch's optimized multi-head attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True  # Important for our [batch, seq, features] format\n",
    "        )\n",
    "        \n",
    "        # Layer normalization for pre-norm architecture (like GPT-2)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, hidden_states, video_representations, video_mask=None, stride=1):\n",
    "        \"\"\"\n",
    "        Compute cross-attention between GPT token representations and video frames.\n",
    "        \"\"\"\n",
    "        # Apply layer normalization to hidden states (pre-norm approach)\n",
    "        query = self.layer_norm(hidden_states)\n",
    "        \n",
    "        # Handle strided video mask\n",
    "        if video_mask is not None and stride > 1:\n",
    "            # Subsample the mask to match video_representations shape\n",
    "            video_mask = video_mask[:, ::stride]\n",
    "            \n",
    "            # Ensure mask length matches\n",
    "            frame_length = video_representations.shape[1]\n",
    "            if video_mask.shape[1] > frame_length:\n",
    "                video_mask = video_mask[:, :frame_length]\n",
    "            elif video_mask.shape[1] < frame_length:\n",
    "                pad_size = frame_length - video_mask.shape[1]\n",
    "                pad = torch.zeros((video_mask.shape[0], pad_size), dtype=torch.bool, device=video_mask.device)\n",
    "                video_mask = torch.cat([video_mask, pad], dim=1)\n",
    "            \n",
    "            # Convert to attention mask format expected by PyTorch\n",
    "            # True = don't attend, False = attend\n",
    "            attn_mask = ~video_mask\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        \n",
    "        # PyTorch's MultiheadAttention expects:\n",
    "        # - query: [batch_size, target_seq_length, embed_dim]\n",
    "        # - key: [batch_size, source_seq_length, embed_dim]\n",
    "        # - value: [batch_size, source_seq_length, embed_dim]\n",
    "        # - attn_mask: [batch_size, target_seq_length, source_seq_length] or [target_seq_length, source_seq_length]\n",
    "        \n",
    "        # Use PyTorch's optimized implementation\n",
    "        cross_attention_output, _ = self.multihead_attn(\n",
    "            query=query,                  # From GPT tokens\n",
    "            key=video_representations,    # From video frames\n",
    "            value=video_representations,  # From video frames\n",
    "            key_padding_mask=attn_mask,   # Mask for padding frames\n",
    "            need_weights=False            # Don't return attention weights to save computation\n",
    "        )\n",
    "        \n",
    "        return cross_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "a56fa133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2158,  0.0108, -0.2586,  ..., -0.2118, -0.7830, -0.2154],\n",
       "         [-0.2158,  0.0108, -0.2586,  ..., -0.2118, -0.7830, -0.2154],\n",
       "         [-0.2158,  0.0108, -0.2586,  ..., -0.2118, -0.7830, -0.2154],\n",
       "         ...,\n",
       "         [-0.2158,  0.0108, -0.2586,  ..., -0.2118, -0.7830, -0.2154],\n",
       "         [-0.2158,  0.0108, -0.2586,  ..., -0.2118, -0.7830, -0.2154],\n",
       "         [-0.2158,  0.0108, -0.2586,  ..., -0.2118, -0.7830, -0.2154]],\n",
       "\n",
       "        [[-0.4754, -0.2581, -0.3415,  ..., -0.3153, -1.0134, -0.2806],\n",
       "         [-0.4754, -0.2581, -0.3415,  ..., -0.3153, -1.0134, -0.2806],\n",
       "         [-0.4754, -0.2581, -0.3415,  ..., -0.3153, -1.0134, -0.2806],\n",
       "         ...,\n",
       "         [-0.4754, -0.2581, -0.3415,  ..., -0.3153, -1.0134, -0.2806],\n",
       "         [-0.4754, -0.2581, -0.3415,  ..., -0.3153, -1.0134, -0.2806],\n",
       "         [-0.4754, -0.2581, -0.3415,  ..., -0.3153, -1.0134, -0.2806]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_cross_att = OptimizedCrossAttention()\n",
    "opt_cross_att.forward(hidden_states=outputs['hidden_states'], video_representations=multi_scale_representation_reinforced, video_mask=batch['mask'], stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7cac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['hidden_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "21077340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['mask'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "7855b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Subsample the mask to match video_representations shape\n",
    "video_mask = batch['mask'][:, ::2]\n",
    "\n",
    "# Ensure mask matches the number of frames after downsampling\n",
    "\n",
    "video_mask = video_mask[:, :batch['mask'].shape[1]]\n",
    "\n",
    "\n",
    "# Reshape mask to match attention scores: [batch_size, 1, 1, frame_length]\n",
    "attention_mask = ~video_mask.unsqueeze(1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "97520773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "4822b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False,  True,  True,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42901064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Integrates pre-trained GPT-2 with cross-attention for video-to-text translation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"distilgpt2\", num_cross_heads=12, freeze_gpt=True, stride=2):\n",
    "        super(VideoGPT, self).__init__()\n",
    "        \n",
    "        # Load pre-trained model\n",
    "        self.gpt = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.config = self.gpt.config\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Dimensions\n",
    "        self.hidden_size = self.config.n_embd  # 768 for distilGPT-2\n",
    "        \n",
    "        # Create cross-attention layer\n",
    "        self.cross_attention = OptimizedCrossAttention(\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_heads=num_cross_heads\n",
    "        )\n",
    "        \n",
    "        # Freeze GPT-2 weights if specified\n",
    "        if freeze_gpt:\n",
    "            self._freeze_gpt_parameters()\n",
    "    \n",
    "    def _freeze_gpt_parameters(self):\n",
    "        \"\"\"Freeze all parameters of the GPT model.\"\"\"\n",
    "        for param in self.gpt.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, video_representations, video_mask=None, \n",
    "               L_index=None, L_values=None, label_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass with integrated cross-attention and semantic smoothing loss.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs for GPT [batch_size, n_tokens]\n",
    "            video_representations: Video frame features [batch_size, n_frames/stride, hidden_size]\n",
    "            video_mask: Mask tensor [batch_size, n_frames] with True for valid frames\n",
    "            attention_mask: Mask for input tokens [batch_size, n_tokens]\n",
    "            L_index: Token indices [batch_size, max_n_tokens, 6]\n",
    "            L_values: Token values [batch_size, max_n_tokens, 6]\n",
    "            label_mask: Boolean mask [batch_size, max_n_tokens]\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Model outputs including loss and logits\n",
    "        \"\"\"\n",
    "        batch_size, n_tokens = input_ids.shape\n",
    "        \n",
    "        # Get GPT embeddings (word + position)\n",
    "        position_ids = torch.arange(0, n_tokens, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        gpt_embeds = self.gpt.transformer.wte(input_ids) + self.gpt.transformer.wpe(position_ids)\n",
    "        \n",
    "        # Store states at each step\n",
    "        hidden_states = gpt_embeds\n",
    "\n",
    "        print(f\"Input IDs min/max: {input_ids.min().item()}, {input_ids.max().item()}\")\n",
    "        print(f\"Video representations has NaN: {torch.isnan(video_representations).any().item()}\")\n",
    "\n",
    "        if label_mask is not None:\n",
    "            # Create attention mask that combines padding and causal constraints\n",
    "            extended_attention_mask = label_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "\n",
    "            # Step 2: Create causal mask (lower triangular matrix)\n",
    "            seq_length = label_mask.size(1)\n",
    "            causal_mask = torch.tril(torch.ones((seq_length, seq_length), \n",
    "                                               device=label_mask.device))\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "            # Step 3: Combine padding mask with causal mask\n",
    "            combined_mask = causal_mask * extended_attention_mask.float()\n",
    "\n",
    "            # Step 4: Convert to additive mask where 0 means \"attend\" and \n",
    "            # a large negative number means \"don't attend\"\n",
    "            attention_mask = combined_mask.to(dtype=hidden_states.dtype)\n",
    "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "\n",
    "        print(f\"Attention mask min/max: {attention_mask.min().item()}, {attention_mask.max().item()}\")\n",
    "        print(f\"Attention mask has -inf: {torch.isinf(attention_mask).any().item()}\")\n",
    "        print(f\"Number of non-masked positions: {(attention_mask > -1000).sum().item()}\")\n",
    "        # Process through GPT layers with cross-attention\n",
    "        for i, block in enumerate(self.gpt.transformer.h):\n",
    "            print(f\"Block {i} hidden states has NaN: {torch.isnan(hidden_states).any().item()}\")\n",
    "            # 1. GPT self-attention\n",
    "            attn_outputs = block.attn(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask if label_mask is not None else None\n",
    "            )\n",
    "\n",
    "            gpt_attn_output = attn_outputs[0]\n",
    "            \n",
    "            # Add residual connection\n",
    "            hidden_states = gpt_attn_output + hidden_states\n",
    "            \n",
    "            # 2. Insert our cross-attention between self-attention and FFN\n",
    "            cross_attention_output = self.cross_attention(\n",
    "                hidden_states, \n",
    "                video_representations, \n",
    "                video_mask=video_mask,\n",
    "                stride=self.stride\n",
    "            )\n",
    "            \n",
    "            # Add residual connection to cross-attention\n",
    "            hidden_states = hidden_states + cross_attention_output\n",
    "            \n",
    "            # 3. Feed-forward network\n",
    "            feed_forward_output = block.mlp(hidden_states)\n",
    "            hidden_states = hidden_states + feed_forward_output\n",
    "        \n",
    "        # Final layer norm\n",
    "        hidden_states = self.gpt.transformer.ln_f(hidden_states)\n",
    "        \n",
    "        # Language modeling head\n",
    "        lm_logits = self.gpt.lm_head(hidden_states)\n",
    "        \n",
    "        # Calculate loss if labels are provided\n",
    "        loss = None\n",
    "\n",
    "        \n",
    "        if L_index is not None and L_values is not None:\n",
    "            # Use our custom semantic smoothing loss\n",
    "            loss = optimized_semantic_smoothing_loss(\n",
    "                logits=lm_logits,\n",
    "                L_index=L_index,\n",
    "                L_values=L_values,\n",
    "                label_mask=label_mask\n",
    "            )\n",
    "    \n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss, \n",
    "            \"logits\": lm_logits, \n",
    "            \"hidden_states\": hidden_states\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "15af986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"distilgpt2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = model.to(\"cuda\")  # Move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "d33815c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize model\n",
    "model = VideoGPT(\n",
    "    model_name=\"distilgpt2\",\n",
    "    num_cross_heads=12,\n",
    "    freeze_gpt=True,\n",
    "    stride=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "9a38e95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7000, 0.0646, 0.0611, 0.0587, 0.0579, 0.0577],\n",
       "         [0.7000, 0.0653, 0.0643, 0.0610, 0.0548, 0.0546],\n",
       "         [0.7000, 0.0738, 0.0602, 0.0559, 0.0556, 0.0545],\n",
       "         [0.7000, 0.0638, 0.0636, 0.0582, 0.0580, 0.0565],\n",
       "         [0.7000, 0.0703, 0.0629, 0.0587, 0.0580, 0.0500],\n",
       "         [0.7000, 0.0756, 0.0569, 0.0564, 0.0562, 0.0548],\n",
       "         [0.7000, 0.0658, 0.0651, 0.0601, 0.0562, 0.0527],\n",
       "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.7000, 0.0662, 0.0633, 0.0572, 0.0567, 0.0565],\n",
       "         [0.7000, 0.0605, 0.0600, 0.0598, 0.0598, 0.0598],\n",
       "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"L_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "5eab9ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   72,   705,    76,  2658,   558,   474,  1952, 50256],\n",
       "        [  404, 23971, 50256,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"L_index\"][:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "eae37436",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_targets = batch[\"L_index\"][:, :, 0].clone()\n",
    "\n",
    "# Create input_ids by shifting right (add BOS at beginning, remove last token)\n",
    "batch_size, seq_len = primary_targets.shape\n",
    "input_ids = torch.zeros_like(primary_targets)\n",
    "input_ids[:, 0] = tokenizer.bos_token_id  # Start with BOS token\n",
    "input_ids[:, 1:] = primary_targets[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "4b392996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True, False, False, False, False, False]])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['label_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "21f6f8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   72,   705,    76,  2658,   558,   474,  1952, 50256],\n",
       "        [  404, 23971, 50256,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"L_index\"][:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "cc04b873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256,    72,   705,    76,  2658,   558,   474,  1952],\n",
       "        [50256,   404, 23971, 50256,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "bb9b86d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"label_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "696b4f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True, False, False, False, False, False]])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['label_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "2c64beb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs min/max: 0, 50256\n",
      "Video representations has NaN: False\n",
      "Attention mask min/max: -10000.0, -0.0\n",
      "Attention mask has -inf: False\n",
      "Number of non-masked positions: 57\n",
      "Block 0 hidden states has NaN: False\n",
      "Block 1 hidden states has NaN: False\n",
      "Block 2 hidden states has NaN: False\n",
      "Block 3 hidden states has NaN: False\n",
      "Block 4 hidden states has NaN: False\n",
      "Block 5 hidden states has NaN: False\n"
     ]
    }
   ],
   "source": [
    "outputs = model(\n",
    "    input_ids=input_ids,\n",
    "    video_representations=multi_scale_representation_reinforced,\n",
    "    video_mask=batch[\"mask\"],\n",
    "    L_index=batch[\"L_index\"],\n",
    "    L_values=batch[\"L_values\"],\n",
    "    label_mask=batch[\"label_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "e708fd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[464, 464, 464, 464, 464, 464, 464, 464],\n",
       "        [464, 464, 464, 464, 464, 464, 464, 464]])"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs['logits'], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "43f3d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_from_logits(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert model logits to human-readable text predictions.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of shape [batch_size, sequence_length, vocab_size]\n",
    "        tokenizer: The GPT tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing token IDs and decoded text for each batch item\n",
    "    \"\"\"\n",
    "    # Get the most likely token at each position (argmax along vocab dimension)\n",
    "    predicted_token_ids = torch.argmax(logits, dim=-1)  # [batch_size, sequence_length]\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    token_ids_np = predicted_token_ids.cpu().numpy()\n",
    "    \n",
    "    # Container for results\n",
    "    results = []\n",
    "    \n",
    "    # Process each sequence in the batch\n",
    "    for i, ids in enumerate(token_ids_np):\n",
    "        # Decode the token IDs to text\n",
    "        text = tokenizer.decode(ids)\n",
    "        \n",
    "        # For more detailed analysis, get individual tokens\n",
    "        tokens = []\n",
    "        for token_id in ids:\n",
    "            token_str = tokenizer.decode([token_id])\n",
    "            tokens.append((token_id, token_str))\n",
    "            \n",
    "        results.append({\n",
    "            \"sequence_idx\": i,\n",
    "            \"token_ids\": ids.tolist(),\n",
    "            \"tokens\": tokens,\n",
    "            \"text\": text,\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "8c8145fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_predictions_from_logits(outputs[\"logits\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "f7273c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence_idx': 0,\n",
       "  'token_ids': [464, 464, 464, 464, 464, 464, 464, 464],\n",
       "  'tokens': [(464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The')],\n",
       "  'text': 'TheTheTheTheTheTheTheThe'},\n",
       " {'sequence_idx': 1,\n",
       "  'token_ids': [464, 464, 464, 464, 464, 464, 464, 464],\n",
       "  'tokens': [(464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The'),\n",
       "   (464, 'The')],\n",
       "  'text': 'TheTheTheTheTheTheTheThe'}]"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "636499e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50256,   404, 23971, 50256,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "39436e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 0:\n",
      "Input:      <|endoftext|>i 'm candace jones\n",
      "Generated:  TheTheTheTheTheTheTheThe\n",
      "\n",
      "Example 1:\n",
      "Input:      <|endoftext|>opinion<|endoftext|>!!!!\n",
      "Generated:  TheTheTheTheTheTheTheThe\n"
     ]
    }
   ],
   "source": [
    "def compare_predictions(input_ids, predictions, L_index, tokenizer):\n",
    "    \"\"\"Show comparison between inputs, predictions, and expected labels\"\"\"\n",
    "    for i, pred in enumerate(predictions):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        \n",
    "        # Input sequence\n",
    "        input_sequence = tokenizer.decode(input_ids[i])\n",
    "        print(f\"Input:      {input_sequence}\")\n",
    "        \n",
    "        # Generated sequence\n",
    "        print(f\"Generated:  {pred['text']}\")\n",
    "\n",
    "\n",
    "# Use in your test\n",
    "compare_predictions(\n",
    "    input_ids,\n",
    "    predictions,\n",
    "    batch[\"L_index\"],\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "ed726c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 predictions for batch 0, position 7:\n",
      "  1. Token: 'The', ID: 464, Probability: 0.3296\n",
      "  2. Token: 'A', ID: 32, Probability: 0.2013\n",
      "  3. Token: '\"', ID: 1, Probability: 0.1883\n",
      "  4. Token: 'I', ID: 40, Probability: 0.1809\n",
      "  5. Token: 'In', ID: 818, Probability: 0.1000\n",
      "\n",
      "Top 5 predictions for batch 1, position 7:\n",
      "  1. Token: 'The', ID: 464, Probability: 0.3314\n",
      "  2. Token: 'A', ID: 32, Probability: 0.2017\n",
      "  3. Token: '\"', ID: 1, Probability: 0.1858\n",
      "  4. Token: 'I', ID: 40, Probability: 0.1813\n",
      "  5. Token: 'In', ID: 818, Probability: 0.0998\n"
     ]
    }
   ],
   "source": [
    "def analyze_next_token_predictions(logits, current_position, tokenizer, top_k=5):\n",
    "    \"\"\"Analyze top-k predictions for the next token at a specified position\"\"\"\n",
    "    batch_size = logits.shape[0]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        next_token_logits = logits[b, current_position, :]\n",
    "        \n",
    "        # Get top-k predictions\n",
    "        values, indices = torch.topk(next_token_logits, top_k)\n",
    "        probs = torch.softmax(values, dim=0)\n",
    "        \n",
    "        print(f\"\\nTop {top_k} predictions for batch {b}, position {current_position}:\")\n",
    "        for i, (idx, prob) in enumerate(zip(indices.tolist(), probs.tolist())):\n",
    "            token = tokenizer.decode([idx])\n",
    "            print(f\"  {i+1}. Token: '{token}', ID: {idx}, Probability: {prob:.4f}\")\n",
    "\n",
    "\n",
    "# Use in your test to analyze what comes after the last input token\n",
    "analyze_next_token_predictions(\n",
    "    outputs[\"logits\"], \n",
    "    current_position=input_ids.shape[1]-1,  # Last position\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74433ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
