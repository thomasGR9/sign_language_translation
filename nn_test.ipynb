{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30acb086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomasdev/anaconda3/envs/ultralytics-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, ConcatDataset\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "import signal\n",
    "import traceback\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a032b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data_standardized(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks_standardized = data['dom_landmarks_standardized']\n",
    "    non_dom_landmarks_standardized = data['non_dom_landmarks_standardized']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores_standardized = data['blendshape_scores_standardized']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist_standardized = data['nose_to_wrist_dist_standardized']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    dom_velocity_small_standardized = data['dom_velocity_small_standardized']\n",
    "    dom_velocity_large_standardized = data['dom_velocity_large_standardized']\n",
    "    non_dom_velocity_small_standardized = data['non_dom_velocity_small_standardized']\n",
    "    non_dom_velocity_large_standardized = data['non_dom_velocity_large_standardized']\n",
    "    velocity_confidence = data['velocity_confidence']\n",
    "    velocity_calculation_confidence = data['velocity_calculation_confidence']\n",
    "    nose_to_wrist_velocity_small_standardized = data['wrist_velocity_small_standardized']\n",
    "    nose_to_wrist_velocity_large_standardized = data['wrist_velocity_large_standardized']\n",
    "    \n",
    "    return (dom_landmarks_standardized, non_dom_landmarks_standardized, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores_standardized, face_detected, \n",
    "            nose_to_wrist_dist_standardized, frame_idx, timestamp_ms, dom_velocity_small_standardized, dom_velocity_large_standardized, non_dom_velocity_small_standardized, non_dom_velocity_large_standardized, velocity_confidence, velocity_calculation_confidence, nose_to_wrist_velocity_small_standardized, nose_to_wrist_velocity_large_standardized)\n",
    "\n",
    "def sorted_npz_files_checked_label(directory_path):\n",
    "    if os.path.exists(directory_path) and os.path.isdir(directory_path):\n",
    "        # List all NPZ files in the directory\n",
    "        npz_files = sorted(glob.glob(os.path.join(directory_path, \"*.npz\")))\n",
    "    else:\n",
    "        print(f\"Directory path {directory_path} doesn't exist or it isn't a directory\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    \n",
    "    # Skip if no files found\n",
    "    if not npz_files:\n",
    "        print(f\"No NPZ files found in {directory_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    \n",
    "    with open(os.path.join(directory_path, 'detection_statistics.json')) as f:\n",
    "        statistics_file = json.load(f)\n",
    "    \n",
    "    if statistics_file['video_info']['total_frames'] != (len(npz_files)-1):\n",
    "        print(\"npz filepath list contain different amount of items than total frames\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    frame_to_file = {}\n",
    "    for file_path in npz_files:\n",
    "        if os.path.basename(file_path) == 'smooth_labels.npz':\n",
    "            label_path = file_path\n",
    "            continue\n",
    "        try:\n",
    "            frame_data = load_frame_data_standardized(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading frame with path: {file_path}: {e}\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "        frame_idx = frame_data[8]  # Index for frame_idx\n",
    "        frame_to_file[frame_idx] = file_path\n",
    "\n",
    "    \n",
    "    frame_indices = sorted(frame_to_file.keys())\n",
    "    if not all(frame_indices[i+1] - frame_indices[i] == 1 for i in range(len(frame_indices) - 1)):\n",
    "        print(\"Consecutive frames are not different by one frame\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    \n",
    "\n",
    "    return frame_to_file, frame_indices, label_path\n",
    "\n",
    "def load_label(label_path):\n",
    "    label_data = np.load(label_path)\n",
    "    L_index = label_data['L_index']\n",
    "    L_values = label_data['L_values']\n",
    "    return L_index, L_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ASLFrameDataset(Dataset):\n",
    "    \"\"\"Dataset for ASL frame data from video clips with feature extraction.\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataframe: Pandas DataFrame containing 'landmarks_file_path' column\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.video_paths = list(dataframe['landmarks_file_path'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of videos in the dataset.\"\"\"\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get data for a complete video with all features.\"\"\"\n",
    "        directory_path = self.video_paths[idx]\n",
    "        \n",
    "        # Get paths to all frame files in this video\n",
    "        frame_to_file, frame_indices, label_path = sorted_npz_files_checked_label(directory_path)\n",
    "        \n",
    "        # Initialize dictionaries to store all data\n",
    "        all_data = {\n",
    "            # Primary features for model input\n",
    "            'dom_landmarks': [],\n",
    "            'non_dom_landmarks': [],\n",
    "            'blendshape_scores': [],\n",
    "            'nose_to_wrist_dist': [],\n",
    "            'dom_velocity_small': [],\n",
    "            'dom_velocity_large': [],\n",
    "            'non_dom_velocity_small': [],\n",
    "            'non_dom_velocity_large': [],\n",
    "            'nose_to_wrist_velocity_small': [],\n",
    "            'nose_to_wrist_velocity_large': [],\n",
    "            \n",
    "            # Additional data for later use\n",
    "            'confidence_scores': [],\n",
    "            'interpolation_scores': [],\n",
    "            'detection_status': [],\n",
    "            'face_detected': [],\n",
    "            'frame_idx': [],\n",
    "            'velocity_confidence': [],\n",
    "            'velocity_calculation_confidence': []\n",
    "        }\n",
    "        \n",
    "        # Load data from each frame\n",
    "        for frame_idx in frame_indices:\n",
    "            file_path = frame_to_file[frame_idx]\n",
    "            frame_data = load_frame_data_standardized(file_path)\n",
    "            \n",
    "            # Unpack frame data\n",
    "            (dom_landmarks_standardized,\n",
    "             non_dom_landmarks_standardized,\n",
    "             confidence_scores,\n",
    "             interpolation_scores,\n",
    "             detection_status,\n",
    "             blendshape_scores_standardized,\n",
    "             face_detected,\n",
    "             nose_to_wrist_dist_standardized,\n",
    "             frame_idx_val,\n",
    "             timestamp_ms,  # We'll skip this one\n",
    "             dom_velocity_small_standardized,\n",
    "             dom_velocity_large_standardized,\n",
    "             non_dom_velocity_small_standardized,\n",
    "             non_dom_velocity_large_standardized,\n",
    "             velocity_confidence,\n",
    "             velocity_calculation_confidence,\n",
    "             nose_to_wrist_velocity_small_standardized,\n",
    "             nose_to_wrist_velocity_large_standardized) = frame_data\n",
    "            \n",
    "            # Store primary features for model input\n",
    "            all_data['dom_landmarks'].append(dom_landmarks_standardized)\n",
    "            all_data['non_dom_landmarks'].append(non_dom_landmarks_standardized)\n",
    "            all_data['blendshape_scores'].append(blendshape_scores_standardized)\n",
    "            all_data['nose_to_wrist_dist'].append(nose_to_wrist_dist_standardized)\n",
    "            all_data['dom_velocity_small'].append(dom_velocity_small_standardized)\n",
    "            all_data['dom_velocity_large'].append(dom_velocity_large_standardized)\n",
    "            all_data['non_dom_velocity_small'].append(non_dom_velocity_small_standardized)\n",
    "            all_data['non_dom_velocity_large'].append(non_dom_velocity_large_standardized)\n",
    "            all_data['nose_to_wrist_velocity_small'].append(nose_to_wrist_velocity_small_standardized)\n",
    "            all_data['nose_to_wrist_velocity_large'].append(nose_to_wrist_velocity_large_standardized)\n",
    "            \n",
    "            # Store additional data for later use\n",
    "            all_data['confidence_scores'].append(confidence_scores)\n",
    "            all_data['interpolation_scores'].append(interpolation_scores)\n",
    "            all_data['detection_status'].append(detection_status)\n",
    "            all_data['face_detected'].append(face_detected)\n",
    "            all_data['frame_idx'].append(frame_idx_val)\n",
    "            all_data['velocity_confidence'].append(velocity_confidence)\n",
    "            all_data['velocity_calculation_confidence'].append(velocity_calculation_confidence)\n",
    "        \n",
    "        # Convert lists to numpy arrays\n",
    "        for key in all_data:\n",
    "            all_data[key] = np.array(all_data[key])\n",
    "        \n",
    "        # Load label data\n",
    "        L_index, L_values = load_label(label_path)\n",
    "        all_data['L_index'] = L_index\n",
    "        all_data['L_values'] = L_values\n",
    "        \n",
    "        # Store sequence length and directory path\n",
    "        all_data['seq_length'] = len(frame_indices)\n",
    "        all_data['directory_path'] = directory_path\n",
    "        \n",
    "        return all_data\n",
    "\n",
    "\n",
    "class SingleDataFrameBatchSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Custom batch sampler that ensures each batch contains samples \n",
    "    from only one dataframe.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_sizes: List[int], batch_size: int, drop_last: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the batch sampler.\n",
    "        \n",
    "        Args:\n",
    "            dataset_sizes: List of sizes for each dataset\n",
    "            batch_size: Batch size\n",
    "            drop_last: Whether to drop the last batch if incomplete\n",
    "        \"\"\"\n",
    "        self.dataset_sizes = dataset_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        \n",
    "        # Calculate offsets for indexing into the combined dataset\n",
    "        self.offsets = [0]\n",
    "        for size in dataset_sizes[:-1]:\n",
    "            self.offsets.append(self.offsets[-1] + size)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate batches of indices, ensuring each batch comes from one dataset.\"\"\"\n",
    "        # Create index lists for each dataset\n",
    "        all_indices = []\n",
    "        for dataset_idx, size in enumerate(self.dataset_sizes):\n",
    "            offset = self.offsets[dataset_idx]\n",
    "            indices = list(range(offset, offset + size))\n",
    "            random.shuffle(indices)\n",
    "            all_indices.append(indices)\n",
    "            \n",
    "        # Create batches for each dataset\n",
    "        all_batches = []\n",
    "        for dataset_idx, indices in enumerate(all_indices):\n",
    "            for i in range(0, len(indices), self.batch_size):\n",
    "                batch = indices[i:min(i + self.batch_size, len(indices))]\n",
    "                \n",
    "                # Skip last incomplete batch if drop_last is True\n",
    "                if self.drop_last and len(batch) < self.batch_size:\n",
    "                    continue\n",
    "                \n",
    "                all_batches.append(batch)\n",
    "        \n",
    "        # Shuffle the order of batches\n",
    "        random.shuffle(all_batches)\n",
    "        \n",
    "        # Yield batches one at a time\n",
    "        for batch in all_batches:\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches.\"\"\"\n",
    "        if self.drop_last:\n",
    "            return sum(size // self.batch_size for size in self.dataset_sizes)\n",
    "        else:\n",
    "            return sum((size + self.batch_size - 1) // self.batch_size for size in self.dataset_sizes)\n",
    "\n",
    "def collate_with_dynamic_padding(batch, device='cuda'):\n",
    "    \"\"\"\n",
    "    Custom collate function that handles variable-length sequences and label data.\n",
    "    \"\"\"\n",
    "    # Find the maximum sequence length in this batch\n",
    "    max_seq_length = max(sample['seq_length'] for sample in batch)\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # Initialize the result dictionary\n",
    "    result = {\n",
    "        'directory_paths': [],\n",
    "        'seq_lengths': []\n",
    "    }\n",
    "    \n",
    "    # Store directory paths and sequence lengths\n",
    "    for sample in batch:\n",
    "        result['directory_paths'].append(sample['directory_path'])\n",
    "        result['seq_lengths'].append(sample['seq_length'])\n",
    "    \n",
    "    result['seq_lengths'] = torch.tensor(result['seq_lengths'], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Create mask tensor for frames [batch_size, max_seq_length]\n",
    "    frame_mask = torch.zeros((batch_size, max_seq_length), dtype=torch.bool, device=device)\n",
    "    \n",
    "    # Handle variable-sized label data\n",
    "    # Find maximum dimensions for L_index and L_values\n",
    "    max_tokens = max(sample['L_index'].shape[0] for sample in batch)\n",
    "    token_width = batch[0]['L_index'].shape[1]  # Assuming all have same width (6)\n",
    "    \n",
    "    # Create padded tensors for labels - using appropriate dtypes\n",
    "    L_index_padded = torch.zeros((batch_size, max_tokens, token_width), dtype=torch.long, device=device)\n",
    "    L_values_padded = torch.zeros((batch_size, max_tokens, token_width), dtype=torch.float32, device=device)\n",
    "    label_mask = torch.zeros((batch_size, max_tokens), dtype=torch.bool, device=device)\n",
    "    \n",
    "    # Fill in label data\n",
    "    for i, sample in enumerate(batch):\n",
    "        num_tokens = sample['L_index'].shape[0]\n",
    "        L_index_padded[i, :num_tokens] = torch.tensor(sample['L_index'], dtype=torch.long, device=device)\n",
    "        L_values_padded[i, :num_tokens] = torch.tensor(sample['L_values'], dtype=torch.float32, device=device)\n",
    "        label_mask[i, :num_tokens] = True\n",
    "    \n",
    "    result['L_index'] = L_index_padded\n",
    "    result['L_values'] = L_values_padded\n",
    "    result['label_mask'] = label_mask\n",
    "    \n",
    "    # Process feature data with consistent dimensions\n",
    "    feature_keys = [\n",
    "        # Primary features for model input\n",
    "        'dom_landmarks', 'non_dom_landmarks', 'blendshape_scores',\n",
    "        'nose_to_wrist_dist', 'dom_velocity_small', 'dom_velocity_large',\n",
    "        'non_dom_velocity_small', 'non_dom_velocity_large',\n",
    "        'nose_to_wrist_velocity_small', 'nose_to_wrist_velocity_large',\n",
    "        \n",
    "        # Additional data for later use\n",
    "        'confidence_scores', 'interpolation_scores', 'detection_status',\n",
    "        'frame_idx', 'velocity_confidence',\n",
    "        'velocity_calculation_confidence'\n",
    "    ]\n",
    "    \n",
    "    # Process all standard features\n",
    "    for key in feature_keys:\n",
    "        try:\n",
    "            # Get the sample feature\n",
    "            sample_feature = batch[0][key]\n",
    "            feature_shape = sample_feature.shape[1:] if len(sample_feature.shape) > 1 else ()\n",
    "            \n",
    "            # Create padded tensor [batch_size, max_seq_length, *feature_shape]\n",
    "            padded_tensor = torch.zeros((batch_size, max_seq_length) + feature_shape, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Fill in the actual data and update the mask\n",
    "            for i, sample in enumerate(batch):\n",
    "                seq_length = sample['seq_length']\n",
    "                feature_data = sample[key]\n",
    "                padded_tensor[i, :seq_length] = torch.tensor(feature_data, dtype=torch.float32, device=device)\n",
    "                frame_mask[i, :seq_length] = True\n",
    "                \n",
    "            # Add to result\n",
    "            result[key] = padded_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing feature '{key}': {e}\")\n",
    "            print(f\"  Shape in first sample: {np.array(batch[0][key]).shape}\")\n",
    "            if i > 0:\n",
    "                print(f\"  Shape in problematic sample {i}: {np.array(sample[key]).shape}\")\n",
    "    \n",
    "    # Process face_detected separately with proper reshaping\n",
    "    try:\n",
    "        # Create a tensor specifically for face_detected (which needs special handling)\n",
    "        face_detected_tensor = torch.zeros((batch_size, max_seq_length), dtype=torch.float32, device=device)\n",
    "        \n",
    "        for i, sample in enumerate(batch):\n",
    "            seq_length = sample['seq_length']\n",
    "            face_data = sample['face_detected']\n",
    "            \n",
    "            # Convert to tensor and ensure it's 1D\n",
    "            face_tensor = torch.tensor(face_data, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Assign directly without reshaping\n",
    "            face_detected_tensor[i, :seq_length] = face_tensor\n",
    "            \n",
    "        result['face_detected'] = face_detected_tensor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing face_detected: {e}\")\n",
    "        print(f\"  Shape: {np.array(batch[0]['face_detected']).shape}\")\n",
    "    \n",
    "    # Add the frame mask\n",
    "    result['mask'] = frame_mask\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def create_asl_dataloader(low_df, mid_df, high_df, batch_size=16, num_workers=4, drop_last=False, device='cuda'):\n",
    "    \"\"\"\n",
    "    Create a data loader for ASL data that ensures batches only contain samples from one dataframe.\n",
    "    \n",
    "    Args:\n",
    "        low_df: DataFrame with low frame count videos\n",
    "        mid_df: DataFrame with medium frame count videos\n",
    "        high_df: DataFrame with high frame count videos\n",
    "        batch_size: Batch size\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        drop_last: Whether to drop the last batch if incomplete\n",
    "        \n",
    "    Returns:\n",
    "        A DataLoader that yields batches from the three dataframes\n",
    "    \"\"\"\n",
    "    # Create datasets for each dataframe\n",
    "    low_dataset = ASLFrameDataset(low_df)\n",
    "    mid_dataset = ASLFrameDataset(mid_df)\n",
    "    high_dataset = ASLFrameDataset(high_df)\n",
    "    \n",
    "    # Get dataset sizes\n",
    "    dataset_sizes = [len(low_dataset), len(mid_dataset), len(high_dataset)]\n",
    "    if drop_last:\n",
    "        expected_batches = sum(size // batch_size for size in dataset_sizes)\n",
    "    else:\n",
    "        expected_batches = sum((size + batch_size - 1) // batch_size for size in dataset_sizes)\n",
    "    # Combine datasets\n",
    "    combined_dataset = ConcatDataset([low_dataset, mid_dataset, high_dataset])\n",
    "    \n",
    "    # Create a batch sampler that ensures batches only contain samples from one dataframe\n",
    "    batch_sampler = SingleDataFrameBatchSampler(dataset_sizes, batch_size, drop_last)\n",
    "    \n",
    "    # Create the data loader\n",
    "    data_loader = DataLoader(\n",
    "        combined_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "        collate_fn=lambda b: collate_with_dynamic_padding(b, device=device)\n",
    "    )\n",
    "    \n",
    "    return data_loader, expected_batches\n",
    "\n",
    "\n",
    "\n",
    "class LandmarkEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates learnable embeddings for hand landmarks.\n",
    "    \n",
    "    This module maps each landmark (across both hands) to a unique \n",
    "    embedding vector that encodes its semantic meaning.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, num_landmarks_per_hand=21):\n",
    "        \"\"\"\n",
    "        Initialize the landmark embedding module.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of the embedding vectors\n",
    "            num_landmarks_per_hand: Number of landmarks per hand (default: 21)\n",
    "        \"\"\"\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_landmarks_per_hand = num_landmarks_per_hand\n",
    "        self.total_landmarks = 2 * num_landmarks_per_hand  # Both hands\n",
    "        \n",
    "        # Create the embedding table: [total_landmarks, embedding_dim]\n",
    "        self.embedding_table = nn.Embedding(\n",
    "            num_embeddings=self.total_landmarks,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        \n",
    "        # Initialize the embeddings with a normal distribution\n",
    "        nn.init.normal_(self.embedding_table.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, landmark_indices=None):\n",
    "        \"\"\"\n",
    "        Get embeddings for landmarks.\n",
    "        \n",
    "        Args:\n",
    "            landmark_indices: Optional tensor of landmark indices to retrieve.\n",
    "                             If None, returns all landmark embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of landmark embeddings\n",
    "        \"\"\"\n",
    "        if landmark_indices is None:\n",
    "            # Return all landmark embeddings\n",
    "            # Create indices for all landmarks: 0 to total_landmarks-1\n",
    "            landmark_indices = torch.arange(self.total_landmarks, device=self.embedding_table.weight.device)\n",
    "        \n",
    "        # Get the embeddings for the specified indices\n",
    "        embeddings = self.embedding_table(landmark_indices)\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "class LandmarkSpatialEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the spatial information (x,y,z coordinates) of individual hand landmarks.\n",
    "    \n",
    "    This module transforms the 3D coordinates of each landmark into a higher-dimensional\n",
    "    representation that captures the 'where' aspect of the landmark.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the spatial encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Base dimension for the model\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*embedding_dim] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(LandmarkSpatialEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension will be 2*embedding_dim as requested\n",
    "        self.output_dim = 2 * embedding_dim\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * embedding_dim] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(3, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.spatial_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, landmarks):\n",
    "        \"\"\"\n",
    "        Encode the spatial coordinates of landmarks.\n",
    "        \n",
    "        Args:\n",
    "            landmarks: Tensor of shape [..., 3] containing x,y,z coordinates\n",
    "                       The leading dimensions can be anything (batch, sequence, landmark)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., output_dim] with the spatial encodings\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = landmarks.shape\n",
    "        \n",
    "        # Reshape to [-1, 3] to process all landmarks in parallel\n",
    "        flat_landmarks = landmarks.reshape(-1, 3)\n",
    "        \n",
    "        # Apply the spatial encoder\n",
    "        encoded = self.spatial_encoder(flat_landmarks)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        reshaped_encoded = encoded.reshape(*original_shape[:-1], self.output_dim)\n",
    "        \n",
    "        return reshaped_encoded\n",
    "    \n",
    "def combine_spatial_and_semantic_features(spatial_features, semantic_features):\n",
    "    \"\"\"\n",
    "    Combines the spatial encoder output with the semantic embedding features.\n",
    "    \n",
    "    This function concatenates the \"where\" (spatial) information with the \"what\" \n",
    "    (semantic) information to create a comprehensive landmark representation.\n",
    "    \n",
    "    Args:\n",
    "        spatial_features: Tensor of shape [..., n_spatial_encode] where\n",
    "                         n_spatial_encode = 2*embedding_dim\n",
    "        semantic_features: Tensor of shape [..., embedding_dim]\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [..., 3*embedding_dim] containing the combined representation\n",
    "    \"\"\"\n",
    "\n",
    "    batch_dims = spatial_features.shape[:-2]\n",
    "    expanded_embeddings = semantic_features.expand(*batch_dims, -1, -1)\n",
    "    # Verify that the batch dimensions match\n",
    "    assert spatial_features.shape[:-1] == expanded_embeddings.shape[:-1], \\\n",
    "        \"Batch dimensions of spatial and semantic features must match\"\n",
    "    \n",
    "    # Concatenate along the last dimension\n",
    "    combined_features = torch.cat([expanded_embeddings, spatial_features], dim=-1)\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "class WristSpatialEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes the spatial information of wrist landmarks relative to the nose.\n",
    "    \n",
    "    This module processes the 2D coordinates (x,y) of each wrist independently\n",
    "    but in parallel, using shared weights across both wrists.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the wrist spatial encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Base dimension for the model\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*embedding_dim] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(WristSpatialEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension will be 2*embedding_dim as requested\n",
    "        self.output_dim = 2 * embedding_dim\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * embedding_dim] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer (2D coordinates instead of 3D)\n",
    "        layers.append(nn.Linear(2, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.wrist_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, wrist_coordinates):\n",
    "        \"\"\"\n",
    "        Encode the spatial coordinates of wrist landmarks.\n",
    "        \n",
    "        Args:\n",
    "            wrist_coordinates: Tensor of shape [..., 2, 2] containing x,y coordinates\n",
    "                              for both wrists. Leading dimensions can be anything\n",
    "                              (batch, sequence), and the last two dimensions are:\n",
    "                              - Dimension -2: Wrist index (0=dominant, 1=non-dominant)\n",
    "                              - Dimension -1: Coordinates (x,y)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., 2, output_dim] with the spatial encodings for each wrist\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = wrist_coordinates.shape\n",
    "        \n",
    "        # Reshape to [-1, 2] to process all wrist coordinates in parallel\n",
    "        # This flattens all leading dimensions and processes each (x,y) pair independently\n",
    "        flat_wrists = wrist_coordinates.reshape(-1, 2)\n",
    "        \n",
    "        # Apply the wrist encoder\n",
    "        encoded = self.wrist_encoder(flat_wrists)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        # Replace the coordinate dimension (2) with output_dim\n",
    "        new_shape = original_shape[:-1] + (self.output_dim,)\n",
    "        reshaped_encoded = encoded.reshape(new_shape)\n",
    "        \n",
    "        return reshaped_encoded\n",
    "    \n",
    "def combine_wrist_embedding_and_spatial(wrist_embeddings, wrist_spatial_features):\n",
    "    \"\"\"\n",
    "    Combines wrist semantic embeddings with their spatial features.\n",
    "    \n",
    "    This function integrates:\n",
    "    1. The semantic meaning of each wrist (from embeddings)\n",
    "    2. The spatial position of each wrist (from the WristSpatialEncoder)\n",
    "    \n",
    "    Args:\n",
    "        wrist_embeddings: Tensor of shape [2, embedding_dim] with wrist embeddings\n",
    "                         where [0] is dom wrist and [1] is non-dom wrist\n",
    "        wrist_spatial_features: Tensor of shape [..., 2, 2*embedding_dim] \n",
    "                               from the WristSpatialEncoder\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [..., 2, 3*embedding_dim] with the combined representation\n",
    "    \"\"\"\n",
    "    # Get the batch dimensions from the spatial features tensor\n",
    "    batch_dims = wrist_spatial_features.shape[:-2]\n",
    "    \n",
    "    # Expand wrist embeddings to match the batch dimensions\n",
    "    # From [2, embedding_dim] to [..., 2, embedding_dim]\n",
    "    expanded_embeddings = wrist_embeddings.expand(*batch_dims, -1, -1)\n",
    "    \n",
    "    # Verify that the shapes are compatible for concatenation\n",
    "    assert expanded_embeddings.shape[:-1] == wrist_spatial_features.shape[:-1], \\\n",
    "        \"Batch dimensions of embeddings and spatial features must match\"\n",
    "    \n",
    "    # Concatenate along the last dimension\n",
    "    combined_features = torch.cat([\n",
    "        expanded_embeddings,     # Wrist identity (what)\n",
    "        wrist_spatial_features   # Wrist position (where)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "class BlendshapeEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes facial blendshape scores into a higher-dimensional representation.\n",
    "    \n",
    "    This network processes the 52 facial blendshape parameters that capture\n",
    "    expressions and face movements relevant to ASL interpretation.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the blendshape encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Base dimension for the model\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*embedding_dim] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(BlendshapeEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension will be 2*embedding_dim as requested\n",
    "        self.output_dim = 2 * embedding_dim\n",
    "        \n",
    "        # Input dimension for blendshape scores\n",
    "        self.input_dim = 52\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * embedding_dim] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer (52 blendshape scores)\n",
    "        layers.append(nn.Linear(self.input_dim, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.blendshape_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, blendshape_scores):\n",
    "        \"\"\"\n",
    "        Encode the facial blendshape scores.\n",
    "        \n",
    "        Args:\n",
    "            blendshape_scores: Tensor of shape [..., 52] containing facial expression parameters.\n",
    "                              Leading dimensions can be anything (batch, sequence).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., output_dim] with the encoded facial features\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = blendshape_scores.shape\n",
    "        \n",
    "        # Reshape to [-1, 52] to process all blendshape scores in parallel\n",
    "        flat_blendshapes = blendshape_scores.reshape(-1, self.input_dim)\n",
    "        \n",
    "        # Apply the blendshape encoder\n",
    "        encoded = self.blendshape_encoder(flat_blendshapes)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        # Replace the blendshape dimension (52) with output_dim\n",
    "        new_shape = original_shape[:-1] + (self.output_dim,)\n",
    "        reshaped_encoded = encoded.reshape(new_shape)\n",
    "        \n",
    "        return reshaped_encoded\n",
    "    \n",
    "class VelocityEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes velocity features of hand landmarks into a higher-dimensional representation.\n",
    "    \n",
    "    This network processes the 5 spherical coordinate velocity features for each landmark\n",
    "    independently but in parallel, using the same weights across all landmarks, hands,\n",
    "    and velocity windows (small and large).\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_velocity_encoding, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the velocity encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            n_velocity_encoding: Output dimension for each landmark's velocity encoding\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*n_velocity_encoding] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(VelocityEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension as specified\n",
    "        self.output_dim = n_velocity_encoding\n",
    "        \n",
    "        # Input dimension for velocity features (spherical coordinates)\n",
    "        self.input_dim = 5\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * n_velocity_encoding] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer (5 velocity features in spherical coordinates)\n",
    "        layers.append(nn.Linear(self.input_dim, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.velocity_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, velocity_features):\n",
    "        \"\"\"\n",
    "        Encode the velocity features for hand landmarks.\n",
    "        \n",
    "        Args:\n",
    "            velocity_features: Tensor of shape [..., 5] containing velocity features\n",
    "                              in spherical coordinates. Leading dimensions can be anything\n",
    "                              (batch, sequence, landmark).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., output_dim] with the encoded velocity features\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = velocity_features.shape\n",
    "        \n",
    "        # Reshape to [-1, 5] to process all velocity features in parallel\n",
    "        flat_velocities = velocity_features.reshape(-1, self.input_dim)\n",
    "        \n",
    "        # Apply the velocity encoder\n",
    "        encoded = self.velocity_encoder(flat_velocities)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        # Replace the velocity dimension (5) with output_dim\n",
    "        new_shape = original_shape[:-1] + (self.output_dim,)\n",
    "        reshaped_encoded = encoded.reshape(new_shape)\n",
    "        \n",
    "        return reshaped_encoded\n",
    "    \n",
    "    def encode_all_velocity_windows(self, dom_vel_small, dom_vel_large, non_dom_vel_small, non_dom_vel_large):\n",
    "        \"\"\"\n",
    "        Encode all four velocity window tensors using the same encoder.\n",
    "        \n",
    "        Args:\n",
    "            dom_vel_small: Dominant hand small window velocities [batch_size, seq_len, 20, 5]\n",
    "            dom_vel_large: Dominant hand large window velocities [batch_size, seq_len, 20, 5]\n",
    "            non_dom_vel_small: Non-dominant hand small window velocities [batch_size, seq_len, 20, 5]\n",
    "            non_dom_vel_large: Non-dominant hand large window velocities [batch_size, seq_len, 20, 5]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing encoded velocity features for all windows\n",
    "        \"\"\"\n",
    "        # Process each velocity window\n",
    "        dom_small_encoded = self.forward(dom_vel_small)  # [batch_size, seq_len, 20, output_dim]\n",
    "        dom_large_encoded = self.forward(dom_vel_large)  # [batch_size, seq_len, 20, output_dim]\n",
    "        non_dom_small_encoded = self.forward(non_dom_vel_small)  # [batch_size, seq_len, 20, output_dim]\n",
    "        non_dom_large_encoded = self.forward(non_dom_vel_large)  # [batch_size, seq_len, 20, output_dim]\n",
    "        \n",
    "        return {\n",
    "            'dom_velocity_small_encoded': dom_small_encoded,\n",
    "            'dom_velocity_large_encoded': dom_large_encoded,\n",
    "            'non_dom_velocity_small_encoded': non_dom_small_encoded,\n",
    "            'non_dom_velocity_large_encoded': non_dom_large_encoded\n",
    "        }\n",
    "    \n",
    "\n",
    "def combine_semantic_and_velocity_features(semantic_features, velocity_small_features, velocity_large_features):\n",
    "    \"\"\"\n",
    "    Combines landmark semantic embeddings with velocity features from both time windows.\n",
    "    \n",
    "    This function concatenates:\n",
    "    1. The \"what\" (semantic embedding) of each landmark\n",
    "    2. The \"how fast small window\" (small window velocity encoding)\n",
    "    3. The \"how fast large window\" (large window velocity encoding)\n",
    "    \n",
    "    Args:\n",
    "        semantic_features: Tensor of shape [..., embedding_dim] containing landmark embeddings\n",
    "        velocity_small_features: Tensor of shape [..., n_velocity_encoding] from small window\n",
    "        velocity_large_features: Tensor of shape [..., n_velocity_encoding] from large window\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [..., embedding_dim + 2*n_velocity_encoding] with the combined representation\n",
    "    \"\"\"\n",
    "    batch_shape = velocity_small_features.shape[:-2]\n",
    "    semantic_features_expanded = semantic_features.expand(*batch_shape, -1, -1)\n",
    "    # Verify that the batch dimensions match\n",
    "    assert semantic_features_expanded.shape[:-1] == velocity_small_features.shape[:-1] == velocity_large_features.shape[:-1], \\\n",
    "        \"Batch dimensions of semantic and velocity features must match\"\n",
    "    \n",
    "    # Concatenate all three feature types along the last dimension\n",
    "    combined_features = torch.cat([\n",
    "        semantic_features_expanded,        # Landmark identity (what)\n",
    "        velocity_small_features,  # Short-term movement (how fast recently)\n",
    "        velocity_large_features   # Long-term movement (how fast overall)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "\n",
    "class WristVelocityEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes velocity features of wrist landmarks relative to the nose.\n",
    "    \n",
    "    This network processes the 3 polar coordinate velocity features for each wrist\n",
    "    independently but in parallel, using the same weights across both wrists\n",
    "    and both velocity windows (small and large).\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_velocity_encoding, \n",
    "                 hidden_dims=None, \n",
    "                 num_layers=2,\n",
    "                 activation='relu',\n",
    "                 init_method='kaiming_normal',\n",
    "                 init_gain=1.0,\n",
    "                 init_nonlinearity='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the wrist velocity encoder with customizable architecture.\n",
    "        \n",
    "        Args:\n",
    "            n_velocity_encoding: Output dimension for each wrist's velocity encoding\n",
    "            hidden_dims: List of hidden layer dimensions. If None, uses [4*n_velocity_encoding] * num_layers\n",
    "            num_layers: Number of hidden layers (default: 2)\n",
    "            activation: Activation function to use ('relu', 'leaky_relu', 'gelu', 'silu', 'tanh', etc.)\n",
    "            init_method: Weight initialization method ('kaiming_normal', 'kaiming_uniform', \n",
    "                        'xavier_normal', 'xavier_uniform', 'normal', 'uniform')\n",
    "            init_gain: Gain parameter for certain initialization methods\n",
    "            init_nonlinearity: Nonlinearity parameter for certain initialization methods\n",
    "        \"\"\"\n",
    "        super(WristVelocityEncoder, self).__init__()\n",
    "        \n",
    "        # The output dimension as specified\n",
    "        self.output_dim = n_velocity_encoding\n",
    "        \n",
    "        # Input dimension for wrist velocity features (polar coordinates)\n",
    "        self.input_dim = 3\n",
    "        \n",
    "        # If hidden_dims not provided, create default configuration\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [4 * n_velocity_encoding] * num_layers\n",
    "        \n",
    "        # Get the activation function\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        \n",
    "        # Create layers list starting with input layer\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer (3 velocity features in polar coordinates)\n",
    "        layers.append(nn.Linear(self.input_dim, hidden_dims[0]))\n",
    "        layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(self.activation_fn)\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], self.output_dim))\n",
    "        \n",
    "        # Create the feed-forward network\n",
    "        self.wrist_velocity_encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using the specified method\n",
    "        self._init_weights(init_method, init_gain, init_nonlinearity)\n",
    "    \n",
    "    def _get_activation(self, activation_name):\n",
    "        \"\"\"Get the activation function based on name.\"\"\"\n",
    "        activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'gelu': nn.GELU(),\n",
    "            'silu': nn.SiLU(),  # Also known as Swish\n",
    "            'tanh': nn.Tanh(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'elu': nn.ELU(),\n",
    "            'prelu': nn.PReLU(),\n",
    "        }\n",
    "        \n",
    "        if activation_name.lower() not in activations:\n",
    "            raise ValueError(f\"Activation function '{activation_name}' not supported. \"\n",
    "                           f\"Choose from: {', '.join(activations.keys())}\")\n",
    "        \n",
    "        return activations[activation_name.lower()]\n",
    "    \n",
    "    def _init_weights(self, init_method, gain, nonlinearity):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "                \n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight, a=0.0, nonlinearity=nonlinearity)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
    "            else:\n",
    "                raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "            \n",
    "            # Initialize bias if it exists\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, wrist_velocity_features):\n",
    "        \"\"\"\n",
    "        Encode the velocity features for wrist landmarks.\n",
    "        \n",
    "        Args:\n",
    "            wrist_velocity_features: Tensor of shape [..., 3] containing velocity features\n",
    "                                    in polar coordinates. Leading dimensions can be anything\n",
    "                                    (batch, sequence, wrist).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [..., output_dim] with the encoded velocity features\n",
    "        \"\"\"\n",
    "        # Get the original shape to reshape the output later\n",
    "        original_shape = wrist_velocity_features.shape\n",
    "        \n",
    "        # Reshape to [-1, 3] to process all velocity features in parallel\n",
    "        flat_velocities = wrist_velocity_features.reshape(-1, self.input_dim)\n",
    "        \n",
    "        # Apply the wrist velocity encoder\n",
    "        encoded = self.wrist_velocity_encoder(flat_velocities)\n",
    "        \n",
    "        # Reshape back to original dimensions but with output_dim as the last dimension\n",
    "        # Replace the velocity dimension (3) with output_dim\n",
    "        new_shape = original_shape[:-1] + (self.output_dim,)\n",
    "        reshaped_encoded = encoded.reshape(new_shape)\n",
    "        \n",
    "        return reshaped_encoded\n",
    "    \n",
    "    def encode_both_velocity_windows(self, wrist_vel_small, wrist_vel_large):\n",
    "        \"\"\"\n",
    "        Encode both velocity window tensors for wrists using the same encoder.\n",
    "        \n",
    "        Args:\n",
    "            wrist_vel_small: Wrist small window velocities [batch_size, seq_len, 2, 3]\n",
    "            wrist_vel_large: Wrist large window velocities [batch_size, seq_len, 2, 3]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing encoded velocity features for both windows\n",
    "        \"\"\"\n",
    "        # Process each velocity window\n",
    "        small_window_encoded = self.forward(wrist_vel_small)  # [batch_size, seq_len, 2, output_dim]\n",
    "        large_window_encoded = self.forward(wrist_vel_large)  # [batch_size, seq_len, 2, output_dim]\n",
    "        \n",
    "        return {\n",
    "            'wrist_velocity_small_encoded': small_window_encoded,\n",
    "            'wrist_velocity_large_encoded': large_window_encoded\n",
    "        }\n",
    "    \n",
    "\n",
    "def combine_wrist_embedding_and_velocity(wrist_embeddings, wrist_velocity_small, wrist_velocity_large):\n",
    "    \"\"\"\n",
    "    Combines wrist semantic embeddings with velocity features from both time windows.\n",
    "    \n",
    "    This function handles the specific arrangement of wrist data in your model:\n",
    "    - In embeddings: Wrists are at indices 20 (dom) and 41 (non-dom) in the embedding table\n",
    "    - In velocity tensors: Wrists are at indices 0 (dom) and 1 (non-dom)\n",
    "    \n",
    "    Args:\n",
    "        wrist_embeddings: Tensor of shape [2, embedding_dim] with wrist embeddings\n",
    "                         where [0] is dom wrist and [1] is non-dom wrist\n",
    "        wrist_velocity_small: Tensor of shape [..., 2, n_velocity_encoding] \n",
    "                             from small window velocity encoder\n",
    "        wrist_velocity_large: Tensor of shape [..., 2, n_velocity_encoding] \n",
    "                             from large window velocity encoder\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [..., 2, embedding_dim + 2*n_velocity_encoding] \n",
    "        with the combined representation for both wrists\n",
    "    \"\"\"\n",
    "    # Get the batch dimensions from the velocity tensors\n",
    "    batch_dims = wrist_velocity_small.shape[:-2]\n",
    "    \n",
    "    # Expand wrist embeddings to match the batch dimensions\n",
    "    # From [2, embedding_dim] to [..., 2, embedding_dim]\n",
    "    expanded_embeddings = wrist_embeddings.expand(*batch_dims, -1, -1)\n",
    "    \n",
    "    # Verify that the shapes are compatible for concatenation\n",
    "    assert expanded_embeddings.shape[:-1] == wrist_velocity_small.shape[:-1] == wrist_velocity_large.shape[:-1], \\\n",
    "        \"Batch dimensions of embeddings and velocity features must match\"\n",
    "    \n",
    "    # Concatenate along the last dimension\n",
    "    combined_features = torch.cat([\n",
    "        expanded_embeddings,      # Wrist identity (what)\n",
    "        wrist_velocity_small,     # Short-term movement (how fast recently)\n",
    "        wrist_velocity_large      # Long-term movement (how fast overall)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "\n",
    "class LandmarkTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder for processing hand landmarks and learning contextual relationships.\n",
    "    \n",
    "    This module treats the set of landmarks as a sequence and applies self-attention\n",
    "    to learn the relationships between different parts of the hand.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 num_layers=2,\n",
    "                 num_heads=8,\n",
    "                 hidden_dim=None,\n",
    "                 ff_dim=None,\n",
    "                 prenorm=True,\n",
    "                 activation='gelu',\n",
    "                 init_method='xavier_uniform',\n",
    "                 init_gain=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the landmark transformer encoder.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input features per landmark (3*embedding_dim)\n",
    "            num_layers: Number of transformer encoder layers\n",
    "            num_heads: Number of attention heads\n",
    "            hidden_dim: Hidden dimension size (if None, uses input_dim)\n",
    "            ff_dim: Feed-forward dimension (if None, uses 4*hidden_dim)\n",
    "            prenorm: Whether to use pre-norm (True) or post-norm (False) architecture\n",
    "            activation: Activation function in feed-forward network\n",
    "            init_method: Weight initialization method\n",
    "            init_gain: Gain parameter for initialization\n",
    "        \"\"\"\n",
    "        super(LandmarkTransformerEncoder, self).__init__()\n",
    "        \n",
    "        # Set dimensions\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim if hidden_dim is not None else input_dim\n",
    "        self.ff_dim = ff_dim if ff_dim is not None else 4 * self.hidden_dim\n",
    "        \n",
    "        # Input projection if needed\n",
    "        self.input_projection = None\n",
    "        if self.input_dim != self.hidden_dim:\n",
    "            self.input_projection = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        \n",
    "        # Create transformer encoder layers\n",
    "        encoder_layer = LandmarkTransformerLayer(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=self.ff_dim,\n",
    "            prenorm=prenorm,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        \n",
    "        # Final normalization\n",
    "        self.norm = nn.LayerNorm(self.hidden_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights(init_method, init_gain)\n",
    "    \n",
    "    def _init_weights(self, init_method, gain):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if init_method == 'xavier_uniform':\n",
    "                    nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "                elif init_method == 'xavier_normal':\n",
    "                    nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "                elif init_method == 'kaiming_uniform':\n",
    "                    nn.init.kaiming_uniform_(module.weight, a=0, mode='fan_in')\n",
    "                elif init_method == 'kaiming_normal':\n",
    "                    nn.init.kaiming_normal_(module.weight, a=0, mode='fan_in')\n",
    "                else:\n",
    "                    raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "                \n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process hand landmarks through the transformer.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, seq_len, 20, input_dim]\n",
    "               where 20 is the number of landmarks and input_dim is 3*embedding_dim\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, 20, hidden_dim]\n",
    "            with contextually enriched landmark representations\n",
    "        \"\"\"\n",
    "        # Get original shape\n",
    "        batch_size, seq_len, num_landmarks, _ = x.shape\n",
    "        \n",
    "        # Reshape to process each frame separately\n",
    "        # [batch_size * seq_len, 20, input_dim]\n",
    "        x_reshaped = x.reshape(-1, num_landmarks, self.input_dim)\n",
    "        \n",
    "        # Apply input projection if needed\n",
    "        if self.input_projection is not None:\n",
    "            x_reshaped = self.input_projection(x_reshaped)\n",
    "        \n",
    "        # Process through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x_reshaped = layer(x_reshaped)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        x_reshaped = self.norm(x_reshaped)\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        # [batch_size, seq_len, 20, hidden_dim]\n",
    "        output = x_reshaped.reshape(batch_size, seq_len, num_landmarks, self.hidden_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class LandmarkTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer encoder layer for landmark processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, prenorm=True, activation='gelu'):\n",
    "        \"\"\"\n",
    "        Initialize a transformer encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            hidden_dim: Hidden dimension size\n",
    "            num_heads: Number of attention heads\n",
    "            ff_dim: Feed-forward dimension\n",
    "            prenorm: Whether to use pre-norm (True) or post-norm (False)\n",
    "            activation: Activation function in feed-forward network\n",
    "        \"\"\"\n",
    "        super(LandmarkTransformerLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prenorm = prenorm\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, ff_dim),\n",
    "            self._get_activation(activation),\n",
    "            nn.Linear(ff_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalizations\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def _get_activation(self, name):\n",
    "        \"\"\"Get activation function by name.\"\"\"\n",
    "        if name.lower() == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif name.lower() == 'gelu':\n",
    "            return nn.GELU()\n",
    "        elif name.lower() == 'silu' or name.lower() == 'swish':\n",
    "            return nn.SiLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{name}' not supported.\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process landmarks through a transformer layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size*seq_len, 20, hidden_dim]\n",
    "               representing landmarks in a single frame\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of same shape with contextualized representations\n",
    "        \"\"\"\n",
    "        # Pre-norm or post-norm architecture\n",
    "        if self.prenorm:\n",
    "            # Pre-norm: Apply normalization before attention\n",
    "            norm_x = self.norm1(x)\n",
    "            attn_output, _ = self.self_attention(norm_x, norm_x, norm_x)\n",
    "            x = x + attn_output  # Residual connection\n",
    "            \n",
    "            # Feed-forward with normalization\n",
    "            norm_x = self.norm2(x)\n",
    "            ff_output = self.ff_network(norm_x)\n",
    "            x = x + ff_output  # Residual connection\n",
    "        else:\n",
    "            # Post-norm: Apply attention then normalization\n",
    "            attn_output, _ = self.self_attention(x, x, x)\n",
    "            x = self.norm1(x + attn_output)  # Residual connection and norm\n",
    "            \n",
    "            # Feed-forward and normalization\n",
    "            ff_output = self.ff_network(x)\n",
    "            x = self.norm2(x + ff_output)  # Residual connection and norm\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class LandmarkAttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies attention pooling over landmarks using PyTorch's MultiheadAttention.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the attention pooling module.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input features per landmark\n",
    "            output_dim: Dimension of the output representation\n",
    "        \"\"\"\n",
    "        super(LandmarkAttentionPooling, self).__init__()\n",
    "        \n",
    "        # Using PyTorch's built-in attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=input_dim,\n",
    "            num_heads=1,  # Single head is sufficient for pooling\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_dim = output_dim\n",
    "        # Learnable query vector\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(input_dim, self.output_dim)\n",
    "        \n",
    "        # Layer normalization for stability\n",
    "        self.layer_norm = nn.LayerNorm(input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply attention pooling over landmarks.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, seq_len, num_landmarks, input_dim]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, output_dim]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, num_landmarks, input_dim = x.shape\n",
    "        \n",
    "        # Reshape to process each sequence element separately\n",
    "        x_reshaped = x.reshape(batch_size * seq_len, num_landmarks, input_dim)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x_norm = self.layer_norm(x_reshaped)\n",
    "        \n",
    "        # Expand query to match the batch size\n",
    "        query = self.query.expand(batch_size * seq_len, -1, -1)\n",
    "        \n",
    "        # Apply attention\n",
    "        # The query attends to all landmarks (keys and values are the same: x_norm)\n",
    "        pooled, _ = self.attention(query, x_norm, x_norm)\n",
    "        \n",
    "        # Remove the sequence dimension (which is 1 for the query)\n",
    "        pooled = pooled.squeeze(1)  # [batch_size * seq_len, input_dim]\n",
    "        \n",
    "        # Project to output dimension\n",
    "        output = self.output_projection(pooled)  # [batch_size * seq_len, output_dim]\n",
    "        \n",
    "        # Reshape back to [batch_size, seq_len, output_dim]\n",
    "        output = output.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "def concat_pooled_wrists(pooled, wrist):\n",
    "# Verify that the shapes are compatible for concatenation\n",
    "    assert pooled.shape[:-1] == wrist.shape[:-1], \\\n",
    "        \"Batch dimensions of embeddings and spatial features must match\"\n",
    "    \n",
    "    # Concatenate along the last dimension\n",
    "    combined_features = torch.cat([\n",
    "        pooled,     # Wrist identity (what)\n",
    "        wrist   # Wrist position (where)\n",
    "    ], dim=-1)\n",
    "    \n",
    "    return combined_features\n",
    "\n",
    "\n",
    "\n",
    "class ConfidenceWeightedTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder that incorporates confidence scores into attention calculations.\n",
    "    \n",
    "    This second-stage transformer learns relationships between the two hands while\n",
    "    taking into account confidence and interpolation scores from both spatial and\n",
    "    velocity features.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 num_layers=2,\n",
    "                 num_heads=8,\n",
    "                 hidden_dim=None,\n",
    "                 ff_dim=None,\n",
    "                 prenorm=True,\n",
    "                 activation='gelu',\n",
    "                 init_method='xavier_uniform',\n",
    "                 init_gain=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the confidence-weighted transformer encoder.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of input features per hand\n",
    "            num_layers: Number of transformer encoder layers\n",
    "            num_heads: Number of attention heads\n",
    "            hidden_dim: Hidden dimension size (if None, uses input_dim)\n",
    "            ff_dim: Feed-forward dimension (if None, uses 4*hidden_dim)\n",
    "            prenorm: Whether to use pre-norm (True) or post-norm (False) architecture\n",
    "            activation: Activation function in feed-forward network\n",
    "            init_method: Weight initialization method\n",
    "            init_gain: Gain parameter for initialization\n",
    "        \"\"\"\n",
    "        super(ConfidenceWeightedTransformerEncoder, self).__init__()\n",
    "        \n",
    "        # Set dimensions\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim if hidden_dim is not None else input_dim\n",
    "        self.ff_dim = ff_dim if ff_dim is not None else 4 * self.hidden_dim\n",
    "        \n",
    "        # Input projection if needed\n",
    "        self.input_projection = None\n",
    "        if self.input_dim != self.hidden_dim:\n",
    "            self.input_projection = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        \n",
    "        # Create transformer encoder layers with confidence weighting\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(\n",
    "                ConfidenceWeightedTransformerLayer(\n",
    "                    hidden_dim=self.hidden_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    ff_dim=self.ff_dim,\n",
    "                    prenorm=prenorm,\n",
    "                    activation=activation\n",
    "                )\n",
    "            )\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "        # Final normalization\n",
    "        self.norm = nn.LayerNorm(self.hidden_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights(init_method, init_gain)\n",
    "    \n",
    "    def _init_weights(self, init_method, gain):\n",
    "        \"\"\"Initialize the weights using the specified method.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if init_method == 'xavier_uniform':\n",
    "                    nn.init.xavier_uniform_(module.weight, gain=gain)\n",
    "                elif init_method == 'xavier_normal':\n",
    "                    nn.init.xavier_normal_(module.weight, gain=gain)\n",
    "                elif init_method == 'kaiming_uniform':\n",
    "                    nn.init.kaiming_uniform_(module.weight, a=0, mode='fan_in')\n",
    "                elif init_method == 'kaiming_normal':\n",
    "                    nn.init.kaiming_normal_(module.weight, a=0, mode='fan_in')\n",
    "                else:\n",
    "                    raise ValueError(f\"Initialization method '{init_method}' not supported.\")\n",
    "                \n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x, confidence_scores):\n",
    "        \"\"\"\n",
    "        Process hand features through the transformer with confidence weighting.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, seq_len, 2, input_dim]\n",
    "               where 2 represents the dom and non-dom hands\n",
    "            confidence_scores: Dictionary containing:\n",
    "                - Cd_spatial: [batch_size, seq_len, 2] confidence scores\n",
    "                - Ci_spatial: [batch_size, seq_len, 2] interpolation scores\n",
    "                - Cd_velocity: [batch_size, seq_len, 2] velocity calculation confidence\n",
    "                - Ci_velocity: [batch_size, seq_len, 2] velocity confidence\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, 2, hidden_dim]\n",
    "            with confidence-weighted contextual representations\n",
    "        \"\"\"\n",
    "        # Get original shape\n",
    "        batch_size, seq_len, num_hands, _ = x.shape\n",
    "        \n",
    "        # Reshape to process each frame separately\n",
    "        # [batch_size * seq_len, 2, input_dim]\n",
    "        x_reshaped = x.reshape(-1, num_hands, self.input_dim)\n",
    "        \n",
    "        # Apply input projection if needed\n",
    "        if self.input_projection is not None:\n",
    "            x_reshaped = self.input_projection(x_reshaped)\n",
    "        \n",
    "        # Reshape confidence scores for per-frame processing\n",
    "        conf_scores_reshaped = {}\n",
    "        for key, tensor in confidence_scores.items():\n",
    "            conf_scores_reshaped[key] = tensor.reshape(-1, num_hands)\n",
    "        \n",
    "        # Process through transformer layers\n",
    "        for layer in self.layers:\n",
    "            x_reshaped = layer(x_reshaped, conf_scores_reshaped)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        x_reshaped = self.norm(x_reshaped)\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        # [batch_size, seq_len, 2, hidden_dim]\n",
    "        output = x_reshaped.reshape(batch_size, seq_len, num_hands, self.hidden_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class ConfidenceWeightedTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder layer with confidence-weighted attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, num_heads, ff_dim, prenorm=True, activation='gelu'):\n",
    "        \"\"\"\n",
    "        Initialize a confidence-weighted transformer encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            hidden_dim: Hidden dimension size\n",
    "            num_heads: Number of attention heads\n",
    "            ff_dim: Feed-forward dimension\n",
    "            prenorm: Whether to use pre-norm (True) or post-norm (False)\n",
    "            activation: Activation function in feed-forward network\n",
    "        \"\"\"\n",
    "        super(ConfidenceWeightedTransformerLayer, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.prenorm = prenorm\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Custom attention with confidence weighting\n",
    "        self.self_attention = ConfidenceWeightedAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, ff_dim),\n",
    "            self._get_activation(activation),\n",
    "            nn.Linear(ff_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalizations\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def _get_activation(self, name):\n",
    "        \"\"\"Get activation function by name.\"\"\"\n",
    "        if name.lower() == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif name.lower() == 'gelu':\n",
    "            return nn.GELU()\n",
    "        elif name.lower() == 'silu' or name.lower() == 'swish':\n",
    "            return nn.SiLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{name}' not supported.\")\n",
    "    \n",
    "    def forward(self, x, confidence_scores):\n",
    "        \"\"\"\n",
    "        Process through a transformer layer with confidence-weighted attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size*seq_len, 2, hidden_dim]\n",
    "            confidence_scores: Dictionary of confidence scores\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of same shape with contextualized representations\n",
    "        \"\"\"\n",
    "        # Pre-norm or post-norm architecture\n",
    "        if self.prenorm:\n",
    "            # Pre-norm: Apply normalization before attention\n",
    "            norm_x = self.norm1(x)\n",
    "            attn_output = self.self_attention(norm_x, norm_x, norm_x, confidence_scores)\n",
    "            x = x + attn_output  # Residual connection\n",
    "            \n",
    "            # Feed-forward with normalization\n",
    "            norm_x = self.norm2(x)\n",
    "            ff_output = self.ff_network(norm_x)\n",
    "            x = x + ff_output  # Residual connection\n",
    "        else:\n",
    "            # Post-norm: Apply attention then normalization\n",
    "            attn_output = self.self_attention(x, x, x, confidence_scores)\n",
    "            x = self.norm1(x + attn_output)  # Residual connection and norm\n",
    "            \n",
    "            # Feed-forward and normalization\n",
    "            ff_output = self.ff_network(x)\n",
    "            x = self.norm2(x + ff_output)  # Residual connection and norm\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class ConfidenceWeightedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention with confidence weighting.\n",
    "    \n",
    "    This applies the formula:\n",
    "    Attention(Q,K,V,Cd_spatial,Ci_spatial,Cd_velocity,Ci_velocity) = \n",
    "        softmax(QK^T/sqrt(dk) + f(Cd_spatial,Ci_spatial,Cd_velocity,Ci_velocity))V\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(ConfidenceWeightedAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Learnable parameters for confidence weighting\n",
    "        self.a = nn.Parameter(torch.zeros(1))  # For Cd_spatial\n",
    "        self.b = nn.Parameter(torch.zeros(1))  # For Cd_velocity\n",
    "        self.c = nn.Parameter(torch.zeros(1))  # For Ci_spatial\n",
    "        self.d = nn.Parameter(torch.zeros(1))  # For Ci_velocity\n",
    "        \n",
    "        # Small epsilon to avoid log(0)\n",
    "        self.epsilon = 0.01\n",
    "    \n",
    "  \n",
    "    def compute_confidence_weights(self, confidence_scores):\n",
    "        \"\"\"\n",
    "        Compute the confidence weighting matrix f(Cd_spatial, Ci_spatial, Cd_velocity, Ci_velocity).\n",
    "\n",
    "        Args:\n",
    "            confidence_scores: Dictionary with confidence score tensors of shape [flattened_batch_size, 2]\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [flattened_batch_size, 2, 2] for weighting attention scores\n",
    "        \"\"\"\n",
    "        Cd_spatial = confidence_scores['Cd_spatial']\n",
    "        Ci_spatial = confidence_scores['Ci_spatial']\n",
    "        Cd_velocity = confidence_scores['Cd_velocity']\n",
    "        Ci_velocity = confidence_scores['Ci_velocity']\n",
    "\n",
    "        \n",
    "        # These tensors have shape [flattened_batch_size, 2]\n",
    "        flattened_batch_size, num_hands = Cd_spatial.shape\n",
    "\n",
    "        # Apply the confidence weighting formula\n",
    "        f_values = (\n",
    "            torch.log2(self.epsilon + Cd_spatial) * torch.sigmoid(self.a) * 0.25 +\n",
    "            torch.log2(self.epsilon + Cd_velocity) * torch.sigmoid(self.b) * 0.25 +\n",
    "            torch.log2(self.epsilon + Ci_spatial) * torch.sigmoid(self.c) * 0.5 +\n",
    "            torch.log2(self.epsilon + Ci_velocity) * torch.sigmoid(self.d) * 0.5\n",
    "        )\n",
    "    \n",
    "        \n",
    "        # Create the 2x2 matrix for each batch item where columns have same values\n",
    "        confidence_matrix = f_values.unsqueeze(1).expand(-1, num_hands, -1)\n",
    "        confidence_matrix = torch.clamp(confidence_matrix, min=-1e9, max=1e9)\n",
    "        \n",
    "        return confidence_matrix\n",
    "    \n",
    "    def forward(self, query, key, value, confidence_scores):\n",
    "        \"\"\"\n",
    "        Apply confidence-weighted attention.\n",
    "        \n",
    "        Args:\n",
    "            query, key, value: Tensors of shape [batch_size*seq_len, num_hands, embed_dim]\n",
    "                              where batch_size*seq_len represents flattened batch and sequence dimensions\n",
    "            confidence_scores: Dictionary of confidence scores\n",
    "            \n",
    "        Returns:\n",
    "            Attention output tensor of same shape\n",
    "        \"\"\"\n",
    "        # Get the shape components - note there's no separate sequence dimension here!\n",
    "        flattened_batch_size, num_hands, embed_dim = query.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.q_proj(query)  # [flattened_batch_size, num_hands, embed_dim]\n",
    "        k = self.k_proj(key)    # [flattened_batch_size, num_hands, embed_dim]\n",
    "        v = self.v_proj(value)  # [flattened_batch_size, num_hands, embed_dim]\n",
    "        \n",
    "        # Compute confidence weights\n",
    "        # This should return: [flattened_batch_size, num_hands, num_hands]\n",
    "        confidence_weights = self.compute_confidence_weights(confidence_scores)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # Split embed_dim into num_heads  head_dim\n",
    "        q = q.reshape(flattened_batch_size, num_hands, self.num_heads, self.head_dim)\n",
    "        q = q.permute(0, 2, 1, 3)  # [flattened_batch_size, num_heads, num_hands, head_dim]\n",
    "        \n",
    "        k = k.reshape(flattened_batch_size, num_hands, self.num_heads, self.head_dim)\n",
    "        k = k.permute(0, 2, 1, 3)  # [flattened_batch_size, num_heads, num_hands, head_dim]\n",
    "        \n",
    "        v = v.reshape(flattened_batch_size, num_hands, self.num_heads, self.head_dim)\n",
    "        v = v.permute(0, 2, 1, 3)  # [flattened_batch_size, num_heads, num_hands, head_dim]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        # [flattened_batch_size, num_heads, num_hands, num_hands]\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Add confidence weights to attention scores\n",
    "        # Expand confidence_weights for all heads\n",
    "        # [flattened_batch_size, 1, num_hands, num_hands]\n",
    "        confidence_weights = confidence_weights.unsqueeze(1)\n",
    "        \n",
    "        # Add confidence weights to attention scores\n",
    "        attention_scores = attention_scores + confidence_weights\n",
    "        attention_scores = torch.clamp(attention_scores, min=-1e9, max=1e9)\n",
    "\n",
    "        all_neg = (attention_scores < -1e8).all(dim=-1, keepdim=True)\n",
    "        attention_scores = torch.where(all_neg, torch.zeros_like(attention_scores), attention_scores)\n",
    "\n",
    "        attn_max, _ = torch.max(attention_scores, dim=-1, keepdim=True)\n",
    "        attention_scores = attention_scores - attn_max.detach()\n",
    "        # Apply softmax\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # [flattened_batch_size, num_heads, num_hands, head_dim]\n",
    "        context = torch.matmul(attention_probs, v)\n",
    "        \n",
    "        # Reshape back\n",
    "        context = context.permute(0, 2, 1, 3)  # [flattened_batch_size, num_hands, num_heads, head_dim]\n",
    "        context = context.reshape(flattened_batch_size, num_hands, embed_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.out_proj(context)  # [flattened_batch_size, num_hands, embed_dim]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class TemporalDownsampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduces frame count using 1D convolution with configurable parameters.\n",
    "    \n",
    "    This module applies a 1D convolution across the temporal dimension,\n",
    "    effectively reducing the number of frames while preserving important\n",
    "    temporal information through learned filters.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_channels=None, \n",
    "                 kernel_size=3, \n",
    "                 stride=2,\n",
    "                 activation='relu',\n",
    "                 norm_layer=True):\n",
    "        \"\"\"\n",
    "        Initialize the temporal downsampler.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input dimension (d) - feature size per frame\n",
    "            output_channels: Number of convolutional filters (C), defaults to input_dim\n",
    "            kernel_size: Size of the convolutional kernel (k)\n",
    "            stride: Stride of the convolution, controls downsampling factor\n",
    "            activation: Activation function ('relu', 'gelu', None)\n",
    "            norm_layer: Whether to include layer normalization after convolution\n",
    "        \"\"\"\n",
    "        super(TemporalDownsampler, self).__init__()\n",
    "        \n",
    "        # Default output channels to input dimension if not specified\n",
    "        self.output_channels = input_dim if output_channels is None else output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Calculate padding to maintain temporal alignment\n",
    "        # For even kernel sizes, we'll use asymmetric padding later\n",
    "        self.padding = (kernel_size - 1) // 2\n",
    "        self.is_even_kernel = (kernel_size % 2 == 0)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=self.output_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=self.padding,  # This will be adjusted for even kernels\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(self.output_channels) if norm_layer else None\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation is None:\n",
    "            self.activation = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply temporal downsampling to the input sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, n_frames, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, n_frames/stride, output_channels]\n",
    "        \"\"\"\n",
    "        batch_size, n_frames, input_dim = x.shape\n",
    "        \n",
    "        # Reshape for conv1d which expects [batch_size, channels, length]\n",
    "        x = x.permute(0, 2, 1)  # -> [batch_size, input_dim, n_frames]\n",
    "        \n",
    "        # Handle even-sized kernels with asymmetric padding if needed\n",
    "        if self.is_even_kernel:\n",
    "            # For even kernels, PyTorch padding is not symmetric\n",
    "            # We'll pad manually to handle this\n",
    "            pad_size = (self.kernel_size - 1) // 2\n",
    "            x = nn.functional.pad(x, (pad_size, pad_size+1), mode='constant', value=0)\n",
    "            \n",
    "        # Apply convolution\n",
    "        x = self.conv(x)  # -> [batch_size, output_channels, n_frames/stride]\n",
    "        \n",
    "        # Reshape back to [batch_size, n_frames/stride, output_channels]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply normalization if specified\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        # Apply activation if specified\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compute_output_shape(self, input_length):\n",
    "        \"\"\"\n",
    "        Calculate the output sequence length given the input length.\n",
    "        \n",
    "        Args:\n",
    "            input_length: Length of the input sequence (n_frames)\n",
    "            \n",
    "        Returns:\n",
    "            Length of the output sequence\n",
    "        \"\"\"\n",
    "        # For even kernels with our manual padding\n",
    "        if self.is_even_kernel:\n",
    "            padding = self.padding + 1\n",
    "        else:\n",
    "            padding = self.padding\n",
    "        \n",
    "        # Standard formula for conv output shape\n",
    "        return math.floor((input_length + 2 * padding - self.kernel_size) / self.stride + 1)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create fixed positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x, scale=1.0):\n",
    "        \"\"\"\n",
    "        Add positional encodings to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [batch_size, seq_len, d_model]\n",
    "            scale: Scaling factor for the positional encodings\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with added positional encodings\n",
    "        \"\"\"\n",
    "        x = x + (self.pe[:, :x.size(1), :] * scale)\n",
    "        return x\n",
    "    \n",
    "class MultiScaleTemporalTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer that processes sequences with multi-scale temporal attention.\n",
    "    \n",
    "    Uses exactly three attention heads:\n",
    "    - Short-term head: Attends to frames within 5 frames\n",
    "    - Medium-term head: Attends to frames within 15 frames\n",
    "    - Long-term head: Attends to frames within 45 frames\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 num_layers=4,\n",
    "                 short_range=5,\n",
    "                 medium_range=15,\n",
    "                 long_range=45,\n",
    "                 dim_feedforward=2048,\n",
    "                 activation='gelu',\n",
    "                 stride=2):  \n",
    "        \"\"\"\n",
    "        Initialize the multi-scale temporal transformer.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension / feature size\n",
    "            num_layers: Number of transformer encoder layers\n",
    "            short_range: Range for short-term attention (frames)\n",
    "            medium_range: Range for medium-term attention (frames)\n",
    "            long_range: Range for long-term attention (frames)\n",
    "            dim_feedforward: Dimension of feedforward network\n",
    "            activation: Activation function type\n",
    "            stride: Stride used in downsampling (needed for mask adjustment)\n",
    "        \"\"\"\n",
    "        super(MultiScaleTemporalTransformer, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.d_model = d_model\n",
    "        self.total_heads = 3  # Exactly 3 heads\n",
    "        self.head_ranges = {\n",
    "            'short': short_range,\n",
    "            'medium': medium_range,\n",
    "            'long': long_range\n",
    "        }\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Create transformer layers\n",
    "        encoder_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            encoder_layers.append(\n",
    "                MultiScaleTransformerEncoderLayer(\n",
    "                    d_model=d_model,\n",
    "                    head_ranges=self.head_ranges,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    activation=activation\n",
    "                )\n",
    "            )\n",
    "        self.layers = nn.ModuleList(encoder_layers)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, src, mask=None):\n",
    "        \"\"\"\n",
    "        Process the input sequence through the transformer.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor [batch_size, seq_len_downsampled, d_model]\n",
    "            mask: Boolean mask [batch_size, seq_len_original] where True indicates valid frames\n",
    "                 and False indicates padding frames\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of same shape as input with multi-scale temporal context\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        \n",
    "        # Adjust mask for downsampled sequence length\n",
    "        if mask is not None:\n",
    "            # Subsample the mask to match downsampled sequence\n",
    "            # Take every stride-th element, starting from 0\n",
    "            # This accounts for how conv1d downsampling affects the sequence length\n",
    "            downsample_mask = mask[:, ::self.stride]\n",
    "            \n",
    "            # Make sure downsampled mask matches sequence length\n",
    "            # It might be off by 1 due to padding in conv1d\n",
    "            if downsample_mask.shape[1] > src.shape[1]:\n",
    "                downsample_mask = downsample_mask[:, :src.shape[1]]\n",
    "            elif downsample_mask.shape[1] < src.shape[1]:\n",
    "                # This shouldn't normally happen, but just in case\n",
    "                pad_size = src.shape[1] - downsample_mask.shape[1]\n",
    "                pad = torch.zeros((downsample_mask.shape[0], pad_size), dtype=torch.bool, device=mask.device)\n",
    "                downsample_mask = torch.cat([downsample_mask, pad], dim=1)\n",
    "            \n",
    "            # Convert from True=valid to True=padding format used by transformer\n",
    "            padding_mask = ~downsample_mask\n",
    "        else:\n",
    "            padding_mask = None\n",
    "        \n",
    "        # Pass through each transformer layer\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, padding_mask=padding_mask)\n",
    "        \n",
    "        # Apply final normalization\n",
    "        output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiScaleTransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder layer with multi-scale temporal attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 head_ranges,\n",
    "                 dim_feedforward=2048, \n",
    "                 activation=\"gelu\"):\n",
    "        super(MultiScaleTransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-scale attention\n",
    "        self.self_attn = MultiScaleAttention(\n",
    "            embed_dim=d_model,\n",
    "            head_ranges=head_ranges\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = F.gelu\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    \n",
    "    def forward(self, src, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer encoder layer.\n",
    "        \n",
    "        Args:\n",
    "            src: Input tensor [batch_size, seq_len_downsampled, d_model]\n",
    "            padding_mask: Boolean mask [batch_size, seq_len_downsampled] \n",
    "                         where True indicates padding\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of the same shape\n",
    "        \"\"\"\n",
    "        # Multi-scale attention with residual connection\n",
    "        src2 = self.norm1(src)\n",
    "        src2 = self.self_attn(src2, src2, src2, padding_mask=padding_mask)\n",
    "        src = src + src2\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.activation(self.linear1(src2)))\n",
    "        src = src + src2\n",
    "        \n",
    "        return src\n",
    "\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention where different heads attend to different temporal ranges.\n",
    "    Uses exactly 3 heads: short, medium, and long-term.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, head_ranges):\n",
    "        super(MultiScaleAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_ranges = head_ranges\n",
    "        self.total_heads = 3  # Fixed: one head per range\n",
    "        \n",
    "        assert embed_dim % self.total_heads == 0, \"embed_dim must be divisible by 3\"\n",
    "        self.head_dim = embed_dim // self.total_heads\n",
    "        \n",
    "        # Create linear projections\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Head indices (fixed for 3 heads)\n",
    "        self.head_indices = {\n",
    "            'short': (0, 1),\n",
    "            'medium': (1, 2),\n",
    "            'long': (2, 3)\n",
    "        }\n",
    "    \n",
    "    def forward(self, query, key, value, padding_mask=None):\n",
    "        \"\"\"\n",
    "        Apply multi-scale attention.\n",
    "        \n",
    "        Args:\n",
    "            query, key, value: Input tensors [batch_size, seq_len, embed_dim]\n",
    "            padding_mask: Boolean mask [batch_size, seq_len] where True indicates padding frames\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len, _ = query.shape\n",
    "        src_len = key.shape[1]\n",
    "        \n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        q = self.q_proj(query).view(batch_size, tgt_len, self.total_heads, self.head_dim)\n",
    "        k = self.k_proj(key).view(batch_size, src_len, self.total_heads, self.head_dim)\n",
    "        v = self.v_proj(value).view(batch_size, src_len, self.total_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q = q.transpose(1, 2)  # [batch_size, total_heads, tgt_len, head_dim]\n",
    "        k = k.transpose(1, 2)  # [batch_size, total_heads, src_len, head_dim]\n",
    "        v = v.transpose(1, 2)  # [batch_size, total_heads, src_len, head_dim]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_output = self._multi_scale_attention(q, k, v, tgt_len, src_len, padding_mask)\n",
    "        \n",
    "        # Reshape and apply final projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, tgt_len, self.embed_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _multi_scale_attention(self, q, k, v, tgt_len, src_len, padding_mask):\n",
    "        \"\"\"\n",
    "        Apply attention with different temporal ranges for different heads.\n",
    "        \"\"\"\n",
    "        # Compute scaled dot-product attention\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Create temporal range masks for each head type\n",
    "        temporal_masks = self._create_temporal_masks(tgt_len, src_len, device=q.device)\n",
    "        \n",
    "        # Apply padding mask if provided\n",
    "        if padding_mask is not None:\n",
    "            # Convert mask from [batch_size, seq_len] to [batch_size, 1, 1, seq_len]\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "            # True values are masked positions (set to -inf)\n",
    "            attn_weights = attn_weights.masked_fill(padding_mask, -1e9)\n",
    "        \n",
    "        # Apply the temporal masks\n",
    "        for scale, (start_idx, end_idx) in self.head_indices.items():\n",
    "            mask = temporal_masks[scale]\n",
    "            attn_weights[:, start_idx:end_idx] = attn_weights[:, start_idx:end_idx].masked_fill(mask, -1e9)\n",
    "        \n",
    "\n",
    "        mask_check = (attn_weights <= -1e9).all(dim=-1, keepdim=True)\n",
    "        attn_weights = attn_weights.masked_fill(mask_check, 0.0)\n",
    "    \n",
    "        attn_weights_max, _ = torch.max(attn_weights, dim=-1, keepdim=True)\n",
    "        attn_weights = attn_weights - attn_weights_max.detach()\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _create_temporal_masks(self, tgt_len, src_len, device):\n",
    "        \"\"\"\n",
    "        Create masks to restrict attention to specific temporal ranges.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of masks for each head type\n",
    "        \"\"\"\n",
    "        temporal_masks = {}\n",
    "        \n",
    "        # Create position indices\n",
    "        pos_i = torch.arange(tgt_len, device=device).unsqueeze(1)\n",
    "        pos_j = torch.arange(src_len, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Calculate distance between positions\n",
    "        dist = torch.abs(pos_i - pos_j)  # [tgt_len, src_len]\n",
    "        \n",
    "        # Create masks for each temporal range\n",
    "        for scale, range_val in self.head_ranges.items():\n",
    "            # True where attention should be blocked (outside of the range)\n",
    "            mask = dist > range_val\n",
    "            # Expand for batch dimension and appropriate number of heads\n",
    "            # Shape: [1, 1, tgt_len, src_len]\n",
    "            temporal_masks[scale] = mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        return temporal_masks\n",
    "    \n",
    "def compute_minimum_loss(target_labels, label_mask=None):\n",
    "    \"\"\"\n",
    "    Compute the theoretical minimum loss (entropy of target distribution)\n",
    "    \"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    # Calculate entropy for each position: -(p_i * log(p_i))\n",
    "    position_entropy = -(target_labels * torch.log(target_labels + epsilon)).sum(dim=2)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if label_mask is not None:\n",
    "        position_entropy = position_entropy * label_mask.float()\n",
    "        # Average entropy over valid tokens\n",
    "        min_loss = position_entropy.sum() / label_mask.sum().clamp(min=1)\n",
    "    else:\n",
    "        # If no mask, use all tokens\n",
    "        min_loss = position_entropy.mean()\n",
    "    \n",
    "    return min_loss\n",
    "\n",
    "def optimized_semantic_smoothing_loss(logits, L_index, L_values, label_mask=None):\n",
    "    \"\"\"\n",
    "    Highly optimized semantic smoothing loss using a single scatter operation.\n",
    "    No loops needed!\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    \n",
    "    # Create target label distributions all at once\n",
    "    target_labels = torch.zeros(batch_size, seq_len, vocab_size, device=logits.device)\n",
    "    target_labels.scatter_(2, L_index, L_values)\n",
    "    \n",
    "    # Apply log_softmax to get log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Compute loss (batch_size, seq_len)\n",
    "    token_losses = -(target_labels * log_probs).sum(dim=2)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if label_mask is not None:\n",
    "        token_losses = token_losses * label_mask.float()\n",
    "        # Average loss over valid tokens\n",
    "        total_loss = token_losses.sum() / label_mask.sum().clamp(min=1)\n",
    "    else:\n",
    "        # If no mask, use all tokens\n",
    "        total_loss = token_losses.mean()\n",
    "    \n",
    "    return total_loss  \n",
    "\n",
    "class OptimizedCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention using PyTorch's optimized MultiheadAttention implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=768, num_heads=12):\n",
    "        super(OptimizedCrossAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # PyTorch's optimized multi-head attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True  # Important for our [batch, seq, features] format\n",
    "        )\n",
    "        \n",
    "        # Layer normalization for pre-norm architecture (like GPT-2)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, hidden_states, video_representations, video_mask=None, stride=1):\n",
    "        \"\"\"\n",
    "        Compute cross-attention between GPT token representations and video frames.\n",
    "        \"\"\"\n",
    "        # Apply layer normalization to hidden states (pre-norm approach)\n",
    "        hidden_states = torch.where(torch.isnan(hidden_states), \n",
    "                                   torch.zeros_like(hidden_states), \n",
    "                                   hidden_states)\n",
    "        \n",
    "        video_representations = torch.where(torch.isnan(video_representations), \n",
    "                                          torch.zeros_like(video_representations), \n",
    "                                          video_representations)\n",
    "        query = self.layer_norm(hidden_states)\n",
    "        \n",
    "        # Handle strided video mask\n",
    "        if video_mask is not None and stride > 1:\n",
    "            # Subsample the mask to match video_representations shape\n",
    "            video_mask = video_mask[:, ::stride]\n",
    "            \n",
    "            # Ensure mask length matches\n",
    "            frame_length = video_representations.shape[1]\n",
    "            if video_mask.shape[1] > frame_length:\n",
    "                video_mask = video_mask[:, :frame_length]\n",
    "            elif video_mask.shape[1] < frame_length:\n",
    "                pad_size = frame_length - video_mask.shape[1]\n",
    "                pad = torch.zeros((video_mask.shape[0], pad_size), dtype=torch.bool, device=video_mask.device)\n",
    "                video_mask = torch.cat([video_mask, pad], dim=1)\n",
    "            \n",
    "            # Convert to attention mask format expected by PyTorch\n",
    "            # True = don't attend, False = attend\n",
    "            attn_mask = ~video_mask\n",
    "            if attn_mask is not None:\n",
    "                completely_masked = attn_mask.all(dim=1, keepdim=True)\n",
    "                if completely_masked.any():\n",
    "                    # For completely masked rows, unmask at least one position\n",
    "                    attn_mask = attn_mask.clone()\n",
    "                    attn_mask[completely_masked.expand_as(attn_mask)] = False\n",
    "        else:\n",
    "            attn_mask = None\n",
    "        \n",
    "        # PyTorch's MultiheadAttention expects:\n",
    "        # - query: [batch_size, target_seq_length, embed_dim]\n",
    "        # - key: [batch_size, source_seq_length, embed_dim]\n",
    "        # - value: [batch_size, source_seq_length, embed_dim]\n",
    "        # - attn_mask: [batch_size, target_seq_length, source_seq_length] or [target_seq_length, source_seq_length]\n",
    "        \n",
    "        # Use PyTorch's optimized implementation\n",
    "        try:\n",
    "            cross_attention_output, _ = self.multihead_attn(\n",
    "                query=query,                  # From GPT tokens\n",
    "                key=video_representations,    # From video frames\n",
    "                value=video_representations,  # From video frames\n",
    "                key_padding_mask=attn_mask,   # Mask for padding frames\n",
    "                need_weights=False            # Don't return attention weights to save computation\n",
    "            )\n",
    "            if torch.isnan(cross_attention_output).any():\n",
    "                # Fall back to identity mapping if NaNs detected\n",
    "                print(\"WARNING: NaNs detected in cross-attention output, using identity mapping\")\n",
    "                cross_attention_output = query\n",
    "                \n",
    "            return cross_attention_output\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in cross-attention: {str(e)}\")\n",
    "            # Fall back to identity mapping if exception occurs\n",
    "            return query\n",
    "    \n",
    "class VideoGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Integrates pre-trained GPT-2 with cross-attention for video-to-text translation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"distilgpt2\", num_cross_heads=12, freeze_gpt=True, stride=2):\n",
    "        super(VideoGPT, self).__init__()\n",
    "        \n",
    "        # Load pre-trained model\n",
    "        self.gpt = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.gpt.to(\"cuda\")\n",
    "        self.config = self.gpt.config\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Dimensions\n",
    "        self.hidden_size = self.config.n_embd  # 768 for distilGPT-2\n",
    "        \n",
    "        # Create cross-attention layer\n",
    "        self.cross_attention = OptimizedCrossAttention(\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_heads=num_cross_heads\n",
    "        )\n",
    "        \n",
    "        # Freeze GPT-2 weights if specified\n",
    "        if freeze_gpt:\n",
    "            self._freeze_gpt_parameters()\n",
    "    \n",
    "    def _freeze_gpt_parameters(self):\n",
    "        \"\"\"Freeze all parameters of the GPT model.\"\"\"\n",
    "        for param in self.gpt.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, video_representations, video_mask=None, \n",
    "               L_index=None, L_values=None, label_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass with integrated cross-attention and semantic smoothing loss.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs for GPT [batch_size, n_tokens]\n",
    "            video_representations: Video frame features [batch_size, n_frames/stride, hidden_size]\n",
    "            video_mask: Mask tensor [batch_size, n_frames] with True for valid frames\n",
    "            attention_mask: Mask for input tokens [batch_size, n_tokens]\n",
    "            L_index: Token indices [batch_size, max_n_tokens, 6]\n",
    "            L_values: Token values [batch_size, max_n_tokens, 6]\n",
    "            label_mask: Boolean mask [batch_size, max_n_tokens]\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Model outputs including loss and logits\n",
    "        \"\"\"\n",
    "        batch_size, n_tokens = input_ids.shape\n",
    "        \n",
    "        # Get GPT embeddings (word + position)\n",
    "        position_ids = torch.arange(0, n_tokens, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        gpt_embeds = self.gpt.transformer.wte(input_ids) + self.gpt.transformer.wpe(position_ids)\n",
    "        \n",
    "        # Store states at each step\n",
    "        hidden_states = gpt_embeds\n",
    "\n",
    "        print(f\"Input IDs min/max: {input_ids.min().item()}, {input_ids.max().item()}\")\n",
    "        print(f\"Video representations has NaN: {torch.isnan(video_representations).any().item()}\")\n",
    "\n",
    "        if label_mask is not None:\n",
    "            # Create attention mask that combines padding and causal constraints\n",
    "            extended_attention_mask = label_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "\n",
    "            # Step 2: Create causal mask (lower triangular matrix)\n",
    "            seq_length = label_mask.size(1)\n",
    "            causal_mask = torch.tril(torch.ones((seq_length, seq_length), \n",
    "                                               device=label_mask.device))\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "            # Step 3: Combine padding mask with causal mask\n",
    "            combined_mask = causal_mask * extended_attention_mask.float()\n",
    "\n",
    "            # Step 4: Convert to additive mask where 0 means \"attend\" and \n",
    "            # a large negative number means \"don't attend\"\n",
    "            attention_mask = combined_mask.to(dtype=hidden_states.dtype)\n",
    "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "\n",
    "        print(f\"Attention mask min/max: {attention_mask.min().item()}, {attention_mask.max().item()}\")\n",
    "        print(f\"Attention mask has -inf: {torch.isinf(attention_mask).any().item()}\")\n",
    "        print(f\"Number of non-masked positions: {(attention_mask > -1000).sum().item()}\")\n",
    "        # Process through GPT layers with cross-attention\n",
    "        for i, block in enumerate(self.gpt.transformer.h):\n",
    "            print(f\"Block {i} hidden states has NaN: {torch.isnan(hidden_states).any().item()}\")\n",
    "            # 1. GPT self-attention\n",
    "            attn_outputs = block.attn(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask if label_mask is not None else None\n",
    "            )\n",
    "\n",
    "            gpt_attn_output = attn_outputs[0]\n",
    "            \n",
    "            # Add residual connection\n",
    "            hidden_states = gpt_attn_output + hidden_states\n",
    "            \n",
    "            # 2. Insert our cross-attention between self-attention and FFN\n",
    "            cross_attention_output = self.cross_attention(\n",
    "                hidden_states, \n",
    "                video_representations, \n",
    "                video_mask=video_mask,\n",
    "                stride=self.stride\n",
    "            )\n",
    "            \n",
    "            # Add residual connection to cross-attention\n",
    "            hidden_states = hidden_states + cross_attention_output\n",
    "            \n",
    "            # 3. Feed-forward network\n",
    "            feed_forward_output = block.mlp(hidden_states)\n",
    "            hidden_states = hidden_states + feed_forward_output\n",
    "        \n",
    "        # Final layer norm\n",
    "        hidden_states = self.gpt.transformer.ln_f(hidden_states)\n",
    "        \n",
    "        # Language modeling head\n",
    "        lm_logits = self.gpt.lm_head(hidden_states)\n",
    "        \n",
    "        # Calculate loss if labels are provided\n",
    "        loss = None\n",
    "\n",
    "        \n",
    "        if L_index is not None and L_values is not None:\n",
    "            # Use our custom semantic smoothing loss\n",
    "            loss = optimized_semantic_smoothing_loss(\n",
    "                logits=lm_logits,\n",
    "                L_index=L_index,\n",
    "                L_values=L_values,\n",
    "                label_mask=label_mask\n",
    "            )\n",
    "    \n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss, \n",
    "            \"logits\": lm_logits, \n",
    "            \"hidden_states\": hidden_states\n",
    "        }\n",
    "    \n",
    "\n",
    "def make_inputs_for_model(L_index, tokenizer):\n",
    "    primary_targets = L_index[:, :, 0].clone()\n",
    "    batch_size, seq_len = primary_targets.shape\n",
    "    input_ids = torch.zeros_like(primary_targets)\n",
    "    input_ids[:, 0] = tokenizer.bos_token_id  # Start with BOS token\n",
    "    input_ids[:, 1:] = primary_targets[:, :-1]\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f77c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_df = pd.read_csv(\"./low_df_only_path.csv\")\n",
    "mid_df = pd.read_csv(\"./mid_df_only_path.csv\")\n",
    "train_high_df = pd.read_csv(\"./high_train_only_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856640e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_df = pd.read_csv(\"./high_df_only_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54f73fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_high_df = pd.read_csv(\"./high_val_only_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f0c8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "find = './OpenASL-main/Open_asl_all_data/clips/OsDhpgMygiQ-00:01:25.311-00:01:30.311_fps15_landmarks/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40e0e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_high_new_df = valid_high_df[valid_high_df['landmarks_file_path'] != find]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acd3dca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landmarks_file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [landmarks_file_path]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_high_new_df[valid_high_new_df['landmarks_file_path'] == find]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfb363c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_high_new_df.to_csv(\"./high_val_only_path.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b727312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader, expected_batches = create_asl_dataloader(\n",
    "    low_df=low_df, \n",
    "    mid_df=mid_df, \n",
    "    high_df=high_df,\n",
    "    batch_size=30,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "primary_features = [\n",
    "    'dom_landmarks', 'non_dom_landmarks', 'blendshape_scores',\n",
    "    'nose_to_wrist_dist', 'dom_velocity_small', 'dom_velocity_large', \n",
    "    'non_dom_velocity_small', 'non_dom_velocity_large',\n",
    "    'nose_to_wrist_velocity_small', 'nose_to_wrist_velocity_large'\n",
    "]\n",
    "\n",
    "additional_features = [\n",
    "    'confidence_scores', 'face_detected', 'velocity_confidence', 'mask', 'label_mask', 'L_index', 'L_values'\n",
    "]\n",
    "batch = next(iter(loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56f6a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26172"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e291040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'directory_paths': ['./ASL_Citizen/videos/6720545064268519-TOP_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/49664574571720155-POINT INDEX_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/21287998395941665-DRY_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/3624721332797367-CALL_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/5860235392879398-OCEAN_fps15_R_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/p7qOW9H42Bs-00:03:51.098-00:03:52.169_fps15_landmarks/',\n",
       "  './ASL_Citizen/videos/5260713987206476-TAIL 2_fps15_R_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/BfUPRI3h4vo-00:15:52.158-00:15:53.994_fps15_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/M1UpvJATinc-00:04:10.917-00:04:13.453_fps15_landmarks/',\n",
       "  './ASL_Citizen/videos/8041759612730164-INCREDIBLE_fps15_R_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/DsgMcPXnruc-00:11:28.419-00:11:29.759_fps15_landmarks/',\n",
       "  './ASL_Citizen/videos/49212530571549884-MONTH_fps15_R_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/rqC0OOFHfgY-00:00:28.399-00:00:30.399_fps15_landmarks/',\n",
       "  './ASL_Citizen/videos/45973128321732126-GIVE UP_fps15_R_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/Glzf4l942RQ-00:06:59.000-00:07:00.420_fps15_landmarks/',\n",
       "  './ASL_Citizen/videos/21166861712980412-EITHER_fps15_R_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/vgV0IUhgcgc-00:00:09.279-00:00:10.960_fps15_landmarks/',\n",
       "  './ASL_Citizen/videos/11797732865735-GATE_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/9554537508077097-BITE_fps15_R_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/ORrMhgCO-HQ-00:14:30.220-00:14:32.039_fps15_landmarks/',\n",
       "  './ASL_Citizen/videos/3460314877226609-TAIL_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/7636779061217127-TURKEY 2_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/020030442376253177-HELLO_fps15_R_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/pkV9v250i4w-00:04:04.578-00:04:06.680_fps15_landmarks/',\n",
       "  './OpenASL-main/Open_asl_all_data/clips/0JCeMZ4Q3XQ-00:00:25.019-00:00:26.980_fps15_landmarks/',\n",
       "  './ASL_Citizen/videos/29040706600157895-MINE_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/4482292915189636-CONCEPT_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/7411488786994433-FIX_fps15_R_landmarks/',\n",
       "  './ASL_Citizen/videos/9590362944869453-LOOK BACK_fps15_L_landmarks/',\n",
       "  './ASL_Citizen/videos/7943488707305504-A-LINE BOB_fps15_R_landmarks/'],\n",
       " 'seq_lengths': tensor([24, 22, 37, 37, 35, 15, 38, 27, 37, 31, 24, 33, 36, 26, 26, 36, 31, 37,\n",
       "         31, 33, 37, 22, 24, 30, 35, 28, 37, 35, 23, 36], device='cuda:0'),\n",
       " 'L_index': tensor([[[ 4852,  9126, 35011, 22487, 35222, 46575],\n",
       "          [50256,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          ...,\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "         [[ 4122, 13033, 12727, 40710, 47809, 29536],\n",
       "          [ 9630, 15732, 17440, 42895, 47798, 28968],\n",
       "          [50256,     0,     0,     0,     0,     0],\n",
       "          ...,\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "         [[39140, 27773, 30905,   193,   219,   186],\n",
       "          [50256,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          ...,\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[13049, 42624, 22743, 34021, 47084, 13715],\n",
       "          [50256,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          ...,\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "         [[ 5460,  8567, 11534, 41102, 15784,  2958],\n",
       "          [ 1891, 10146,  7282, 31098, 17078, 49978],\n",
       "          [50256,     0,     0,     0,     0,     0],\n",
       "          ...,\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "         [[20663, 14414,   124, 30898,   195,   208],\n",
       "          [   65,    33,    66,    67,    69, 17457],\n",
       "          [  672,  8158,  9864,  5944,  5910, 45292],\n",
       "          ...,\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0,     0,     0]]], device='cuda:0'),\n",
       " 'L_values': tensor([[[0.7000, 0.0701, 0.0668, 0.0574, 0.0555, 0.0503],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.7000, 0.0774, 0.0674, 0.0560, 0.0538, 0.0454],\n",
       "          [0.7000, 0.0857, 0.0545, 0.0536, 0.0536, 0.0525],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.7000, 0.0608, 0.0598, 0.0598, 0.0598, 0.0598],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.7000, 0.0711, 0.0654, 0.0589, 0.0582, 0.0464],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.7000, 0.0740, 0.0618, 0.0560, 0.0559, 0.0523],\n",
       "          [0.7000, 0.0753, 0.0651, 0.0633, 0.0495, 0.0469],\n",
       "          [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.7000, 0.0709, 0.0574, 0.0573, 0.0573, 0.0572],\n",
       "          [0.7000, 0.0763, 0.0593, 0.0558, 0.0551, 0.0535],\n",
       "          [0.7000, 0.0651, 0.0648, 0.0624, 0.0540, 0.0537],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]], device='cuda:0'),\n",
       " 'label_mask': tensor([[ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False]],\n",
       "        device='cuda:0'),\n",
       " 'dom_landmarks': tensor([[[[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.7655,  1.1525,  0.5936],\n",
       "           [-0.9083,  1.1929,  0.8082],\n",
       "           [-0.9576,  1.1283,  0.9809],\n",
       "           ...,\n",
       "           [-0.7026,  0.5196,  1.3471],\n",
       "           [-0.7438,  0.4699,  1.2834],\n",
       "           [-0.7348,  0.4358,  1.1980]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0'),\n",
       " 'non_dom_landmarks': tensor([[[[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.3178, -0.3113,  0.5557],\n",
       "           [-0.2476, -0.2235,  0.7211],\n",
       "           [-0.1929, -0.1711,  0.8346],\n",
       "           ...,\n",
       "           [-0.2089, -0.3040,  1.1389],\n",
       "           [-0.1791, -0.3033,  1.0926],\n",
       "           [-0.1642, -0.3009,  1.0356]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0'),\n",
       " 'blendshape_scores': tensor([[[-3.4579e-03, -6.0333e-01, -6.4824e-01,  ..., -3.3766e-01,\n",
       "           -1.4308e-01, -1.7021e-01],\n",
       "          [-1.4455e-02, -6.2528e-01, -6.5206e-01,  ..., -3.3773e-01,\n",
       "           -1.2147e-01, -1.7619e-01],\n",
       "          [-7.4278e-04, -6.3040e-01, -6.7313e-01,  ..., -3.3626e-01,\n",
       "           -4.3127e-02, -1.4906e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 1.6553e-02, -8.1043e-02, -2.7947e-01,  ..., -2.7057e-01,\n",
       "           -1.6222e-01, -1.9318e-01],\n",
       "          [ 4.2060e-02, -1.1462e-01, -3.7147e-01,  ..., -2.8448e-01,\n",
       "           -1.3580e-01, -1.3977e-01],\n",
       "          [ 8.1553e-02, -2.3482e-02, -3.3219e-01,  ..., -2.6824e-01,\n",
       "           -1.5450e-01, -1.0752e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[-7.1936e-03, -5.1698e-01, -4.7089e-01,  ..., -3.3806e-01,\n",
       "           -1.8364e-01, -1.3727e-01],\n",
       "          [-1.7616e-02, -4.8473e-01, -4.8806e-01,  ..., -3.3796e-01,\n",
       "           -1.5873e-01, -1.6765e-01],\n",
       "          [-2.0010e-02, -5.4901e-01, -5.3076e-01,  ..., -3.3764e-01,\n",
       "           -1.4443e-01, -1.4769e-01],\n",
       "          ...,\n",
       "          [-1.3120e-03, -5.6427e-01, -5.8672e-01,  ..., -3.3738e-01,\n",
       "           -1.6428e-01,  3.0137e-02],\n",
       "          [-9.3779e-03, -5.5836e-01, -5.6436e-01,  ..., -3.3770e-01,\n",
       "           -1.7205e-01, -8.1898e-02],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-5.4699e-03, -6.3519e-01, -6.8426e-01,  ..., -3.3514e-01,\n",
       "           -2.7163e-02, -1.0400e-01],\n",
       "          [-1.0557e-02, -6.3506e-01, -6.8429e-01,  ..., -3.3678e-01,\n",
       "           -2.8933e-02, -1.1802e-01],\n",
       "          [-1.2188e-02, -6.3488e-01, -6.8417e-01,  ..., -3.3545e-01,\n",
       "           -6.9098e-03, -1.2071e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[-1.9937e-02, -6.0551e-01, -6.7384e-01,  ..., -3.3686e-01,\n",
       "           -8.9257e-02, -2.0987e-01],\n",
       "          [-1.9804e-02, -6.0991e-01, -6.7473e-01,  ..., -3.3723e-01,\n",
       "           -1.0922e-01, -2.1057e-01],\n",
       "          [-1.8606e-02, -6.1742e-01, -6.7920e-01,  ..., -3.3672e-01,\n",
       "           -9.6885e-02, -1.9038e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 1.0278e-02, -6.2957e-01, -6.8153e-01,  ..., -3.3640e-01,\n",
       "           -3.8046e-02,  9.8731e-02],\n",
       "          [-1.3733e-02, -6.3182e-01, -6.8163e-01,  ..., -3.3700e-01,\n",
       "           -1.5681e-01, -1.7848e-01],\n",
       "          [ 1.2169e-04, -6.3316e-01, -6.8260e-01,  ..., -3.0134e-01,\n",
       "           -6.7176e-03, -1.0605e-01],\n",
       "          ...,\n",
       "          [-2.4174e-03, -6.3223e-01, -6.8108e-01,  ..., -3.3557e-01,\n",
       "            7.5769e-02, -3.2365e-02],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]], device='cuda:0'),\n",
       " 'nose_to_wrist_dist': tensor([[[[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.6922, -1.4198],\n",
       "           [-0.5221, -1.2536]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000]]]], device='cuda:0'),\n",
       " 'dom_velocity_small': tensor([[[[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.3306e-01,  1.5163e-03, -8.3996e-03, -1.4158e+00, -3.4625e-04],\n",
       "           [-6.2030e-01, -4.5734e-03, -1.3236e-02, -1.4160e+00,  1.6485e-03],\n",
       "           [-6.2291e-01, -8.8747e-03, -1.5631e-02, -1.4160e+00,  2.4655e-03],\n",
       "           ...,\n",
       "           [-6.1970e-01, -2.8810e-02, -2.1644e-02, -1.4159e+00,  1.8211e-03],\n",
       "           [-6.2193e-01, -3.1881e-02, -2.1655e-02, -1.4160e+00, -8.9399e-04],\n",
       "           [-6.2235e-01, -3.2573e-02, -1.8268e-02, -1.4160e+00, -1.6418e-03]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]],\n",
       "        device='cuda:0'),\n",
       " 'dom_velocity_large': tensor([[[[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.5580e-01,  1.0133e-02, -6.7086e-03, -1.2902e+00, -5.7876e-03],\n",
       "           [-7.4075e-01, -2.6342e-03, -1.0554e-02, -1.2903e+00, -9.0373e-04],\n",
       "           [-7.4142e-01, -1.0879e-02, -1.2382e-02, -1.2903e+00, -9.7350e-06],\n",
       "           ...,\n",
       "           [-7.7042e-01, -3.8379e-02, -1.9291e-02, -1.2903e+00,  6.2506e-03],\n",
       "           [-7.7277e-01, -4.1206e-02, -1.9521e-02, -1.2903e+00,  2.7772e-03],\n",
       "           [-7.7231e-01, -4.0855e-02, -1.7056e-02, -1.2903e+00,  1.0569e-03]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]],\n",
       "        device='cuda:0'),\n",
       " 'non_dom_velocity_small': tensor([[[[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.0053e-01,  9.1131e-04,  7.9846e-03, -1.0977e+00, -3.9288e-04],\n",
       "           [-3.1527e-01, -4.6617e-03,  9.5220e-03, -1.0977e+00, -1.1080e-03],\n",
       "           [-3.2788e-01, -8.4755e-03,  9.5827e-03, -1.0977e+00, -2.0819e-03],\n",
       "           ...,\n",
       "           [-3.3318e-01, -1.8537e-02,  1.7004e-02, -1.0977e+00, -4.5696e-03],\n",
       "           [-3.3816e-01, -1.9848e-02,  1.6815e-02, -1.0977e+00, -5.4269e-03],\n",
       "           [-3.4369e-01, -2.0193e-02,  1.5074e-02, -1.0977e+00, -5.1476e-03]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "          [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "           [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]],\n",
       "        device='cuda:0'),\n",
       " 'non_dom_velocity_large': tensor([[[[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.3930,  0.0201,  0.0145, -1.0331,  0.0014],\n",
       "           [-0.4172,  0.0114,  0.0151, -1.0332,  0.0016],\n",
       "           [-0.4378,  0.0059,  0.0153, -1.0332,  0.0018],\n",
       "           ...,\n",
       "           [-0.4508, -0.0071,  0.0216, -1.0332, -0.0052],\n",
       "           [-0.4580, -0.0088,  0.0210, -1.0332, -0.0072],\n",
       "           [-0.4655, -0.0088,  0.0192, -1.0332, -0.0069]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           ...,\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0'),\n",
       " 'nose_to_wrist_velocity_small': tensor([[[[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.3644, -0.0383, -0.0022],\n",
       "           [-0.3113, -0.0307,  0.0108]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0'),\n",
       " 'nose_to_wrist_velocity_large': tensor([[[[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]],\n",
       " \n",
       " \n",
       "         [[[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.5147, -0.0215, -0.0032],\n",
       "           [-0.4816, -0.0126,  0.0123]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "          [[ 0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000]]]], device='cuda:0'),\n",
       " 'confidence_scores': tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]], device='cuda:0'),\n",
       " 'interpolation_scores': tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]], device='cuda:0'),\n",
       " 'detection_status': tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]], device='cuda:0'),\n",
       " 'frame_idx': tensor([[ 0.,  1.,  2.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  2.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  2.,  ..., 35., 36.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  1.,  2.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  2.,  ...,  0.,  0.,  0.],\n",
       "         [ 0.,  1.,  2.,  ..., 35.,  0.,  0.]], device='cuda:0'),\n",
       " 'velocity_confidence': tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]], device='cuda:0'),\n",
       " 'velocity_calculation_confidence': tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          ...,\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]], device='cuda:0'),\n",
       " 'face_detected': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 1., 0., 0.]], device='cuda:0'),\n",
       " 'mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ...,  True,  True, False],\n",
       "         ...,\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ...,  True, False, False]], device='cuda:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24fd879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomasdev/anaconda3/envs/ultralytics-env/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/thomasdev/anaconda3/envs/ultralytics-env/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "embedding_dim = 30\n",
    "embedding_table = LandmarkEmbedding(embedding_dim=embedding_dim, num_landmarks_per_hand=21)\n",
    "embedding_table.to(device)\n",
    "\n",
    "\n",
    "landmark_encoder_hidden_dims = [30, 30, 30] \n",
    "landmark_encoder_activation = 'gelu'\n",
    "landmark_encoder_init_method = 'kaiming_normal'\n",
    "landmark_encoder_init_gain=1.0\n",
    "landmark_encoder_init_nonlinearity = 'relu'\n",
    "landmark_encoder = LandmarkSpatialEncoder(embedding_dim, hidden_dims=landmark_encoder_hidden_dims, activation=landmark_encoder_activation, init_method=landmark_encoder_init_method, init_gain=landmark_encoder_init_gain,init_nonlinearity=landmark_encoder_init_nonlinearity)\n",
    "landmark_encoder.to(device)\n",
    "\n",
    "\n",
    "wrist_encoder_hidden_dims = [30, 60, 30] \n",
    "wrist_encoder_activation = 'gelu'\n",
    "wrist_encoder_init_method = 'kaiming_normal'\n",
    "wrist_encoder_init_gain=1.0\n",
    "wrist_encoder_init_nonlinearity = 'relu'\n",
    "wrist_encoder = WristSpatialEncoder(embedding_dim, hidden_dims=wrist_encoder_hidden_dims,activation=wrist_encoder_activation,init_method=wrist_encoder_init_method, init_gain=wrist_encoder_init_gain, init_nonlinearity=wrist_encoder_init_nonlinearity)\n",
    "wrist_encoder.to(device)\n",
    "\n",
    "blendshapes_encoder_hidden_dims = [60, 60, 60, 60] \n",
    "blendshapes_encoder_activation = 'gelu'\n",
    "blendshapes_encoder_init_method = 'kaiming_normal'\n",
    "blendshapes_encoder_init_gain=1.0\n",
    "blendshapes_encoder_init_nonlinearity = 'relu'\n",
    "blendshapes_feedforward = BlendshapeEncoder(embedding_dim, hidden_dims=blendshapes_encoder_hidden_dims, activation=blendshapes_encoder_activation,init_method=blendshapes_encoder_init_method, init_gain=blendshapes_encoder_init_gain, init_nonlinearity=blendshapes_encoder_init_nonlinearity)\n",
    "blendshapes_feedforward.to(device)\n",
    "\n",
    "\n",
    "velocity_encoder_hidden_dims = [30, 30, 30, 30] \n",
    "velocity_encoder_activation = 'gelu'\n",
    "velocity_encoder_init_method = 'kaiming_normal'\n",
    "velocity_encoder_init_gain=1.0\n",
    "velocity_encoder_init_nonlinearity = 'relu'\n",
    "velocity_feedforward = VelocityEncoder(n_velocity_encoding=2*embedding_dim, hidden_dims=velocity_encoder_hidden_dims, activation=velocity_encoder_activation, init_method=velocity_encoder_init_method, init_gain=velocity_encoder_init_gain, init_nonlinearity=velocity_encoder_init_nonlinearity)\n",
    "velocity_feedforward.to(device)\n",
    "\n",
    "\n",
    "wrist_vel_encoder_hidden_dims = [30, 30, 30, 30] \n",
    "wrist_vel_encoder_activation = 'gelu'\n",
    "wrist_vel_encoder_init_method = 'kaiming_normal'\n",
    "wrist_vel_encoder_init_gain=1.0\n",
    "wrist_vel_encoder_init_nonlinearity = 'relu'\n",
    "wrist_vel_feedforward = WristVelocityEncoder(n_velocity_encoding=2*embedding_dim, hidden_dims=wrist_vel_encoder_hidden_dims, activation=wrist_vel_encoder_activation, init_method=wrist_vel_encoder_init_method, init_gain=wrist_vel_encoder_init_gain, init_nonlinearity=wrist_vel_encoder_init_nonlinearity)\n",
    "wrist_vel_feedforward.to(device)\n",
    "\n",
    "\n",
    "first_stage_transformer_num_layers = 4\n",
    "first_stage_transformer_num_heads = 8\n",
    "first_stage_transformer_hidden_dim = 256 #Output dimensionality\n",
    "first_stage_transformer_ff_dim=4*first_stage_transformer_hidden_dim #Feedforward networks hidden dimension\n",
    "first_stage_transformer_activation = 'gelu'\n",
    "first_stage_transformer_init_method = 'kaiming_normal'\n",
    "first_stage_transformer_prenorm = True\n",
    "first_stage_transformer_init_gain=1.0\n",
    "dom_transformer = LandmarkTransformerEncoder(input_dim=3 * embedding_dim, num_layers=first_stage_transformer_num_layers, num_heads=first_stage_transformer_num_heads, hidden_dim=first_stage_transformer_hidden_dim, ff_dim=first_stage_transformer_ff_dim, activation=first_stage_transformer_activation, prenorm=first_stage_transformer_prenorm, init_method=first_stage_transformer_init_method, init_gain=first_stage_transformer_init_gain)\n",
    "non_dom_transformer = LandmarkTransformerEncoder(input_dim=3 * embedding_dim, num_layers=first_stage_transformer_num_layers, num_heads=first_stage_transformer_num_heads, hidden_dim=first_stage_transformer_hidden_dim, ff_dim=first_stage_transformer_ff_dim, activation=first_stage_transformer_activation, prenorm=first_stage_transformer_prenorm, init_method=first_stage_transformer_init_method, init_gain=first_stage_transformer_init_gain)\n",
    "dom_transformer.to(device)\n",
    "non_dom_transformer.to(device)\n",
    "\n",
    "\n",
    "first_stage_pooling_output_dim=256\n",
    "dom_pooling = LandmarkAttentionPooling(input_dim=dom_transformer.hidden_dim,output_dim=first_stage_pooling_output_dim)\n",
    "non_dom_pooling = LandmarkAttentionPooling(input_dim=non_dom_transformer.hidden_dim,output_dim=first_stage_pooling_output_dim)\n",
    "dom_pooling.to(device)\n",
    "non_dom_pooling.to(device)\n",
    "\n",
    "\n",
    "first_stage_velocity_transformer_num_layers = 4\n",
    "first_stage_velocity_transformer_num_heads = 8\n",
    "first_stage_velocity_transformer_hidden_dim = 256 #Output dimensionality\n",
    "first_stage_velocity_transformer_ff_dim=4*first_stage_velocity_transformer_hidden_dim #Feedforward networks hidden dimension\n",
    "first_stage_velocity_transformer_activation = 'gelu'\n",
    "first_stage_velocity_transformer_init_method = 'kaiming_normal'\n",
    "first_stage_velocity_transformer_prenorm = True\n",
    "first_stage_velocity_transformer_init_gain=1.0\n",
    "dom_vel_transformer = LandmarkTransformerEncoder(input_dim=velocity_feedforward.output_dim*2+embedding_dim, num_layers=first_stage_velocity_transformer_num_layers, num_heads=first_stage_velocity_transformer_num_heads, hidden_dim=first_stage_velocity_transformer_hidden_dim, ff_dim=first_stage_velocity_transformer_ff_dim, activation=first_stage_velocity_transformer_activation, prenorm=first_stage_velocity_transformer_prenorm, init_method=first_stage_velocity_transformer_init_method, init_gain=first_stage_velocity_transformer_init_gain)\n",
    "non_dom_vel_transformer = LandmarkTransformerEncoder(input_dim=velocity_feedforward.output_dim*2+embedding_dim, num_layers=first_stage_velocity_transformer_num_layers, num_heads=first_stage_velocity_transformer_num_heads, hidden_dim=first_stage_velocity_transformer_hidden_dim, ff_dim=first_stage_velocity_transformer_ff_dim, activation=first_stage_velocity_transformer_activation, prenorm=first_stage_velocity_transformer_prenorm, init_method=first_stage_velocity_transformer_init_method, init_gain=first_stage_velocity_transformer_init_gain)\n",
    "dom_vel_transformer.to(device)\n",
    "non_dom_vel_transformer.to(device)\n",
    "\n",
    "\n",
    "first_stage_velocity_pooling_output_dim=256\n",
    "dom_vel_pooling = LandmarkAttentionPooling(input_dim=dom_vel_transformer.hidden_dim,output_dim=first_stage_velocity_pooling_output_dim)\n",
    "non_dom_vel_pooling = LandmarkAttentionPooling(input_dim=non_dom_vel_transformer.hidden_dim,output_dim=first_stage_velocity_pooling_output_dim)\n",
    "dom_vel_pooling.to(device)\n",
    "non_dom_vel_pooling.to(device)\n",
    "\n",
    "input_dim_for_cross_hand_transformer = dom_pooling.output_dim + 4*embedding_dim+dom_vel_pooling.output_dim+2*wrist_vel_feedforward.output_dim\n",
    "\n",
    "second_stage_transformer_num_layers = 4\n",
    "second_stage_transformer_num_heads = 8\n",
    "second_stage_transformer_hidden_dim = input_dim_for_cross_hand_transformer #Output dimensionality\n",
    "second_stage_transformer_ff_dim=4*second_stage_transformer_hidden_dim #Feedforward networks hidden dimension\n",
    "second_stage_transformer_activation = 'gelu'\n",
    "second_stage_transformer_init_method = 'kaiming_normal'\n",
    "second_stage_transformer_prenorm = True\n",
    "second_stage_transformer_init_gain=1.0\n",
    "cross_hand_transformer = ConfidenceWeightedTransformerEncoder(\n",
    "    input_dim=input_dim_for_cross_hand_transformer,\n",
    "    num_layers=second_stage_transformer_num_layers,\n",
    "    num_heads=second_stage_transformer_num_heads,\n",
    "    hidden_dim=second_stage_transformer_hidden_dim,\n",
    "    ff_dim=second_stage_transformer_ff_dim,\n",
    "    prenorm=second_stage_transformer_prenorm,\n",
    "    activation=second_stage_transformer_activation,\n",
    "    init_method=second_stage_transformer_init_method,\n",
    "    init_gain=second_stage_transformer_init_gain\n",
    ")\n",
    "cross_hand_transformer.to(device)\n",
    "\n",
    "\n",
    "final_pooling_output_dim=second_stage_transformer_hidden_dim\n",
    "final_pooling = LandmarkAttentionPooling(\n",
    "    input_dim=second_stage_transformer_hidden_dim,\n",
    "    output_dim=final_pooling_output_dim)\n",
    "final_pooling.to(device)\n",
    "\n",
    "\n",
    "stride = 2\n",
    "number_of_filters = 768 #Output dim\n",
    "kernel_size = 5\n",
    "convolution_activation = 'gelu'\n",
    "convolution_norm_layer = True\n",
    "conv1d = TemporalDownsampler(\n",
    "    input_dim=input_dim_for_cross_hand_transformer + 2*embedding_dim,          # Feature dimension (d)\n",
    "    output_channels=number_of_filters,    \n",
    "    kernel_size=kernel_size,          \n",
    "    stride=stride,               \n",
    "    activation=convolution_activation,      \n",
    "    norm_layer=convolution_norm_layer         \n",
    ")\n",
    "conv1d.to(device)\n",
    "\n",
    "\n",
    "max_possible_number_of_frames = 120\n",
    "positional_encoder = PositionalEncoding(\n",
    "    d_model=number_of_filters,  # Feature dimension\n",
    "    max_len=int(max_possible_number_of_frames/stride)  \n",
    ")\n",
    "positional_encoder.to(device)\n",
    "\n",
    "\n",
    "temporal_transformer_num_layers = 4\n",
    "temporal_transformer_hidden_dim = input_dim_for_cross_hand_transformer #Output dimensionality\n",
    "temporal_transformer_ff_dim=4*second_stage_transformer_hidden_dim #Feedforward networks hidden dimension\n",
    "temporal_transformer_activation ='gelu'\n",
    "temporal_transformer = MultiScaleTemporalTransformer(\n",
    "    d_model=number_of_filters,\n",
    "    num_layers=temporal_transformer_num_layers,\n",
    "    short_range=5,\n",
    "    medium_range=15,\n",
    "    long_range=45,\n",
    "    dim_feedforward=temporal_transformer_ff_dim,\n",
    "    activation=temporal_transformer_activation,\n",
    "    stride=stride \n",
    ")\n",
    "temporal_transformer.to(device)\n",
    "\n",
    "\n",
    "GPT_model_name = \"distilgpt2\" \n",
    "model_cross_attention_heads = 12\n",
    "freeze_GPT_model_weights=True\n",
    "model = VideoGPT(\n",
    "    model_name=GPT_model_name,\n",
    "    num_cross_heads=model_cross_attention_heads,\n",
    "    freeze_gpt=freeze_GPT_model_weights,\n",
    "    stride=stride\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(GPT_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f4fcb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs min/max: 0, 50256\n",
      "Video representations has NaN: False\n",
      "Attention mask min/max: -10000.0, -0.0\n",
      "Attention mask has -inf: False\n",
      "Number of non-masked positions: 87\n",
      "Block 0 hidden states has NaN: False\n",
      "Block 1 hidden states has NaN: False\n",
      "Block 2 hidden states has NaN: False\n",
      "Block 3 hidden states has NaN: False\n",
      "Block 4 hidden states has NaN: False\n",
      "Block 5 hidden states has NaN: False\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_table.forward()\n",
    "\n",
    "dom_landmark_embeddings = embeddings[:20]\n",
    "dom_wrist_embedding = embeddings[20]\n",
    "\n",
    "non_dom_landmark_embeddings = embeddings[21:41]\n",
    "non_dom_wrist_embedding = embeddings[41]\n",
    "\n",
    "#Spatial encoding\n",
    "dom_landmarks_where = landmark_encoder.forward(batch['dom_landmarks'])\n",
    "non_dom_landmarks_where = landmark_encoder.forward(batch['non_dom_landmarks'])\n",
    "\n",
    "dom_landmarks_conc = combine_spatial_and_semantic_features(spatial_features=dom_landmarks_where, semantic_features=dom_landmark_embeddings)\n",
    "non_dom_landmarks_conc = combine_spatial_and_semantic_features(spatial_features=non_dom_landmarks_where, semantic_features=non_dom_landmark_embeddings)\n",
    "\n",
    "wrists_where = wrist_encoder.forward(wrist_coordinates=batch['nose_to_wrist_dist'])\n",
    "\n",
    "wrists_conc = combine_wrist_embedding_and_spatial(wrist_embeddings=torch.cat([dom_wrist_embedding, non_dom_wrist_embedding], dim=-1).reshape((2,-1)), wrist_spatial_features=wrists_where)\n",
    "\n",
    "#Velocity encoding (both windows)\n",
    "dom_small_vel_encoded = velocity_feedforward.forward(batch['dom_velocity_small']) \n",
    "dom_large_vel_encoded = velocity_feedforward.forward(batch['dom_velocity_large']) \n",
    "non_dom_small_vel_encoded = velocity_feedforward.forward(batch['non_dom_velocity_small']) \n",
    "non_dom_large_vel_encoded = velocity_feedforward.forward(batch['non_dom_velocity_large']) \n",
    "\n",
    "dom_landmarks_velocity_conc = combine_semantic_and_velocity_features(semantic_features=dom_landmark_embeddings, velocity_small_features=dom_small_vel_encoded, velocity_large_features=dom_large_vel_encoded)\n",
    "non_dom_landmarks_velocity_conc = combine_semantic_and_velocity_features(semantic_features=non_dom_landmark_embeddings, velocity_small_features=non_dom_small_vel_encoded, velocity_large_features=non_dom_large_vel_encoded)\n",
    "\n",
    "wrist_vel_small_encoded = wrist_vel_feedforward.forward(batch['nose_to_wrist_velocity_small'])\n",
    "wrist_vel_large_encoded = wrist_vel_feedforward.forward(batch['nose_to_wrist_velocity_large'])\n",
    "\n",
    "wrists_vel_conc = combine_wrist_embedding_and_velocity(wrist_embeddings=torch.cat([dom_wrist_embedding, non_dom_wrist_embedding], dim=-1).reshape((2,-1)), wrist_velocity_small=wrist_vel_small_encoded, wrist_velocity_large=wrist_vel_large_encoded)\n",
    "\n",
    "#Blendshapes encoding\n",
    "blendshapes_encoded = blendshapes_feedforward(batch['blendshape_scores'])\n",
    "\n",
    "\n",
    "#Spatial transformer\n",
    "dom_contextualized = dom_transformer(dom_landmarks_conc)\n",
    "non_dom_contextualized=non_dom_transformer(non_dom_landmarks_conc)\n",
    "\n",
    "dom_pooled = dom_pooling(dom_contextualized)\n",
    "non_dom_pooled = non_dom_pooling(non_dom_contextualized)\n",
    "\n",
    "dom_wrist_conc = wrists_conc[:,:,0]\n",
    "non_dom_wrist_conc = wrists_conc[:,:,1]\n",
    "\n",
    "dom_spatial_combined = concat_pooled_wrists(pooled=dom_pooled, wrist=dom_wrist_conc)\n",
    "non_dom_spatial_combined = concat_pooled_wrists(pooled=non_dom_pooled, wrist=non_dom_wrist_conc)\n",
    "\n",
    "#Velocity transformer\n",
    "dom_vel_contextualized = dom_vel_transformer(dom_landmarks_velocity_conc)\n",
    "non_dom_vel_contextualized=non_dom_vel_transformer(non_dom_landmarks_velocity_conc)\n",
    "\n",
    "dom_vel_pooled = dom_vel_pooling(dom_vel_contextualized)\n",
    "non_dom_vel_pooled = non_dom_vel_pooling(non_dom_vel_contextualized)\n",
    "\n",
    "dom_wrist_vel_conc = wrists_vel_conc[:,:,0]\n",
    "non_dom_wrist_vel_conc = wrists_vel_conc[:,:,1]\n",
    "\n",
    "dom_velocity_combined = concat_pooled_wrists(pooled=dom_vel_pooled, wrist=dom_wrist_vel_conc)\n",
    "non_dom_velocity_combined = concat_pooled_wrists(pooled=non_dom_vel_pooled, wrist=non_dom_wrist_vel_conc)\n",
    "\n",
    "#Combining spatial with velocity features\n",
    "dom_combined = concat_pooled_wrists(dom_spatial_combined, dom_velocity_combined)\n",
    "non_dom_combined = concat_pooled_wrists(non_dom_spatial_combined, non_dom_velocity_combined)\n",
    "\n",
    "hands_combined = torch.stack([dom_combined, non_dom_combined], dim=2)\n",
    "\n",
    "#Second stage transformers between the two hands\n",
    "confidence_scores = {\n",
    "    'Cd_spatial': batch['confidence_scores'],\n",
    "    'Ci_spatial': batch['interpolation_scores'],\n",
    "    'Cd_velocity': batch['velocity_calculation_confidence'],\n",
    "    'Ci_velocity': batch['velocity_confidence']\n",
    "}\n",
    "\n",
    "enhanced_hands = cross_hand_transformer(hands_combined, confidence_scores)\n",
    "\n",
    "#Attention pooling to keep a weighted avg of the two\n",
    "final_hands_representation = final_pooling(enhanced_hands)\n",
    "\n",
    "#Combine with blendshapes\n",
    "frame_representation = concat_pooled_wrists(final_hands_representation, blendshapes_encoded)\n",
    "\n",
    "#Downsample with 1d convolution\n",
    "downsampled_representation = conv1d(frame_representation)\n",
    "\n",
    "#Positional encodings + temporal transformer\n",
    "downsampled_with_positional_encoding = positional_encoder(downsampled_representation, scale=1.0)\n",
    "multi_scale_representation = temporal_transformer(downsampled_with_positional_encoding, mask=batch['mask'])\n",
    "\n",
    "#Re-enforce positional encodings with smaller scale\n",
    "video_representation = positional_encoder(multi_scale_representation, scale=0.25)\n",
    "\n",
    "\n",
    "\n",
    "input_ids = make_inputs_for_model(L_index=batch['L_index'], tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids.to(\"cuda\"),\n",
    "    video_representations=video_representation.to(\"cuda\"),\n",
    "    video_mask=batch[\"mask\"].to(\"cuda\"),\n",
    "    L_index=batch[\"L_index\"].to(\"cuda\"),\n",
    "    L_values=batch[\"L_values\"].to(\"cuda\"),\n",
    "    label_mask=batch[\"label_mask\"].to(\"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4170803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[464, 464, 464, 464, 464, 464, 464, 464, 464, 464, 464],\n",
       "       [464, 464, 464, 464, 464, 464, 464, 464, 464, 464, 464]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs['logits'], dim=-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43f3d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_from_logits(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert model logits to human-readable text predictions.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of shape [batch_size, sequence_length, vocab_size]\n",
    "        tokenizer: The GPT tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing token IDs and decoded text for each batch item\n",
    "    \"\"\"\n",
    "    # Get the most likely token at each position (argmax along vocab dimension)\n",
    "    predicted_token_ids = torch.argmax(logits, dim=-1)  # [batch_size, sequence_length]\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    token_ids_np = predicted_token_ids.cpu().numpy()\n",
    "    \n",
    "    # Container for results\n",
    "    results = []\n",
    "    \n",
    "    # Process each sequence in the batch\n",
    "    for i, ids in enumerate(token_ids_np):\n",
    "        # Decode the token IDs to text\n",
    "        text = tokenizer.decode(ids)\n",
    "        \n",
    "        # For more detailed analysis, get individual tokens\n",
    "        tokens = []\n",
    "        for token_id in ids:\n",
    "            token_str = tokenizer.decode([token_id])\n",
    "            tokens.append((token_id, token_str))\n",
    "            \n",
    "        results.append({\n",
    "            \"label\": i,\n",
    "            \"text\": text,\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c8145fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_predictions_from_logits(outputs[\"logits\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7273c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 0, 'text': 'TheTheTheTheTheTheTheTheTheTheThe'},\n",
       " {'label': 1, 'text': 'TheTheTheTheTheTheTheTheTheTheThe'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "636499e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50256,   404, 23971, 50256,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39436e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label 0:\n",
      "Input:      <|endoftext|>captioned by aslcaptions.com\n",
      "Generated:  TheTheTheTheTheTheTheTheTheTheThe\n",
      "\n",
      "Label 1:\n",
      "Input:      <|endoftext|>movie<|endoftext|>!!!!!!!!\n",
      "Generated:  TheTheTheTheTheTheTheTheTheTheThe\n"
     ]
    }
   ],
   "source": [
    "def compare_predictions(input_ids, tokenizer):\n",
    "    \"\"\"Show comparison between inputs, predictions, and expected labels\"\"\"\n",
    "    predictions = get_predictions_from_logits(logits=outputs['logits'], tokenizer=tokenizer)\n",
    "    for i, pred in enumerate(predictions):\n",
    "        print(f\"\\nLabel {i}:\")\n",
    "        \n",
    "        # Input sequence\n",
    "        input_sequence = tokenizer.decode(input_ids[i])\n",
    "        print(f\"Input:      {input_sequence}\")\n",
    "        \n",
    "        # Generated sequence\n",
    "        print(f\"Generated:  {pred['text']}\")\n",
    "\n",
    "\n",
    "# Use in your test\n",
    "compare_predictions(\n",
    "    input_ids,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed726c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 predictions for batch 0, position 10:\n",
      "  1. Token: 'The', ID: 464, Probability: 0.3320\n",
      "  2. Token: '\"', ID: 1, Probability: 0.2005\n",
      "  3. Token: 'A', ID: 32, Probability: 0.1968\n",
      "  4. Token: 'I', ID: 40, Probability: 0.1718\n",
      "  5. Token: 'In', ID: 818, Probability: 0.0989\n",
      "\n",
      "Top 5 predictions for batch 1, position 10:\n",
      "  1. Token: 'The', ID: 464, Probability: 0.3301\n",
      "  2. Token: 'A', ID: 32, Probability: 0.2000\n",
      "  3. Token: '\"', ID: 1, Probability: 0.1914\n",
      "  4. Token: 'I', ID: 40, Probability: 0.1785\n",
      "  5. Token: 'In', ID: 818, Probability: 0.1000\n"
     ]
    }
   ],
   "source": [
    "def analyze_next_token_predictions(logits, current_position, tokenizer, top_k=5):\n",
    "    \"\"\"Analyze top-k predictions for the next token at a specified position\"\"\"\n",
    "    batch_size = logits.shape[0]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        next_token_logits = logits[b, current_position, :]\n",
    "        \n",
    "        # Get top-k predictions\n",
    "        values, indices = torch.topk(next_token_logits, top_k)\n",
    "        probs = torch.softmax(values, dim=0)\n",
    "        \n",
    "        print(f\"\\nTop {top_k} predictions for batch {b}, position {current_position}:\")\n",
    "        for i, (idx, prob) in enumerate(zip(indices.tolist(), probs.tolist())):\n",
    "            token = tokenizer.decode([idx])\n",
    "            print(f\"  {i+1}. Token: '{token}', ID: {idx}, Probability: {prob:.4f}\")\n",
    "\n",
    "\n",
    "# Use in your test to analyze what comes after the last input token\n",
    "analyze_next_token_predictions(\n",
    "    outputs[\"logits\"], \n",
    "    current_position=input_ids.shape[1]-1,  # Last position\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74433ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_minimum_loss(target_labels, label_mask=None):\n",
    "    \"\"\"\n",
    "    Compute the theoretical minimum loss (entropy of target distribution)\n",
    "    \"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    # Calculate entropy for each position: -(p_i * log(p_i))\n",
    "    position_entropy = -(target_labels * torch.log(target_labels + epsilon)).sum(dim=2)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if label_mask is not None:\n",
    "        position_entropy = position_entropy * label_mask.float()\n",
    "        # Average entropy over valid tokens\n",
    "        min_loss = position_entropy.sum() / label_mask.sum().clamp(min=1)\n",
    "    else:\n",
    "        # If no mask, use all tokens\n",
    "        min_loss = position_entropy.mean()\n",
    "    \n",
    "    return min_loss\n",
    "\n",
    "\n",
    "def get_predictions_from_logits(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert model logits to human-readable text predictions.\n",
    "    \"\"\"\n",
    "    # Get the most likely token at each position\n",
    "    predicted_token_ids = torch.argmax(logits, dim=-1)  # [batch_size, sequence_length]\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    token_ids_np = predicted_token_ids.cpu().numpy()\n",
    "    \n",
    "    # Container for results\n",
    "    results = []\n",
    "    \n",
    "    # Process each sequence in the batch\n",
    "    for i, ids in enumerate(token_ids_np):\n",
    "        # Decode the token IDs to text\n",
    "        text = tokenizer.decode(ids)\n",
    "        results.append({\n",
    "            \"label\": i,\n",
    "            \"text\": text,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_predictions(input_ids, logits, tokenizer):\n",
    "    \"\"\"Show comparison between inputs, predictions\"\"\"\n",
    "    predictions = get_predictions_from_logits(logits=logits, tokenizer=tokenizer)\n",
    "    results = []\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        # Input sequence\n",
    "        input_sequence = tokenizer.decode(input_ids[i])\n",
    "        results.append({\n",
    "            \"index\": i,\n",
    "            \"input\": input_sequence,\n",
    "            \"generated\": pred['text']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def train_asl_model(\n",
    "    # Data parameters\n",
    "    train_loader,\n",
    "    val_loader, \n",
    "    train_batches,\n",
    "    val_batches,\n",
    "    \n",
    "    # Model components (already initialized and on correct device)\n",
    "    models_dict,  # Dictionary containing all model components\n",
    "    tokenizer,    # Tokenizer for GPT model\n",
    "    \n",
    "    # Optimizer parameters\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-6,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs=30,\n",
    "    grad_clip_value=1.0,\n",
    "    scheduler_type='cosine',  # 'cosine', 'linear', 'step', None\n",
    "    scheduler_params=None,    # Dict of params specific to the scheduler\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    early_stopping=True,\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_dir='./checkpoints',\n",
    "    save_best_only=True,\n",
    "    save_freq_epochs=1,\n",
    "    auto_rescue=True,\n",
    "    # Validation and visualization\n",
    "    num_examples_to_display=5,  # Number of examples to display during validation\n",
    "    \n",
    "    # Logging\n",
    "    log_freq_batches=10,\n",
    "    verbose=1,\n",
    "    \n",
    "    # Device\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive training loop for the ASL Translation model.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create directory for checkpoints\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Timestamp for this training run\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(save_dir, f\"run_{timestamp}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    def save_rescue_checkpoint(signal_received=None, frame=None):\n",
    "            \"\"\"Save a rescue checkpoint and exit gracefully if needed\"\"\"\n",
    "            try:\n",
    "                rescue_path = os.path.join(run_dir, f\"rescue_checkpoint_epoch_{epoch+1}_batch_{batch_idx+1}.pt\")\n",
    "                print(f\"\\n\\n{'='*50}\")\n",
    "                if signal_received:\n",
    "                    print(f\"Signal {signal_received} received. Saving rescue checkpoint...\")\n",
    "                else:\n",
    "                    print(f\"Exception detected. Saving rescue checkpoint...\")\n",
    "\n",
    "                # Save current state\n",
    "                save_checkpoint(rescue_path, models_dict, optimizer, scheduler, epoch, history, \n",
    "                               extra_info={'interrupted_at_batch': batch_idx, 'exception': traceback.format_exc()})\n",
    "\n",
    "                print(f\"Rescue checkpoint saved to: {rescue_path}\")\n",
    "                print(f\"You can resume training from this checkpoint later.\")\n",
    "                print(f\"{'='*50}\\n\")\n",
    "\n",
    "                if signal_received:  # If this was triggered by a signal, exit\n",
    "                    sys.exit(0)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save rescue checkpoint: {e}\")\n",
    "\n",
    "    if auto_rescue:\n",
    "        signal.signal(signal.SIGINT, save_rescue_checkpoint)  # Ctrl+C\n",
    "        signal.signal(signal.SIGTERM, save_rescue_checkpoint)  # Termination request\n",
    "    \n",
    "    # Extract all models from dictionary\n",
    "    model_components = list(models_dict.values())\n",
    "    \n",
    "    # Create optimizer with all parameters from all models\n",
    "    all_params = []\n",
    "    for model in model_components:\n",
    "        all_params.extend(model.parameters())\n",
    "    \n",
    "    optimizer = optim.Adam(all_params, lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Create scheduler if requested\n",
    "    scheduler = None\n",
    "    if scheduler_type == 'cosine':\n",
    "        scheduler_params = scheduler_params or {'T_max': num_epochs}\n",
    "        scheduler = CosineAnnealingLR(optimizer, **scheduler_params)\n",
    "    elif scheduler_type == 'step':\n",
    "        scheduler_params = scheduler_params or {'step_size': 10, 'gamma': 0.1}\n",
    "        scheduler = StepLR(optimizer, **scheduler_params)\n",
    "    elif scheduler_type == 'linear':\n",
    "        scheduler_params = scheduler_params or {'start_factor': 1.0, 'end_factor': 0.1, 'total_iters': num_epochs}\n",
    "        scheduler = LinearLR(optimizer, **scheduler_params)\n",
    "    elif scheduler_type == 'plateau':\n",
    "        scheduler_params = scheduler_params or {'mode': 'min', 'factor': 0.1, 'patience': 5, 'verbose': verbose > 0}\n",
    "        scheduler = ReduceLROnPlateau(optimizer, **scheduler_params)\n",
    "    \n",
    "    # Initialize early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Initialize history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_min_loss': [],  # Theoretical minimum loss\n",
    "        'val_normalized_loss': [],  # Loss normalized by theoretical minimum\n",
    "        'learning_rates': [],\n",
    "        'examples': []  # Store validation examples\n",
    "    }\n",
    "    \n",
    "    # Define a function to run the forward pass\n",
    "    def forward_pass(batch):\n",
    "        # Get embeddings\n",
    "        embeddings = models_dict['embedding_table'].forward()\n",
    "        dom_landmark_embeddings = embeddings[:20]\n",
    "        dom_wrist_embedding = embeddings[20]\n",
    "        non_dom_landmark_embeddings = embeddings[21:41]\n",
    "        non_dom_wrist_embedding = embeddings[41]\n",
    "        \n",
    "        # Spatial encoding\n",
    "        dom_landmarks_where = models_dict['landmark_encoder'].forward(batch['dom_landmarks'])\n",
    "        non_dom_landmarks_where = models_dict['landmark_encoder'].forward(batch['non_dom_landmarks'])\n",
    "        \n",
    "        dom_landmarks_conc = combine_spatial_and_semantic_features(\n",
    "            spatial_features=dom_landmarks_where, \n",
    "            semantic_features=dom_landmark_embeddings\n",
    "        )\n",
    "        non_dom_landmarks_conc = combine_spatial_and_semantic_features(\n",
    "            spatial_features=non_dom_landmarks_where, \n",
    "            semantic_features=non_dom_landmark_embeddings\n",
    "        )\n",
    "        \n",
    "        wrists_where = models_dict['wrist_encoder'].forward(wrist_coordinates=batch['nose_to_wrist_dist'])\n",
    "        \n",
    "        wrists_conc = combine_wrist_embedding_and_spatial(\n",
    "            wrist_embeddings=torch.cat([dom_wrist_embedding, non_dom_wrist_embedding], dim=-1).reshape((2,-1)), \n",
    "            wrist_spatial_features=wrists_where\n",
    "        )\n",
    "        \n",
    "        # Velocity encoding (both windows)\n",
    "        dom_small_vel_encoded = models_dict['velocity_feedforward'].forward(batch['dom_velocity_small']) \n",
    "        dom_large_vel_encoded = models_dict['velocity_feedforward'].forward(batch['dom_velocity_large']) \n",
    "        non_dom_small_vel_encoded = models_dict['velocity_feedforward'].forward(batch['non_dom_velocity_small']) \n",
    "        non_dom_large_vel_encoded = models_dict['velocity_feedforward'].forward(batch['non_dom_velocity_large']) \n",
    "        \n",
    "        dom_landmarks_velocity_conc = combine_semantic_and_velocity_features(\n",
    "            semantic_features=dom_landmark_embeddings, \n",
    "            velocity_small_features=dom_small_vel_encoded, \n",
    "            velocity_large_features=dom_large_vel_encoded\n",
    "        )\n",
    "        non_dom_landmarks_velocity_conc = combine_semantic_and_velocity_features(\n",
    "            semantic_features=non_dom_landmark_embeddings, \n",
    "            velocity_small_features=non_dom_small_vel_encoded, \n",
    "            velocity_large_features=non_dom_large_vel_encoded\n",
    "        )\n",
    "        \n",
    "        wrist_vel_small_encoded = models_dict['wrist_vel_feedforward'].forward(batch['nose_to_wrist_velocity_small'])\n",
    "        wrist_vel_large_encoded = models_dict['wrist_vel_feedforward'].forward(batch['nose_to_wrist_velocity_large'])\n",
    "        \n",
    "        wrists_vel_conc = combine_wrist_embedding_and_velocity(\n",
    "            wrist_embeddings=torch.cat([dom_wrist_embedding, non_dom_wrist_embedding], dim=-1).reshape((2,-1)), \n",
    "            wrist_velocity_small=wrist_vel_small_encoded, \n",
    "            wrist_velocity_large=wrist_vel_large_encoded\n",
    "        )\n",
    "        \n",
    "        # Blendshapes encoding\n",
    "        blendshapes_encoded = models_dict['blendshapes_feedforward'](batch['blendshape_scores'])\n",
    "        \n",
    "        # Spatial transformer\n",
    "        dom_contextualized = models_dict['dom_transformer'](dom_landmarks_conc)\n",
    "        non_dom_contextualized = models_dict['non_dom_transformer'](non_dom_landmarks_conc)\n",
    "        \n",
    "        dom_pooled = models_dict['dom_pooling'](dom_contextualized)\n",
    "        non_dom_pooled = models_dict['non_dom_pooling'](non_dom_contextualized)\n",
    "        \n",
    "        dom_wrist_conc = wrists_conc[:,:,0]\n",
    "        non_dom_wrist_conc = wrists_conc[:,:,1]\n",
    "        \n",
    "        dom_spatial_combined = concat_pooled_wrists(pooled=dom_pooled, wrist=dom_wrist_conc)\n",
    "        non_dom_spatial_combined = concat_pooled_wrists(pooled=non_dom_pooled, wrist=non_dom_wrist_conc)\n",
    "        \n",
    "        # Velocity transformer\n",
    "        dom_vel_contextualized = models_dict['dom_vel_transformer'](dom_landmarks_velocity_conc)\n",
    "        non_dom_vel_contextualized = models_dict['non_dom_vel_transformer'](non_dom_landmarks_velocity_conc)\n",
    "        \n",
    "        dom_vel_pooled = models_dict['dom_vel_pooling'](dom_vel_contextualized)\n",
    "        non_dom_vel_pooled = models_dict['non_dom_vel_pooling'](non_dom_vel_contextualized)\n",
    "        \n",
    "        dom_wrist_vel_conc = wrists_vel_conc[:,:,0]\n",
    "        non_dom_wrist_vel_conc = wrists_vel_conc[:,:,1]\n",
    "        \n",
    "        dom_velocity_combined = concat_pooled_wrists(pooled=dom_vel_pooled, wrist=dom_wrist_vel_conc)\n",
    "        non_dom_velocity_combined = concat_pooled_wrists(pooled=non_dom_vel_pooled, wrist=non_dom_wrist_vel_conc)\n",
    "        \n",
    "        # Combining spatial with velocity features\n",
    "        dom_combined = concat_pooled_wrists(dom_spatial_combined, dom_velocity_combined)\n",
    "        non_dom_combined = concat_pooled_wrists(non_dom_spatial_combined, non_dom_velocity_combined)\n",
    "        \n",
    "        hands_combined = torch.stack([dom_combined, non_dom_combined], dim=2)\n",
    "        \n",
    "        # Second stage transformers between the two hands\n",
    "        confidence_scores = {\n",
    "            'Cd_spatial': batch['confidence_scores'],\n",
    "            'Ci_spatial': batch['interpolation_scores'],\n",
    "            'Cd_velocity': batch['velocity_calculation_confidence'],\n",
    "            'Ci_velocity': batch['velocity_confidence']\n",
    "        }\n",
    "        \n",
    "        enhanced_hands = models_dict['cross_hand_transformer'](hands_combined, confidence_scores)\n",
    "        \n",
    "        # Attention pooling to keep a weighted avg of the two\n",
    "        final_hands_representation = models_dict['final_pooling'](enhanced_hands)\n",
    "        \n",
    "        # Combine with blendshapes\n",
    "        frame_representation = concat_pooled_wrists(final_hands_representation, blendshapes_encoded)\n",
    "        \n",
    "        # Downsample with 1d convolution\n",
    "        downsampled_representation = models_dict['conv1d'](frame_representation)\n",
    "        \n",
    "        # Positional encodings + temporal transformer\n",
    "        downsampled_with_positional_encoding = models_dict['positional_encoder'](\n",
    "            downsampled_representation, scale=1.0\n",
    "        )\n",
    "        \n",
    "        multi_scale_representation = models_dict['temporal_transformer'](\n",
    "            downsampled_with_positional_encoding, \n",
    "            mask=batch['mask']\n",
    "        )\n",
    "        \n",
    "        # Re-enforce positional encodings with smaller scale\n",
    "        video_representation = models_dict['positional_encoder'](multi_scale_representation, scale=0.25)\n",
    "        \n",
    "        # Prepare input IDs for GPT model\n",
    "        input_ids = make_inputs_for_model(L_index=batch['L_index'], tokenizer=tokenizer)\n",
    "        \n",
    "        # Final GPT model\n",
    "        outputs = models_dict['model'](\n",
    "            input_ids=input_ids,\n",
    "            video_representations=video_representation,\n",
    "            video_mask=batch[\"mask\"],\n",
    "            L_index=batch[\"L_index\"],\n",
    "            L_values=batch[\"L_values\"],\n",
    "            label_mask=batch[\"label_mask\"]\n",
    "        )\n",
    "        \n",
    "        return outputs, input_ids\n",
    "    \n",
    "    # Training loop\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            # Set all models to training mode\n",
    "            for model in model_components:\n",
    "                model.train()\n",
    "\n",
    "            # Initialize metrics\n",
    "            train_loss = 0.0\n",
    "            batch_count = 0\n",
    "\n",
    "            # Progress tracking\n",
    "            if verbose > 0:\n",
    "                print(f\"\\n{'='*20} Epoch {epoch+1}/{num_epochs} {'='*20}\")\n",
    "\n",
    "            # Training loop\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Run forward pass\n",
    "                outputs, input_ids = forward_pass(batch)\n",
    "\n",
    "                # Get the loss\n",
    "                loss = outputs['loss']\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                if grad_clip_value is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(all_params, grad_clip_value)\n",
    "\n",
    "                # Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update metrics\n",
    "                train_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "                # Log progress\n",
    "                if verbose > 1 and batch_idx % log_freq_batches == 0:\n",
    "                    print(f\"Batch {batch_idx+1}/{train_batches} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "                # Check if we've processed enough batches\n",
    "                if batch_idx + 1 >= train_batches:\n",
    "                    break\n",
    "                \n",
    "            # Calculate epoch metrics\n",
    "            train_loss /= max(1, batch_count)\n",
    "\n",
    "            # Validation phase\n",
    "            val_loss = 0.0\n",
    "            val_min_loss = 0.0\n",
    "            val_batch_count = 0\n",
    "            validation_examples = []\n",
    "\n",
    "            # Set all models to evaluation mode\n",
    "            for model in model_components:\n",
    "                model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(val_loader):\n",
    "                    # Run forward pass\n",
    "                    outputs, input_ids = forward_pass(batch)\n",
    "\n",
    "                    # Get loss\n",
    "                    loss = outputs['loss']\n",
    "\n",
    "                    # Calculate theoretical minimum loss\n",
    "                    target_labels = torch.zeros(batch['L_values'].shape[0], batch['L_values'].shape[1], \n",
    "                                               tokenizer.vocab_size, device=device)\n",
    "                    target_labels.scatter_(2, batch['L_index'], batch['L_values'])\n",
    "                    min_loss = compute_minimum_loss(target_labels, batch['label_mask'])\n",
    "\n",
    "                    # If this is the first few batches, get example translations\n",
    "                    if batch_idx < num_examples_to_display:\n",
    "                        example_comparisons = compare_predictions(input_ids, outputs['logits'], tokenizer)\n",
    "                        for ex in example_comparisons[:min(len(example_comparisons), 5)]:  # Limit to 5 examples\n",
    "                            validation_examples.append({\n",
    "                                \"epoch\": epoch + 1,\n",
    "                                \"batch\": batch_idx,\n",
    "                                \"input\": ex[\"input\"],\n",
    "                                \"generated\": ex[\"generated\"]\n",
    "                            })\n",
    "\n",
    "                    # Update metrics\n",
    "                    val_loss += loss.item()\n",
    "                    val_min_loss += min_loss.item()\n",
    "                    val_batch_count += 1\n",
    "\n",
    "                    # Check if we've processed enough batches\n",
    "                    if batch_idx + 1 >= val_batches:\n",
    "                        break\n",
    "                    \n",
    "            # Calculate validation metrics\n",
    "            val_loss /= max(1, val_batch_count)\n",
    "            val_min_loss /= max(1, val_batch_count)\n",
    "            val_normalized_loss = val_loss / max(val_min_loss, 1e-8)  # Avoid division by zero\n",
    "\n",
    "            # Update history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_min_loss'].append(val_min_loss)\n",
    "            history['val_normalized_loss'].append(val_normalized_loss)\n",
    "\n",
    "            # Get current learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            history['learning_rates'].append(current_lr)\n",
    "\n",
    "            # Add examples to history\n",
    "            if validation_examples:\n",
    "                history['examples'].extend(validation_examples)\n",
    "\n",
    "            # Step the scheduler if it exists\n",
    "            if scheduler is not None:\n",
    "                if scheduler_type == 'plateau':\n",
    "                    scheduler.step(val_loss)  # Pass validation loss to plateau scheduler\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            # Time taken for epoch\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "            # Print epoch summary\n",
    "            if verbose > 0:\n",
    "                print(f\"\\nEpoch {epoch+1}/{num_epochs} completed in {epoch_time:.2f}s\")\n",
    "                print(f\"Train Loss: {train_loss:.4f}\")\n",
    "                print(f\"Val Loss: {val_loss:.4f} (Min: {val_min_loss:.4f}, Normalized: {val_normalized_loss:.4f})\")\n",
    "                print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "                # Display example translations\n",
    "                if validation_examples:\n",
    "                    print(\"\\nExample Translations:\")\n",
    "                    table_data = []\n",
    "                    for i, ex in enumerate(validation_examples[-5:]):  # Show last 5 examples\n",
    "                        table_data.append([f\"Example {i+1}\", ex[\"input\"], ex[\"generated\"]])\n",
    "\n",
    "                    print(tabulate(table_data, headers=[\"\", \"Input\", \"Generated\"], tablefmt=\"grid\"))\n",
    "\n",
    "            # Save checkpoint if needed\n",
    "            if (epoch + 1) % save_freq_epochs == 0 or epoch == num_epochs - 1:\n",
    "                if not save_best_only or val_loss < best_val_loss:\n",
    "                    checkpoint_path = os.path.join(run_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "                    save_checkpoint(checkpoint_path, models_dict, optimizer, scheduler, epoch, history)\n",
    "                    if verbose > 0:\n",
    "                        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if early_stopping:\n",
    "                if val_loss < best_val_loss - min_delta:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "\n",
    "                    # Save best model\n",
    "                    best_model_path = os.path.join(run_dir, \"best_model.pt\")\n",
    "                    save_checkpoint(best_model_path, models_dict, optimizer, scheduler, epoch, history)\n",
    "                    if verbose > 0:\n",
    "                        print(f\"New best model saved with val_loss: {best_val_loss:.4f}\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if verbose > 0:\n",
    "                        print(f\"Early stopping patience: {patience_counter}/{patience}\")\n",
    "\n",
    "                    if patience_counter >= patience:\n",
    "                        if verbose > 0:\n",
    "                            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                        break\n",
    "    except Exception as e:\n",
    "        if auto_rescue:\n",
    "            print(f\"\\nTraining interrupted by exception: {e}\")\n",
    "            save_rescue_checkpoint()\n",
    "        raise  # Re-raise the exception after saving\n",
    "\n",
    "    finally:\n",
    "        # Always save a final checkpoint regardless of how training ended\n",
    "        if auto_rescue:\n",
    "            final_path = os.path.join(run_dir, \"final_state_checkpoint.pt\")\n",
    "            save_checkpoint(final_path, models_dict, optimizer, scheduler, epoch, history)\n",
    "            if verbose > 0:\n",
    "                print(f\"\\nFinal state saved to {final_path}\")\n",
    "\n",
    "    # Training complete\n",
    "    if verbose > 0:\n",
    "        print(\"\\nTraining completed!\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Save history to JSON\n",
    "    with open(os.path.join(run_dir, \"history.json\"), \"w\") as f:\n",
    "        # Convert any non-serializable objects\n",
    "        serializable_history = {k: v for k, v in history.items() if k != 'examples'}\n",
    "        serializable_history['examples'] = history['examples']  # These should already be serializable\n",
    "        \n",
    "        # Convert NumPy arrays to lists\n",
    "        for key in serializable_history:\n",
    "            if isinstance(serializable_history[key], list) and serializable_history[key] and isinstance(serializable_history[key][0], np.ndarray):\n",
    "                serializable_history[key] = [x.tolist() for x in serializable_history[key]]\n",
    "        \n",
    "        json.dump(serializable_history, f, indent=2)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_history(history, os.path.join(run_dir, \"training_history.png\"))\n",
    "    \n",
    "    return history\n",
    "\n",
    "def resume_training_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    train_batches,\n",
    "    val_batches,\n",
    "    models_dict,\n",
    "    tokenizer,\n",
    "    learning_rate=None,  # Will use from checkpoint if None\n",
    "    num_epochs=30,       # Additional epochs to train\n",
    "    **kwargs             # Other training parameters\n",
    "):\n",
    "    \"\"\"Resume training from a checkpoint\"\"\"\n",
    "    import torch\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    device = kwargs.get('device', 'cuda')\n",
    "    \n",
    "    # Create optimizer shell (will be overwritten)\n",
    "    all_params = []\n",
    "    for model in models_dict.values():\n",
    "        all_params.extend(model.parameters())\n",
    "    temp_optimizer = optim.Adam(all_params, lr=learning_rate or 1e-4)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    epoch, history, extra_info = load_checkpoint(\n",
    "        checkpoint_path, models_dict, temp_optimizer, None, device=device\n",
    "    )\n",
    "    \n",
    "    if extra_info and 'interrupted_at_batch' in extra_info:\n",
    "        print(f\"Resuming from checkpoint saved at epoch {epoch+1}, batch {extra_info['interrupted_at_batch']+1}\")\n",
    "        if 'exception' in extra_info:\n",
    "            print(f\"Previous training was interrupted by exception:\\n{extra_info['exception']}\")\n",
    "    else:\n",
    "        print(f\"Resuming from checkpoint saved at epoch {epoch+1}\")\n",
    "    \n",
    "    # Use the learning rate from checkpoint if not specified\n",
    "    if learning_rate is None:\n",
    "        learning_rate = temp_optimizer.param_groups[0]['lr']\n",
    "        print(f\"Using learning rate from checkpoint: {learning_rate}\")\n",
    "    \n",
    "    # Train for additional epochs\n",
    "    print(f\"Training for {num_epochs} additional epochs\")\n",
    "    \n",
    "    # Resume training with the loaded state\n",
    "    return train_asl_model(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        train_batches=train_batches,\n",
    "        val_batches=val_batches,\n",
    "        models_dict=models_dict,\n",
    "        tokenizer=tokenizer,\n",
    "        learning_rate=learning_rate,\n",
    "        num_epochs=num_epochs,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def save_checkpoint(path, models_dict, optimizer, scheduler, epoch, history, extra_info=None):\n",
    "    \"\"\"Save a checkpoint with all model states and training information.\"\"\"\n",
    "    state_dict = {\n",
    "        'epoch': epoch,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'history': {k: v for k, v in history.items() if k != 'examples'},  # Don't save examples\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    if extra_info is not None:\n",
    "        state_dict['extra_info'] = extra_info\n",
    "\n",
    "\n",
    "    # Add scheduler state if it exists\n",
    "    if scheduler is not None:\n",
    "        state_dict['scheduler'] = scheduler.state_dict()\n",
    "    \n",
    "    # Add model states\n",
    "    for name, model in models_dict.items():\n",
    "        state_dict[f'model_{name}'] = model.state_dict()\n",
    "    \n",
    "    torch.save(state_dict, path)\n",
    "\n",
    "\n",
    "def load_checkpoint(path, models_dict, optimizer=None, scheduler=None):\n",
    "    \"\"\"Load a checkpoint into models and training state.\"\"\"\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    # Load models\n",
    "    for name, model in models_dict.items():\n",
    "        model_key = f'model_{name}'\n",
    "        if model_key in checkpoint:\n",
    "            model.load_state_dict(checkpoint[model_key])\n",
    "    \n",
    "    # Load optimizer if provided\n",
    "    if optimizer is not None and 'optimizer' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    \n",
    "    # Load scheduler if provided\n",
    "    if scheduler is not None and 'scheduler' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    \n",
    "    # Return epoch and history\n",
    "    return (\n",
    "        checkpoint.get('epoch', -1), \n",
    "        checkpoint.get('history', {}),\n",
    "        checkpoint.get('extra_info', None)\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_history(history, save_path=None):\n",
    "    \"\"\"Plot training history metrics.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    if 'val_min_loss' in history and history['val_min_loss']:\n",
    "        plt.plot(history['val_min_loss'], label='Theoretical Min Loss', linestyle='--')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Normalized loss plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    if 'val_normalized_loss' in history and history['val_normalized_loss']:\n",
    "        plt.plot(history['val_normalized_loss'], label='Normalized Val Loss')\n",
    "        plt.axhline(y=1.0, color='r', linestyle='--', label='Theoretical Minimum')\n",
    "    plt.title('Normalized Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Ratio')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Learning rate\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(history['learning_rates'], label='Learning Rate')\n",
    "    plt.title('Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Empty subplot or future metric\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.title('Reserved for Future Metric')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'embedding_table': embedding_table,\n",
    "    'landmark_encoder': landmark_encoder,\n",
    "    'wrist_encoder': wrist_encoder,\n",
    "    'blendshapes_feedforward': blendshapes_feedforward,\n",
    "    'velocity_feedforward': velocity_feedforward,\n",
    "    'wrist_vel_feedforward': wrist_vel_feedforward,\n",
    "    'dom_transformer': dom_transformer,\n",
    "    'non_dom_transformer': non_dom_transformer,\n",
    "    'dom_pooling': dom_pooling,\n",
    "    'non_dom_pooling': non_dom_pooling,\n",
    "    'dom_vel_transformer': dom_vel_transformer,\n",
    "    'non_dom_vel_transformer': non_dom_vel_transformer,\n",
    "    'dom_vel_pooling': dom_vel_pooling,\n",
    "    'non_dom_vel_pooling': non_dom_vel_pooling,\n",
    "    'cross_hand_transformer': cross_hand_transformer,\n",
    "    'final_pooling': final_pooling,\n",
    "    'conv1d': conv1d,\n",
    "    'positional_encoder': positional_encoder,\n",
    "    'temporal_transformer': temporal_transformer,\n",
    "    'model': model\n",
    "}\n",
    "\n",
    "train_high_df = pd.read_csv(\"./high_train_only_path.csv\")\n",
    "val_high_df = pd.read_csv(\"./high_val_only_path.csv\")\n",
    "\n",
    "\n",
    "train_mid_df = pd.read_csv(\"./mid_train_only_path.csv\")\n",
    "val_mid_df = pd.read_csv(\"./mid_val_only_path.csv\")\n",
    "\n",
    "train_low_df = pd.read_csv(\"./low_train_only_path.csv\")\n",
    "val_low_df = pd.read_csv(\"./low_val_only_path.csv\")\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, train_expected_batches = create_asl_dataloader(\n",
    "    low_df=train_low_df, \n",
    "    mid_df=train_mid_df, \n",
    "    high_df=train_high_df,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "val_loader, val_expected_batches = create_asl_dataloader(\n",
    "    low_df=val_low_df, \n",
    "    mid_df=val_mid_df, \n",
    "    high_df=val_high_df,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# Start training\n",
    "history = train_asl_model(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    train_batches=train_expected_batches,\n",
    "    val_batches=val_expected_batches,\n",
    "    models_dict=models_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-6,\n",
    "    num_epochs=30,\n",
    "    grad_clip_value=1.0,\n",
    "    scheduler_type='plateau',\n",
    "    scheduler_params={'factor': 0.1, 'patience': 3, 'threshold': 0.005},\n",
    "    early_stopping=True,\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    verbose=2  # Detailed logging\n",
    "    save_dir='./checkpoints',\n",
    "    save_best_only=False,\n",
    "    save_freq_epochs=1,\n",
    "    auto_rescue=True,\n",
    "    # Validation and visualization\n",
    "    num_examples_to_display=10,  # Number of examples to display during validation\n",
    "    \n",
    "    # Logging\n",
    "    log_freq_batches=50,\n",
    "    verbose=2,\n",
    "    \n",
    "    # Device\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "  \n",
    "#Later, resuming from a checkpoint\n",
    "'''\n",
    "history = resume_training_from_checkpoint(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    train_batches=train_expected_batches,\n",
    "    val_batches=val_expected_batches,\n",
    "    models_dict=models_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-6,scheduler_type\n",
    "    scheduler_params={'factor': 0.2, 'patience': 3, 'threshold': 0.005},\n",
    "    early_stopping=True,\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    verbose=2  # Detailed logging\n",
    "    save_dir='./checkpoints',\n",
    "    save_best_only=False,\n",
    "    save_freq_epochs=1,\n",
    "    auto_rescue=True,\n",
    "    # Validation and visualization\n",
    "    num_examples_to_display=5,  # Number of examples to display during validation\n",
    "    \n",
    "    # Logging\n",
    "    log_freq_batches=50,\n",
    "    verbose=2,\n",
    "    \n",
    "    # Device\n",
    "    device='cuda'\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
