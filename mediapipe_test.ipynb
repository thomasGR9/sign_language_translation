{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"test_Im.png\"\n",
    "grimace_2_path = \"grimace_2.png\"\n",
    "grimace_3_path = \"grimace_3.png\"\n",
    "not_a_face_path = \"not_a_face.png\"\n",
    "without_face_path = \"without_face.png\"\n",
    "without_right_hand_path = \"without_right_hand.png\"\n",
    "without_left_hand_path = \"without_left_hand.png\"\n",
    "hand_model_path = \"hand_landmarker.task\"\n",
    "face_model_path = \"face_landmarker.task\"\n",
    "video_path = \"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1, 2, 3, 4, 5, 9, 10, 11, 12, 21, 22,23, 24, 25, 26, 27, 33, 39, 42, 43, 44, 45, 46, 47, 50, 51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=INFO, 2=WARNING, 3=ERROR\n",
    "logging.getLogger(\"mediapipe\").setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
    "os.environ['GLOG_minloglevel'] = '3'      # Suppress Google logging (used by MediaPipe)\n",
    "os.environ['MEDIAPIPE_DISABLE_GPU'] = '1'  # Optional: Disable GPU logging messages\n",
    "logging.getLogger(\"mediapipe\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"absl\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def detect(image_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, min_face_detection_confidence=0.5, min_face_presence_confidence=0.5, num_hands=2, dominand_hand='Right', visualize=False, output_face_blendshapes=True, adaptive_threshold=True, max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects hands and face in an image, extracts hand landmark coordinates and face blendshapes.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        min_hand_detection_confidence (float): Confidence threshold for hand detection (0.0-1.0)\n",
    "        min_hand_presence_confidence (float): Confidence threshold for hand presence (0.0-1.0)\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        visualize (bool): Whether to visualize the results\n",
    "        output_face_blendshapes (bool): Whether to detect and extract face blendshapes\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (dom_landmarks, non_dom_landmarks, wrists, confidence_scores, detection_status, \n",
    "                blendshape_scores, face_landmark_5, face_detected)\n",
    "               - dom_landmarks: NumPy array of shape [20, 3] with coordinates of dominant hand landmarks\n",
    "               - non_dom_landmarks: NumPy array of shape [20, 3] with coordinates of non-dominant hand landmarks\n",
    "               - wrists: NumPy array of shape [2, 2] with coordinates of both wrists [x, y]\n",
    "               - confidence_scores: NumPy array of shape [2] with confidence scores [dominant_hand, non_dominant_hand]\n",
    "               - detection_status: NumPy array of shape [2] with binary detection status [dominant_hand, non_dominant_hand]\n",
    "               - blendshape_scores: NumPy array of shape [26] with selected face blendshape scores\n",
    "               - face_landmark_5: NumPy array of shape [2] with coordinates of the 5th face landmark [x, y]\n",
    "               - face_detected: Binary value (1 if face detected, 0 if not)\n",
    "    \"\"\"\n",
    "    # Initialize output arrays for face detection\n",
    "    blendshape_scores = np.zeros(52)\n",
    "    nose_landmark = np.zeros(2)\n",
    "    left_eye_landmark = np.zeros(2)\n",
    "    right_eye_landmark = np.zeros(2)\n",
    "    face_detected = 0\n",
    "    \n",
    "    # PART 1: HAND LANDMARK DETECTION\n",
    "    # 1.1: Configure the hand landmarker\n",
    "    hand_base_options = python.BaseOptions(\n",
    "        model_asset_path=hand_model_path\n",
    "    )\n",
    "\n",
    "    VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "    # Configure detection options\n",
    "    hand_options = vision.HandLandmarkerOptions(\n",
    "        base_options=hand_base_options,\n",
    "        num_hands=num_hands,                             \n",
    "        min_hand_detection_confidence=min_hand_detection_confidence,       \n",
    "        min_hand_presence_confidence=min_hand_presence_confidence,        \n",
    "        min_tracking_confidence=0.5,             \n",
    "        running_mode=VisionRunningMode.IMAGE\n",
    "    )\n",
    "\n",
    "    # Create the hand detector\n",
    "    hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "\n",
    "    # 1.2: Load the input image\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "\n",
    "    # 1.3: Detect hand landmarks\n",
    "    hand_detection_result = hand_detector.detect(image)\n",
    "    \n",
    "    # Initialize hand output arrays with zeros\n",
    "    dom_landmarks = np.zeros((20, 3))       # 20 landmarks (excluding wrist), [x,y,z]\n",
    "    non_dom_landmarks = np.zeros((20, 3))   # 20 landmarks (excluding wrist), [x,y,z]\n",
    "    wrists = np.zeros((2, 2))               # 2 wrists, [x,y]\n",
    "    confidence_scores = np.zeros(2)         # Confidence scores for [dominant, non-dominant]\n",
    "    interpolation_scores = np.zeros(2) #Interpolation scores for [dominant, non-dominant]. Used later.\n",
    "    detection_status = np.zeros(2, dtype=np.int32)  # Binary detection status [dominant, non-dominant]\n",
    "    nose_to_wrist_dist = np.zeros((2, 2))\n",
    "    \n",
    "    # 1.4: Process hand landmarks if hands are detected\n",
    "    if hand_detection_result.hand_landmarks and hand_detection_result.handedness:\n",
    "        dom_hand_found = False\n",
    "        non_dom_hand_found = False\n",
    "        \n",
    "        # First, find the dominant and non-dominant hands in detection results\n",
    "        for idx, handedness in enumerate(hand_detection_result.handedness):\n",
    "            hand_type = handedness[0].category_name  # 'Left' or 'Right'\n",
    "            hand_score = handedness[0].score  # Confidence score for the handedness classification\n",
    "            \n",
    "            if hand_type == dominand_hand:\n",
    "                # This is the dominant hand\n",
    "                dom_hand_found = True\n",
    "                detection_status[0] = 1  # Set detection status to 1 (detected)\n",
    "                confidence_scores[0] = hand_score  # Store confidence score\n",
    "                interpolation_scores[0] = 1\n",
    "                \n",
    "                # Store dominant hand wrist coordinates [x,y]\n",
    "                dom_hand_landmarks = hand_detection_result.hand_landmarks[idx]\n",
    "                wrists[0, 0] = dom_hand_landmarks[0].x\n",
    "                wrists[0, 1] = dom_hand_landmarks[0].y\n",
    "                \n",
    "                # Store all other dominant hand landmarks (excluding wrist)\n",
    "                for i in range(1, 21):  # Landmarks 1-20 (skipping wrist which is index 0)\n",
    "                    dom_landmarks[i-1, 0] = dom_hand_landmarks[i].x\n",
    "                    dom_landmarks[i-1, 1] = dom_hand_landmarks[i].y\n",
    "                    dom_landmarks[i-1, 2] = dom_hand_landmarks[i].z\n",
    "                    \n",
    "            elif hand_type != dominand_hand:\n",
    "                # This is the non-dominant hand\n",
    "                non_dom_hand_found = True\n",
    "                detection_status[1] = 1  # Set detection status to 1 (detected)\n",
    "                confidence_scores[1] = hand_score  # Store confidence score\n",
    "                interpolation_scores[1] = 1\n",
    "                \n",
    "                # Store non-dominant hand wrist coordinates [x,y]\n",
    "                non_dom_hand_landmarks = hand_detection_result.hand_landmarks[idx]\n",
    "                wrists[1, 0] = non_dom_hand_landmarks[0].x\n",
    "                wrists[1, 1] = non_dom_hand_landmarks[0].y\n",
    "                \n",
    "                # Store all other non-dominant hand landmarks (excluding wrist)\n",
    "                for i in range(1, 21):  # Landmarks 1-20 (skipping wrist)\n",
    "                    non_dom_landmarks[i-1, 0] = non_dom_hand_landmarks[i].x\n",
    "                    non_dom_landmarks[i-1, 1] = non_dom_hand_landmarks[i].y\n",
    "                    non_dom_landmarks[i-1, 2] = non_dom_hand_landmarks[i].z\n",
    "                    \n",
    "        # Log information about which hands were found\n",
    "        print(f\"Dominant hand ({dominand_hand}) detected: {dom_hand_found}\")\n",
    "        print(f\"Non-dominant hand detected: {non_dom_hand_found}\")\n",
    "    \n",
    "\n",
    "   # PART 2: FACE LANDMARK DETECTION (If requested)\n",
    "    if output_face_blendshapes:\n",
    "        try:\n",
    "            # 2.1: Configure the face landmarker\n",
    "            face_base_options = python.BaseOptions(\n",
    "                model_asset_path=face_model_path\n",
    "            )\n",
    "            \n",
    "            # Configure face detection options\n",
    "            face_options = vision.FaceLandmarkerOptions(\n",
    "                base_options=face_base_options,\n",
    "                min_face_detection_confidence=min_face_detection_confidence,\n",
    "                min_face_presence_confidence=min_face_presence_confidence,\n",
    "                output_face_blendshapes=True,\n",
    "                num_faces=1,\n",
    "                running_mode=VisionRunningMode.IMAGE\n",
    "            )\n",
    "            \n",
    "            # Create the face detector\n",
    "            face_detector = vision.FaceLandmarker.create_from_options(face_options)\n",
    "            \n",
    "            # 2.2: Detect face landmarks (reuse the same image)\n",
    "            face_detection_result = face_detector.detect(image)\n",
    "            \n",
    "            # 2.3: Process face blendshapes if face is detected\n",
    "            if (face_detection_result.face_blendshapes and len(face_detection_result.face_blendshapes) > 0 and\n",
    "                face_detection_result.face_landmarks and len(face_detection_result.face_landmarks) > 0):\n",
    "                \n",
    "                # Set face detected flag to 1\n",
    "                face_detected = 1\n",
    "                \n",
    "                # Get all blendshapes from the first face\n",
    "                all_blendshapes = face_detection_result.face_blendshapes[0]\n",
    "                \n",
    "                # Initialize blendshape_scores with the correct size to hold all blendshapes\n",
    "                # Assuming MediaPipe returns all 52 blendshapes\n",
    "                blendshape_scores = np.zeros(len(all_blendshapes))\n",
    "                \n",
    "                # Fill the blendshape_scores array with ALL scores\n",
    "                for i in range(len(all_blendshapes)):\n",
    "                    blendshape_scores[i] = all_blendshapes[i].score\n",
    "                \n",
    "                # Get nose coordinates\n",
    "                nose = face_detection_result.face_landmarks[0][4]\n",
    "                nose_landmark[0] = nose.x\n",
    "                nose_landmark[1] = nose.y\n",
    "    \n",
    "                # Get eye coordinates\n",
    "                left_eye = face_detection_result.face_landmarks[0][473]\n",
    "                left_eye_landmark[0] = left_eye.x\n",
    "                left_eye_landmark[1] = left_eye.y\n",
    "    \n",
    "                right_eye = face_detection_result.face_landmarks[0][468]\n",
    "                right_eye_landmark[0] = right_eye.x\n",
    "                right_eye_landmark[1] = right_eye.y\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during face detection: {e}\")\n",
    "            # Keep default zero values for face outputs if detection fails\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PART 3: VISUALIZATION\n",
    "    if visualize:\n",
    "        # Load the image with OpenCV for visualization\n",
    "        img_cv = cv2.imread(image_path)\n",
    "        img_height, img_width, _ = img_cv.shape\n",
    "\n",
    "        # 3.1: Draw hand landmarks if hands are detected\n",
    "        if hand_detection_result.hand_landmarks:\n",
    "            print(f\"Visualizing {len(hand_detection_result.hand_landmarks)} hands\")\n",
    "            \n",
    "            # Define connections between landmarks for hand skeleton\n",
    "            connections = [\n",
    "                # Thumb connections\n",
    "                (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "                # Index finger connections\n",
    "                (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "                # Middle finger connections\n",
    "                (0, 9), (9, 10), (10, 11), (11, 12),\n",
    "                # Ring finger connections\n",
    "                (0, 13), (13, 14), (14, 15), (15, 16),\n",
    "                # Pinky finger connections\n",
    "                (0, 17), (17, 18), (18, 19), (19, 20),\n",
    "                # Palm connections\n",
    "                (0, 5), (5, 9), (9, 13), (13, 17)\n",
    "            ]\n",
    "            \n",
    "            for idx, hand_landmarks in enumerate(hand_detection_result.hand_landmarks):\n",
    "                # Determine if this is the dominant hand\n",
    "                is_dominant = False\n",
    "                if hand_detection_result.handedness:\n",
    "                    hand_type = hand_detection_result.handedness[idx][0].category_name\n",
    "                    is_dominant = (hand_type == dominand_hand)\n",
    "                \n",
    "                # Use different colors for dominant vs non-dominant hand\n",
    "                hand_color = (0, 0, 255) if is_dominant else (255, 0, 0)  # Blue for dominant, Red for non-dominant\n",
    "                \n",
    "                # Draw all landmark points\n",
    "                for landmark in hand_landmarks:\n",
    "                    # Convert normalized coordinates to pixel coordinates\n",
    "                    x = int(landmark.x * img_width)\n",
    "                    y = int(landmark.y * img_height)\n",
    "                    \n",
    "                    # Draw the landmark point\n",
    "                    cv2.circle(img_cv, (x, y), 5, hand_color, -1)\n",
    "                \n",
    "                # Draw connections between landmarks (hand skeleton)\n",
    "                for connection in connections:\n",
    "                    start_idx, end_idx = connection\n",
    "                    \n",
    "                    if start_idx < len(hand_landmarks) and end_idx < len(hand_landmarks):\n",
    "                        start_point = hand_landmarks[start_idx]\n",
    "                        end_point = hand_landmarks[end_idx]\n",
    "                        \n",
    "                        # Convert normalized coordinates to pixel coordinates\n",
    "                        start_x = int(start_point.x * img_width)\n",
    "                        start_y = int(start_point.y * img_height)\n",
    "                        end_x = int(end_point.x * img_width)\n",
    "                        end_y = int(end_point.y * img_height)\n",
    "                        \n",
    "                        # Draw the connection line\n",
    "                        cv2.line(img_cv, (start_x, start_y), (end_x, end_y), hand_color, 2)\n",
    "                \n",
    "                # Add hand type label (Left/Right, Dominant/Non-dominant)\n",
    "                if hand_detection_result.handedness:\n",
    "                    handedness = hand_detection_result.handedness[idx]\n",
    "                    hand_type = handedness[0].category_name  # 'Left' or 'Right'\n",
    "                    hand_score = handedness[0].score\n",
    "                    dom_status = \"Dominant\" if hand_type == dominand_hand else \"Non-dominant\"\n",
    "                    cv2.putText(img_cv, f\"{hand_type} Hand - {dom_status} ({hand_score:.2f})\", \n",
    "                            (10, 30 + idx * 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            0.8, hand_color, 2)\n",
    "                    \n",
    "                    # Calculate and draw a bounding box\n",
    "                    x_coords = [landmark.x for landmark in hand_landmarks]\n",
    "                    y_coords = [landmark.y for landmark in hand_landmarks]\n",
    "                    min_x, max_x = min(x_coords), max(x_coords)\n",
    "                    min_y, max_y = min(y_coords), max(y_coords)\n",
    "                    \n",
    "                    # Convert to pixel coordinates\n",
    "                    min_x, max_x = int(min_x * img_width), int(max_x * img_width)\n",
    "                    min_y, max_y = int(min_y * img_height), int(max_y * img_height)\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(img_cv, (min_x, min_y), (max_x, max_y), hand_color, 2)\n",
    "\n",
    "        # 3.2: Draw Nose if face was detected\n",
    "        if face_detected == 1:\n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            face_x = int(nose_landmark[0] * img_width)\n",
    "            face_y = int(nose_landmark[1] * img_height)\n",
    "            \n",
    "            # Draw the Nose with a distinctive color and size\n",
    "            cv2.circle(img_cv, (face_x, face_y), 8, (0, 255, 255), -1)  # Yellow circle\n",
    "            cv2.putText(img_cv, \"Nose\", (face_x + 10, face_y), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "            # Draw eyes\n",
    "            left_eye_x = int(left_eye_landmark[0] * img_width)\n",
    "            left_eye_y = int(left_eye_landmark[1] * img_height)\n",
    "            right_eye_x = int(right_eye_landmark[0] * img_width)\n",
    "            right_eye_y = int(right_eye_landmark[1] * img_height)\n",
    "            \n",
    "            cv2.circle(img_cv, (left_eye_x, left_eye_y), 6, (255, 255, 0), -1)  # Cyan circle\n",
    "            cv2.circle(img_cv, (right_eye_x, right_eye_y), 6, (255, 255, 0), -1)  # Cyan circle\n",
    "            cv2.line(img_cv, (left_eye_x, left_eye_y), (right_eye_x, right_eye_y), (255, 255, 0), 2)\n",
    "        # 3.3: Add detection status information to visualization\n",
    "        y_pos = img_height - 80\n",
    "        hand_status_text = f\"Hand Detection: Dom={detection_status[0]}, Non-Dom={detection_status[1]}\"\n",
    "        hand_conf_text = f\"Hand Confidence: Dom={confidence_scores[0]:.2f}, Non-Dom={confidence_scores[1]:.2f}\"\n",
    "        face_status_text = f\"Face Detection: {face_detected}\"\n",
    "        \n",
    "        cv2.putText(img_cv, hand_status_text, (10, y_pos), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img_cv, hand_conf_text, (10, y_pos + 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img_cv, face_status_text, (10, y_pos + 60), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # 3.4: Display the result\n",
    "        cv2.imshow('Hand and Face Landmarks', img_cv)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    if face_detected==1:\n",
    "        #Calculate distance between the eyes\n",
    "        eyes_diff = right_eye_landmark-left_eye_landmark\n",
    "        eyes_distance = np.sqrt(eyes_diff.dot(eyes_diff))\n",
    "        if detection_status[0]==1 and detection_status[1]==1:\n",
    "            nose_to_wrist_dist = (wrists-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / eyes_distance\n",
    "        elif detection_status[0]==1 and detection_status[1]==0:\n",
    "            nose_to_wrist_dist[0, :] = (wrists[0, :]-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "        elif detection_status[0]==0 and detection_status[1]==1:\n",
    "            nose_to_wrist_dist[1,:] = (wrists[1,:]-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "        \n",
    "    elif face_detected==0 and detection_status[0]==1:\n",
    "        #Calculate palm width distance as fallback scaling factor\n",
    "        palm_width_diff = dom_landmarks[5, :]- dom_landmarks[17, :]\n",
    "        palm_width_dist = np.sqrt(palm_width_diff.dot(palm_width_diff))\n",
    "        if detection_status[1]==1:\n",
    "            nose_to_wrist_dist = (wrists-nose_landmark) / palm_width_dist\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / palm_width_dist\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / palm_width_dist\n",
    "        elif detection_status[1]==0:\n",
    "            nose_to_wrist_dist[0,:] = (wrists[0,:]-nose_landmark) / palm_width_dist\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / palm_width_dist\n",
    "    elif face_detected==0 and detection_status[0]==0 and detection_status[1]==1:\n",
    "        #Calculate palm width distance as fallback scaling factor\n",
    "        palm_width_diff = non_dom_landmarks[5, :]- non_dom_landmarks[17, :]\n",
    "        palm_width_dist = np.sqrt(palm_width_diff.dot(palm_width_diff))\n",
    "        nose_to_wrist_dist[1,:] = (wrists[1,:]-nose_landmark) / palm_width_dist\n",
    "        #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "        non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / palm_width_dist\n",
    "    \n",
    "\n",
    "    \n",
    "    # Return all requested outputs\n",
    "    return dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824825.617509    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824825.620415  282728 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824825.856196  282729 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824825.881208  282730 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824825.948322    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824825.952441  282744 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824825.953249    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824825.961192  282746 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824825.988733  282749 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: True\n",
      "Non-dominant hand detected: True\n",
      "Visualizing 2 hands\n"
     ]
    }
   ],
   "source": [
    "lol = detect(grimace_2_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, num_hands=2, dominand_hand='Left', visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.22157899, -0.47131054, -0.03073101],\n",
       "        [-0.17301513, -0.94633667, -0.04778747],\n",
       "        [ 0.09016798, -1.17645313, -0.06066544],\n",
       "        [ 0.44656346, -1.2285489 , -0.07052706],\n",
       "        [ 0.75239831, -1.00154878, -0.03440262],\n",
       "        [ 0.74546371, -0.32252427, -0.05854694],\n",
       "        [ 0.52926255,  0.01389155, -0.07399104],\n",
       "        [ 0.3284734 ,  0.18489803, -0.08165359],\n",
       "        [ 0.96142811, -0.83551715, -0.02785412],\n",
       "        [ 0.733497  , -0.12437514, -0.05066952],\n",
       "        [ 0.38909197,  0.19143662, -0.05764463],\n",
       "        [ 0.1054417 ,  0.3583336 , -0.05927849],\n",
       "        [ 1.04701376, -0.64099421, -0.02452194],\n",
       "        [ 0.80774103, -0.04213003, -0.04881671],\n",
       "        [ 0.46820677,  0.20524576, -0.04839684],\n",
       "        [ 0.19587368,  0.31181122, -0.04229698],\n",
       "        [ 1.07308434, -0.42637636, -0.02360797],\n",
       "        [ 1.01975656, -0.01941465, -0.04110365],\n",
       "        [ 0.78549988,  0.19136538, -0.03911975],\n",
       "        [ 0.56846131,  0.24917491, -0.03242829]]),\n",
       " array([[ 0.43443525, -0.28010929, -0.01027886],\n",
       "        [ 0.80353707, -0.54089751, -0.0247014 ],\n",
       "        [ 0.97556812, -0.65599555, -0.03816538],\n",
       "        [ 1.01942988, -0.56354025, -0.05229027],\n",
       "        [ 0.86045716, -0.13682166, -0.03400054],\n",
       "        [ 1.31591499,  0.24062976, -0.04829271],\n",
       "        [ 1.60218625,  0.43870974, -0.05713674],\n",
       "        [ 1.81017121,  0.57159162, -0.06226961],\n",
       "        [ 0.61163023,  0.19540783, -0.03670957],\n",
       "        [ 1.09116728,  0.50590183, -0.04773441],\n",
       "        [ 1.4089959 ,  0.66185109, -0.05110947],\n",
       "        [ 1.64211591,  0.76527366, -0.05398791],\n",
       "        [ 0.37204199,  0.52394618, -0.03922421],\n",
       "        [ 0.87258241,  0.80436906, -0.0487699 ],\n",
       "        [ 1.172205  ,  0.94312534, -0.04679542],\n",
       "        [ 1.3891072 ,  1.00824313, -0.04479555],\n",
       "        [ 0.14446114,  0.84810335, -0.04231404],\n",
       "        [ 0.57520175,  1.08297814, -0.05030103],\n",
       "        [ 0.84108896,  1.15069684, -0.04695994],\n",
       "        [ 1.05146618,  1.16495436, -0.04323559]]),\n",
       " array([0.84230059, 0.72651029]),\n",
       " array([1., 1.]),\n",
       " array([1, 1], dtype=int32),\n",
       " array([5.13106784e-07, 2.02846597e-03, 3.18804756e-03, 5.41452050e-01,\n",
       "        7.69849420e-02, 3.81750800e-02, 1.87551632e-05, 3.96643820e-08,\n",
       "        2.66600040e-07, 3.10787767e-01, 3.27007324e-01, 3.97320539e-01,\n",
       "        3.99153769e-01, 1.49953105e-02, 5.87362051e-01, 5.51258922e-01,\n",
       "        2.45020236e-03, 3.55305187e-02, 3.04265637e-02, 4.82355088e-01,\n",
       "        5.29462278e-01, 4.21624212e-03, 2.38157995e-03, 6.42919476e-05,\n",
       "        1.46195479e-03, 3.30839418e-02, 7.51869346e-04, 9.70096048e-03,\n",
       "        8.69681120e-01, 8.20557177e-01, 7.09664860e-09, 2.41063027e-08,\n",
       "        2.43527279e-03, 4.74228486e-02, 2.91958713e-04, 1.99188435e-04,\n",
       "        3.98596115e-02, 1.40069559e-01, 1.56948388e-01, 7.12577777e-04,\n",
       "        1.05726868e-01, 7.86331594e-01, 1.80265668e-03, 3.23777436e-04,\n",
       "        1.01934398e-04, 2.63166294e-04, 7.19477441e-07, 1.26338258e-04,\n",
       "        9.95448863e-06, 5.62861487e-06, 3.39345775e-07, 4.37951542e-08]),\n",
       " 1,\n",
       " array([[ 3.81955315,  4.40232786],\n",
       "        [-3.99970874,  4.21392517]]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_detect(image_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, \n",
    "                   min_face_detection_confidence=0.5, min_face_presence_confidence=0.5, \n",
    "                   num_hands=2, dominand_hand='Right', visualize=False, output_face_blendshapes=True,\n",
    "                   max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Adaptively detects hands and face by progressively lowering detection thresholds\n",
    "    for undetected body parts.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        min_hand_detection_confidence (float): Initial confidence threshold for hand detection\n",
    "        min_hand_presence_confidence (float): Initial confidence threshold for hand presence\n",
    "        min_face_detection_confidence (float): Initial confidence threshold for face detection\n",
    "        min_face_presence_confidence (float): Initial confidence threshold for face presence\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        visualize (bool): Whether to visualize the final results\n",
    "        output_face_blendshapes (bool): Whether to detect and extract face blendshapes\n",
    "        max_attempts (int): Maximum number of detection attempts with lowered thresholds\n",
    "        threshold_reduction_factor (float): Factor to multiply thresholds by on each attempt (0-1)\n",
    "        min_threshold (float): Minimum threshold to prevent excessive lowering\n",
    "        \n",
    "    Returns:\n",
    "        Same output as the detect() function\n",
    "    \"\"\"\n",
    "    # Import the original detect function\n",
    "    #from your_module import detect  # Replace with actual module name\n",
    "    \n",
    "    # Store original thresholds\n",
    "    orig_hand_detection_conf = min_hand_detection_confidence\n",
    "    orig_hand_presence_conf = min_hand_presence_confidence\n",
    "    orig_face_detection_conf = min_face_detection_confidence\n",
    "    orig_face_presence_conf = min_face_presence_confidence\n",
    "    \n",
    "    # Initialize best results and detection status\n",
    "    best_results = None\n",
    "    best_detection_status = [0, 0]  # [dom_hand, non_dom_hand]\n",
    "    best_face_detected = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    # Try detection with progressively lower thresholds\n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"\\n--- Attempt {attempt+1}/{max_attempts} ---\")\n",
    "        \n",
    "        # Calculate current thresholds\n",
    "        if attempt > 0:\n",
    "            # Only lower thresholds for undetected parts\n",
    "            # For hands\n",
    "            if best_detection_status[0] == 0:  # Dominant hand not detected\n",
    "                hand_detection_conf_dom = max(orig_hand_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                hand_presence_conf_dom = max(orig_hand_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering dominant hand thresholds: {hand_detection_conf_dom:.3f}, {hand_presence_conf_dom:.3f}\")\n",
    "            else:\n",
    "                hand_detection_conf_dom = orig_hand_detection_conf\n",
    "                hand_presence_conf_dom = orig_hand_presence_conf\n",
    "                \n",
    "            if best_detection_status[1] == 0:  # Non-dominant hand not detected\n",
    "                hand_detection_conf_non_dom = max(orig_hand_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                hand_presence_conf_non_dom = max(orig_hand_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering non-dominant hand thresholds: {hand_detection_conf_non_dom:.3f}, {hand_presence_conf_non_dom:.3f}\")\n",
    "            else:\n",
    "                hand_detection_conf_non_dom = orig_hand_detection_conf\n",
    "                hand_presence_conf_non_dom = orig_hand_presence_conf\n",
    "            \n",
    "            # Use the minimum of the two calculated thresholds (MediaPipe doesn't support per-hand thresholds)\n",
    "            current_hand_detection_conf = min(hand_detection_conf_dom, hand_detection_conf_non_dom)\n",
    "            current_hand_presence_conf = min(hand_presence_conf_dom, hand_presence_conf_non_dom)\n",
    "            \n",
    "            # For face\n",
    "            if output_face_blendshapes and best_face_detected == 0:  # Face not detected\n",
    "                current_face_detection_conf = max(orig_face_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                current_face_presence_conf = max(orig_face_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering face thresholds: {current_face_detection_conf:.3f}, {current_face_presence_conf:.3f}\")\n",
    "            else:\n",
    "                current_face_detection_conf = orig_face_detection_conf\n",
    "                current_face_presence_conf = orig_face_presence_conf\n",
    "        else:\n",
    "            # Use original thresholds for first attempt\n",
    "            current_hand_detection_conf = orig_hand_detection_conf\n",
    "            current_hand_presence_conf = orig_hand_presence_conf\n",
    "            current_face_detection_conf = orig_face_detection_conf\n",
    "            current_face_presence_conf = orig_face_presence_conf\n",
    "            print(f\"Using original thresholds: hands={current_hand_detection_conf}, face={current_face_detection_conf}\")\n",
    "        \n",
    "        # Call detect with current thresholds (don't visualize intermediate attempts)\n",
    "        results = detect(image_path,  hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                        min_hand_detection_confidence=current_hand_detection_conf,\n",
    "                        min_hand_presence_confidence=current_hand_presence_conf,\n",
    "                        min_face_detection_confidence=current_face_detection_conf,\n",
    "                        min_face_presence_confidence=current_face_presence_conf,\n",
    "                        num_hands=num_hands,\n",
    "                        dominand_hand=dominand_hand,\n",
    "                        visualize=False,\n",
    "                        output_face_blendshapes=output_face_blendshapes)\n",
    "        \n",
    "        # Unpack results\n",
    "        dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
    "        \n",
    "        # Compare with best results so far\n",
    "        current_detection_count = detection_status[0] + detection_status[1] + face_detected\n",
    "        best_detection_count = best_detection_status[0] + best_detection_status[1] + best_face_detected\n",
    "        \n",
    "        if best_results is None or current_detection_count > best_detection_count:\n",
    "            best_results = results\n",
    "            best_detection_status = [detection_status[0], detection_status[1]]\n",
    "            best_face_detected = face_detected\n",
    "            \n",
    "            print(f\"New best detection: dominant hand={detection_status[0]}, \"\n",
    "                  f\"non-dominant hand={detection_status[1]}, face={face_detected}\")\n",
    "            \n",
    "            # If everything is detected, we can stop early\n",
    "            if detection_status[0] == 1 and detection_status[1] == 1 and (face_detected == 1 or not output_face_blendshapes):\n",
    "                print(\"All body parts detected. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"No improvement in detection. Continuing to next attempt.\")\n",
    "    \n",
    "    # Run final detection with visualization if requested\n",
    "    if visualize:\n",
    "        print(\"\\n--- Visualizing final results ---\")\n",
    "        # Call detect one more time with the parameters that gave best results, but with visualize=True\n",
    "        # For simplicity, we'll just use the best thresholds we found\n",
    "        # This is slightly inefficient (one extra detection) but keeps the code clean\n",
    "        \n",
    "        # Determine which thresholds gave the best results\n",
    "        if best_detection_status[0] == 0:  # If dominant hand not detected in best result\n",
    "            hand_detection_conf = min_threshold\n",
    "            hand_presence_conf = min_threshold\n",
    "        else:\n",
    "            hand_detection_conf = orig_hand_detection_conf\n",
    "            hand_presence_conf = orig_hand_presence_conf\n",
    "            \n",
    "        if output_face_blendshapes and best_face_detected == 0:  # If face not detected in best result\n",
    "            face_detection_conf = min_threshold\n",
    "            face_presence_conf = min_threshold\n",
    "        else:\n",
    "            face_detection_conf = orig_face_detection_conf\n",
    "            face_presence_conf = orig_face_presence_conf\n",
    "        \n",
    "        # Run final detection with visualization\n",
    "        final_results = detect(image_path, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                              min_hand_detection_confidence=hand_detection_conf,\n",
    "                              min_hand_presence_confidence=hand_presence_conf, \n",
    "                              min_face_detection_confidence=face_detection_conf,\n",
    "                              min_face_presence_confidence=face_presence_conf,\n",
    "                              num_hands=num_hands,\n",
    "                              dominand_hand=dominand_hand,\n",
    "                              visualize=True,\n",
    "                              output_face_blendshapes=output_face_blendshapes)\n",
    "        \n",
    "        # Use these results if they're better than our best so far\n",
    "        dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = final_results\n",
    "        current_detection_count = detection_status[0] + detection_status[1] + face_detected\n",
    "        best_detection_count = best_detection_status[0] + best_detection_status[1] + best_face_detected\n",
    "        \n",
    "        if current_detection_count > best_detection_count:\n",
    "            best_results = final_results\n",
    "    \n",
    "    # Print final detection summary\n",
    "    print(\"\\n=== Detection Summary ===\")\n",
    "    dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = best_results\n",
    "    print(f\"Dominant hand detected: {detection_status[0] == 1} (confidence: {confidence_scores[0]:.3f})\")\n",
    "    print(f\"Non-dominant hand detected: {detection_status[1] == 1} (confidence: {confidence_scores[1]:.3f})\")\n",
    "    if output_face_blendshapes:\n",
    "        print(f\"Face detected: {face_detected == 1}\")\n",
    "    print(f\"Total detection attempts: {attempt+1}\")\n",
    "    return best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=0, non-dominant hand=1, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824986.936870    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824986.940818  284930 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.039888  284934 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.056971  284940 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.133708    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.135590  284946 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.136250    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.142800  284947 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.160368  284950 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.199556    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.202131  284962 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.238721  284964 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.252189  284966 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering dominant hand thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824987.304140    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.307504  284978 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.308167    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.314875  284980 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.507001  284979 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.543464    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.547164  284994 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.568495  284996 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.686856  285000 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Visualizing final results ---\n",
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824987.738748    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.741043  285010 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.741756    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.750388  285011 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.779803  285022 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.819273    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.822941  285026 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.853940  285027 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.874366  285035 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.927914    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.931627  285064 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.932330    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.944897  285072 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.965721  285078 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 1 hands\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: False (confidence: 0.000)\n",
      "Non-dominant hand detected: True (confidence: 0.948)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n"
     ]
    }
   ],
   "source": [
    "best_results = adaptive_detect(without_left_hand_path, hand_model_path=hand_model_path, face_model_path=face_model_path,  min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, num_hands=2, dominand_hand='Left', visualize=True,output_face_blendshapes=True,max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dom_landmarks, non_dom_landmarks, confidence_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[ 4.4302598 ,  5.79258936, -0.01529764],\n",
       "        [ 4.97843194,  5.60716573, -0.03023903],\n",
       "        [ 5.4227688 ,  5.474167  , -0.04531926],\n",
       "        [ 5.6396736 ,  5.29133956, -0.06161126],\n",
       "        [ 5.46952226,  5.95203357, -0.02546122],\n",
       "        [ 6.14746077,  5.96317265, -0.03837759],\n",
       "        [ 6.5033938 ,  5.98504281, -0.04724246],\n",
       "        [ 6.77242064,  5.99687042, -0.05398038],\n",
       "        [ 5.45199885,  6.19957023, -0.02953801],\n",
       "        [ 6.20066676,  6.22105431, -0.03755009],\n",
       "        [ 6.60851154,  6.23570749, -0.04424638],\n",
       "        [ 6.9013028 ,  6.24974116, -0.05119639],\n",
       "        [ 5.33187311,  6.40705239, -0.03495561],\n",
       "        [ 6.02315108,  6.45053573, -0.04479358],\n",
       "        [ 6.42136374,  6.46895391, -0.05380662],\n",
       "        [ 6.71558972,  6.47136295, -0.0608963 ],\n",
       "        [ 5.14666546,  6.58524524, -0.04154579],\n",
       "        [ 5.66482483,  6.65936167, -0.04902381],\n",
       "        [ 5.98583324,  6.67587705, -0.05188949],\n",
       "        [ 6.25919546,  6.67568786, -0.0540517 ]]),\n",
       " array([0.       , 0.9484275]),\n",
       " array([0., 1.]),\n",
       " array([0, 1], dtype=int32),\n",
       " array([2.92115374e-07, 1.33861089e-02, 1.24196718e-02, 2.88109779e-01,\n",
       "        5.75697683e-02, 3.07354219e-02, 1.32326995e-05, 1.58878031e-08,\n",
       "        8.72999664e-08, 3.63642573e-01, 4.11365360e-01, 6.62605762e-01,\n",
       "        6.69030905e-01, 2.73421267e-03, 5.51178098e-01, 5.63342750e-01,\n",
       "        1.03895655e-02, 1.23140477e-02, 8.84756818e-03, 2.24526033e-01,\n",
       "        2.50775576e-01, 4.66695800e-03, 1.89312676e-03, 3.73961666e-05,\n",
       "        1.19879853e-03, 7.60920672e-03, 4.57036338e-04, 1.92889536e-03,\n",
       "        8.89772832e-01, 8.59930277e-01, 8.38296987e-10, 1.49412671e-09,\n",
       "        1.53606525e-02, 1.33757144e-02, 1.26740182e-04, 8.43084490e-05,\n",
       "        7.94648603e-02, 9.92558002e-02, 8.17362487e-01, 1.14337606e-02,\n",
       "        1.67473480e-02, 3.32397074e-01, 3.43104475e-03, 1.09493383e-03,\n",
       "        1.44784761e-04, 2.09721227e-04, 8.35517653e-07, 5.79597008e-06,\n",
       "        3.15646721e-05, 2.39465880e-05, 5.11401367e-07, 3.26468843e-08]),\n",
       " 1,\n",
       " array([[ 0.        ,  0.        ],\n",
       "        [-1.39355698,  4.85907125]]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_video(video_path, adaptive_detect_func, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                 min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5,\n",
    "                 min_face_detection_confidence=0.5, min_face_presence_confidence=0.5,\n",
    "                 num_hands=2, output_face_blendshapes=True,\n",
    "                 max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2, \n",
    "                 frame_step=1, start_time_seconds=0, end_time_seconds=None,\n",
    "                 save_failure_screenshots=False):\n",
    "    \"\"\"\n",
    "    Process a video frame-by-frame using the adaptive_detect function and save results.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file\n",
    "        adaptive_detect_func: The adaptive detection function to use\n",
    "        min_hand_detection_confidence (float): Initial confidence threshold for hand detection\n",
    "        min_hand_presence_confidence (float): Initial confidence threshold for hand presence\n",
    "        min_face_detection_confidence (float): Initial confidence threshold for face detection\n",
    "        min_face_presence_confidence (float): Initial confidence threshold for face presence\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        output_face_blendshapes (bool): Whether to detect face blendshapes\n",
    "        max_attempts (int): Maximum detection attempts for adaptive detection\n",
    "        threshold_reduction_factor (float): Factor to reduce thresholds by\n",
    "        min_threshold (float): Minimum threshold limit\n",
    "        frame_step (int): Process every Nth frame (1 = all frames)\n",
    "        start_time_seconds (float): Time in seconds to start processing from\n",
    "        end_time_seconds (float): Time in seconds to end processing (None = process until end)\n",
    "        save_failure_screenshots (bool): Save screenshots for all frames with any detection failures\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the directory containing saved frame results\n",
    "    \"\"\"\n",
    "    # Extract video name for directory creation\n",
    "    video_path = Path(video_path)\n",
    "    video_name = video_path.stem  # Get filename without extension\n",
    "    \n",
    "    # Extract dominant hand information from filename\n",
    "    if video_name.endswith(\"_Right\"):\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "    elif video_name.endswith(\"_Left\"):\n",
    "        extracted_dominant_hand = \"Left\"\n",
    "    else:\n",
    "        # Default if not specified in filename\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "        print(f\"Warning: Could not determine dominant hand from filename, using default: {dominand_hand}\")\n",
    "\n",
    "    # Use the extracted dominant hand instead of the parameter\n",
    "    dominand_hand = extracted_dominant_hand\n",
    "    print(f\"Detected dominant hand from filename: {dominand_hand}\")\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(f\"{video_name}_landmarks\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create screenshots directory if screenshot option is enabled\n",
    "    screenshots_dir = None\n",
    "    if save_failure_screenshots:\n",
    "        screenshots_dir = output_dir / \"failure_screenshots\"\n",
    "        screenshots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a log file to track processing\n",
    "    log_file = output_dir / \"processing_log.txt\"\n",
    "    \n",
    "    # Create a detailed statistics file\n",
    "    stats_file = output_dir / \"detection_statistics.json\"\n",
    "    \n",
    "    # Initialize statistics tracking\n",
    "    stats = {\n",
    "        \"video_info\": {\n",
    "            \"name\": video_name,\n",
    "            \"path\": str(video_path),\n",
    "            \"total_frames\": 0,\n",
    "            \"processed_frames\": 0,\n",
    "            \"fps\": 0,\n",
    "            \"duration_seconds\": 0,\n",
    "            \"start_time\": start_time_seconds,\n",
    "            \"end_time\": end_time_seconds,\n",
    "            \"dominant_hand\": dominand_hand,\n",
    "            \"processing_started\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"processing_completed\": None\n",
    "        },\n",
    "        \"detection_rates\": {\n",
    "            \"dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"non_dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"face\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"overall\": {\n",
    "                \"all_detected\": 0,\n",
    "                \"partial_detections\": 0,\n",
    "                \"no_detections\": 0,\n",
    "                \"success_rate\": 0\n",
    "            }\n",
    "        },\n",
    "        \"failed_frames\": {\n",
    "            \"dominant_hand_failures\": [],\n",
    "            \"non_dominant_hand_failures\": [],\n",
    "            \"face_failures\": [],\n",
    "            \"all_failures\": []\n",
    "        },\n",
    "        \"processing_performance\": {\n",
    "            \"average_processing_time_ms\": 0,\n",
    "            \"total_processing_time_seconds\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(log_file, \"w\") as log:\n",
    "        log.write(f\"Processing video: {video_path}\\n\")\n",
    "        log.write(f\"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        log.write(f\"Parameters:\\n\")\n",
    "        log.write(f\"  - frame_step: {frame_step}\\n\")\n",
    "        log.write(f\"  - start_time: {start_time_seconds} seconds\\n\")\n",
    "        if end_time_seconds is not None:\n",
    "            log.write(f\"  - end_time: {end_time_seconds} seconds\\n\")\n",
    "        log.write(f\"  - dominand_hand: {dominand_hand}\\n\")\n",
    "        log.write(f\"  - num_hands: {num_hands}\\n\")\n",
    "        log.write(f\"  - detection confidence thresholds: {min_hand_detection_confidence}, {min_face_detection_confidence}\\n\")\n",
    "        log.write(\"\\n--- Frame processing log ---\\n\")\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration_seconds = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    # Update stats with video info\n",
    "    stats[\"video_info\"][\"total_frames\"] = total_frames\n",
    "    stats[\"video_info\"][\"fps\"] = fps\n",
    "    stats[\"video_info\"][\"duration_seconds\"] = duration_seconds\n",
    "    \n",
    "    # Convert time to frame indices\n",
    "    start_frame = int(max(0, start_time_seconds * fps))\n",
    "    \n",
    "    # Set end frame if specified\n",
    "    if end_time_seconds is not None:\n",
    "        end_frame = min(total_frames, int(end_time_seconds * fps))\n",
    "    else:\n",
    "        end_frame = total_frames\n",
    "    \n",
    "    print(f\"Video: {video_name}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"Duration: {duration_seconds:.2f} seconds\")\n",
    "    print(f\"Processing frames {start_frame} to {end_frame} (time {start_time_seconds:.2f}s to {end_time_seconds if end_time_seconds is not None else duration_seconds:.2f}s)\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Process frames\n",
    "    frame_idx = 0\n",
    "    processed_count = 0\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    # Skip to start_frame\n",
    "    if start_frame > 0:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        frame_idx = start_frame\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        while frame_idx < end_frame:\n",
    "            # Read the next frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "            \n",
    "            # Only process every frame_step frames\n",
    "            if (frame_idx - start_frame) % frame_step != 0:\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "                \n",
    "            # Get timestamp in milliseconds\n",
    "            timestamp_ms = int(frame_idx * 1000 / fps)\n",
    "            timestamp_formatted = f\"{timestamp_ms//60000:02d}m{(timestamp_ms//1000)%60:02d}s{timestamp_ms%1000:03d}ms\"\n",
    "            \n",
    "            # Temporary frame path\n",
    "            temp_frame_path = Path(temp_dir) / f\"temp_frame_{frame_idx}.jpg\"\n",
    "            \n",
    "            # Save the current frame as an image\n",
    "            cv2.imwrite(str(temp_frame_path), frame)\n",
    "            \n",
    "            # Process the frame with adaptive_detect\n",
    "            print(f\"Processing frame {frame_idx}/{total_frames} (timestamp: {timestamp_formatted})\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # Use adaptive_detect on the frame\n",
    "                results = adaptive_detect_func(\n",
    "                    str(temp_frame_path), hand_model_path, face_model_path,\n",
    "                    min_hand_detection_confidence=min_hand_detection_confidence,\n",
    "                    min_hand_presence_confidence=min_hand_presence_confidence,\n",
    "                    min_face_detection_confidence=min_face_detection_confidence,\n",
    "                    min_face_presence_confidence=min_face_presence_confidence,\n",
    "                    num_hands=num_hands,\n",
    "                    dominand_hand=dominand_hand,\n",
    "                    visualize=False,\n",
    "                    output_face_blendshapes=output_face_blendshapes,\n",
    "                    max_attempts=max_attempts,\n",
    "                    threshold_reduction_factor=threshold_reduction_factor,\n",
    "                    min_threshold=min_threshold\n",
    "                )\n",
    "                \n",
    "                # Calculate processing time\n",
    "                proc_time = time.time() - start_time\n",
    "                total_processing_time += proc_time\n",
    "                \n",
    "                # Unpack results\n",
    "                dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
    "                \n",
    "                # Update detection statistics\n",
    "                dom_hand_detected = detection_status[0] == 1\n",
    "                non_dom_hand_detected = detection_status[1] == 1\n",
    "                face_was_detected = face_detected == 1\n",
    "                \n",
    "                if dom_hand_detected:\n",
    "                    stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"dominant_hand\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"dominant_hand_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                if non_dom_hand_detected:\n",
    "                    stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"non_dominant_hand\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"non_dominant_hand_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                if face_was_detected:\n",
    "                    stats[\"detection_rates\"][\"face\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"face\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"face_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                # Track combined detection status\n",
    "                detection_count = dom_hand_detected + non_dom_hand_detected + face_was_detected\n",
    "                \n",
    "                if detection_count == 3:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"all_detected\"] += 1\n",
    "                elif detection_count == 0:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"no_detections\"] += 1\n",
    "                    stats[\"failed_frames\"][\"all_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"partial_detections\"] += 1\n",
    "                \n",
    "                # Save screenshot if any detection failed and screenshots are enabled\n",
    "                if save_failure_screenshots and (not dom_hand_detected or not non_dom_hand_detected or not face_was_detected):\n",
    "                    # Create a detailed failure type description for the filename\n",
    "                    failure_type = []\n",
    "                    if not dom_hand_detected:\n",
    "                        failure_type.append(\"DomHand\")\n",
    "                    if not non_dom_hand_detected:\n",
    "                        failure_type.append(\"NonDomHand\")\n",
    "                    if not face_was_detected:\n",
    "                        failure_type.append(\"Face\")\n",
    "                    \n",
    "                    failure_str = \"_\".join(failure_type)\n",
    "                    screenshot_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}_missing_{failure_str}.jpg\"\n",
    "                    screenshot_path = screenshots_dir / screenshot_filename\n",
    "                    \n",
    "                    # Copy the frame to the screenshots directory\n",
    "                    cv2.imwrite(str(screenshot_path), frame)\n",
    "                    print(f\"Saved failure screenshot: {screenshot_filename}\")\n",
    "                \n",
    "                # Create output filename with frame info\n",
    "                output_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                output_path = output_dir / output_filename\n",
    "                \n",
    "                # Save all results in a single .npz file\n",
    "                np.savez(\n",
    "                    output_path,\n",
    "                    dom_landmarks=dom_landmarks,\n",
    "                    non_dom_landmarks=non_dom_landmarks,\n",
    "                    confidence_scores=confidence_scores,\n",
    "                    interpolation_scores=interpolation_scores,\n",
    "                    detection_status=detection_status,\n",
    "                    blendshape_scores=blendshape_scores,\n",
    "                    face_detected=face_detected,\n",
    "                    nose_to_wrist_dist=nose_to_wrist_dist,\n",
    "                    frame_idx=np.array([frame_idx]),\n",
    "                    timestamp_ms=np.array([timestamp_ms])\n",
    "                )\n",
    "                \n",
    "                # Update processing log\n",
    "                detection_summary = f\"Dom: {detection_status[0]}, Non-dom: {detection_status[1]}, Face: {face_detected}\"\n",
    "                log_entry = f\"Frame {frame_idx}: {detection_summary} (proc time: {proc_time:.2f}s)\\n\"\n",
    "                \n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(log_entry)\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {frame_idx}: {e}\")\n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(f\"Error on frame {frame_idx}: {str(e)}\\n\")\n",
    "            \n",
    "            # Clean up temporary frame file\n",
    "            if temp_frame_path.exists():\n",
    "                temp_frame_path.unlink()\n",
    "                \n",
    "            frame_idx += 1\n",
    "    \n",
    "    # Close the video file\n",
    "    cap.release()\n",
    "    \n",
    "    # Update final statistics\n",
    "    stats[\"video_info\"][\"processed_frames\"] = processed_count\n",
    "    stats[\"video_info\"][\"processing_completed\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Calculate detection rates\n",
    "    if processed_count > 0:\n",
    "        stats[\"detection_rates\"][\"dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"non_dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"face\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"face\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"overall\"][\"success_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"overall\"][\"all_detected\"] / processed_count * 100\n",
    "        )\n",
    "    \n",
    "    # Calculate processing performance\n",
    "    if processed_count > 0:\n",
    "        stats[\"processing_performance\"][\"average_processing_time_ms\"] = (\n",
    "            total_processing_time / processed_count * 1000\n",
    "        )\n",
    "    stats[\"processing_performance\"][\"total_processing_time_seconds\"] = total_processing_time\n",
    "    \n",
    "    # Save statistics to JSON file\n",
    "    with open(stats_file, \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # Add summary statistics to log file\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"\\n\\n===== PROCESSING SUMMARY =====\\n\")\n",
    "        log.write(f\"Completed at: {stats['video_info']['processing_completed']}\\n\")\n",
    "        log.write(f\"Frames processed: {processed_count} from {start_frame} to {min(end_frame, frame_idx-1)}\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION RATES:\\n\")\n",
    "        log.write(f\"  Dominant hand ({dominand_hand}): {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Non-dominant hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  All parts detected: {stats['detection_rates']['overall']['success_rate']:.1f}%\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION FAILURES:\\n\")\n",
    "        log.write(f\"  Frames with dominant hand failures: {len(stats['failed_frames']['dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with non-dominant hand failures: {len(stats['failed_frames']['non_dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with face failures: {len(stats['failed_frames']['face_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with all parts missing: {len(stats['failed_frames']['all_failures'])}\\n\\n\")\n",
    "        \n",
    "        log.write(\"PERFORMANCE:\\n\")\n",
    "        log.write(f\"  Average processing time per frame: {stats['processing_performance']['average_processing_time_ms']:.2f} ms\\n\")\n",
    "        log.write(f\"  Total processing time: {stats['processing_performance']['total_processing_time_seconds']:.2f} seconds\\n\")\n",
    "    \n",
    "    print(f\"\\n===== PROCESSING SUMMARY =====\")\n",
    "    print(f\"Processed {processed_count} frames\")\n",
    "    print(f\"Detection rates: Dom hand: {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "          f\"Non-dom hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "          f\"Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\")\n",
    "    print(f\"All parts detected in {stats['detection_rates']['overall']['success_rate']:.1f}% of frames\")\n",
    "    print(f\"Full statistics saved to: {stats_file}\")\n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "    \n",
    "    return str(output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks = data['dom_landmarks']\n",
    "    non_dom_landmarks = data['non_dom_landmarks']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores = data['blendshape_scores']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist = data['nose_to_wrist_dist']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    \n",
    "    return (dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores, face_detected, \n",
    "            nose_to_wrist_dist, frame_idx, timestamp_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected dominant hand from filename: Right\n",
      "Video: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right\n",
      "Total frames: 65\n",
      "FPS: 1.0\n",
      "Duration: 65.00 seconds\n",
      "Processing frames 30 to 60 (time 30.20s to 60.40s)\n",
      "Output directory: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\n",
      "Processing frame 30/65 (timestamp: 00m30s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918003.837132    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918003.885513  322849 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918003.930412  322856 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918003.958553  322850 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918004.035883    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918004.038905  322865 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918004.040456    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918004.051836  322869 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918004.068560  322872 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918004.152673    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918004.155091  322881 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918004.174335  322883 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918004.192393  322885 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918004.274255    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918004.276401  322897 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918004.277161    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918004.283155  322902 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918004.311322  322905 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.974)\n",
      "Non-dominant hand detected: True (confidence: 0.996)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 31/65 (timestamp: 00m31s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918004.370216    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918004.372878  322913 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918004.400007  322919 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918004.419034  322921 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918004.490898    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918004.495149  322929 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918004.496044    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918004.501696  322936 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918004.522536  322934 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918004.574280    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918004.577676  322945 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918004.600587  322951 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918004.620167  322947 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.980)\n",
      "Non-dominant hand detected: True (confidence: 0.998)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 32/65 (timestamp: 00m32s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.966)\n",
      "Non-dominant hand detected: True (confidence: 0.970)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918004.695086    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918004.700063  322961 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918004.700667    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918004.708162  322964 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918004.730695  322966 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918004.774930    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918004.779315  322977 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918004.802277  322978 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918004.821064  322980 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 33/65 (timestamp: 00m33s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918004.910756    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918004.913843  322993 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918004.914578    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918004.919810  322994 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918004.957582  322997 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918005.005630    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.008449  323009 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.046078  323019 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.066799  323013 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918005.131382    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.133632  323025 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.134190    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918005.138234  323035 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.155106  323029 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering face thresholds: 0.245, 0.245\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.997)\n",
      "Non-dominant hand detected: True (confidence: 0.997)\n",
      "Face detected: False\n",
      "Total detection attempts: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918005.184322    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.186910  323041 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.206109  323046 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.223286  323048 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918005.298380    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.300203  323085 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.300610    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918005.305292  323090 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.322830  323093 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918005.377173    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.380299  323101 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.411095  323110 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.427561  323111 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000033_00m33s000ms_missing_Face.jpg\n",
      "Processing frame 34/65 (timestamp: 00m34s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: False\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=0, non-dominant hand=1, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918005.493491    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.496968  323117 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.497536    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918005.502504  323119 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.520464  323124 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918005.557895    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.562447  323133 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.588263  323134 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.606224  323143 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918005.689640    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.691782  323149 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.692790    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918005.702659  323152 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.718251  323153 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.949)\n",
      "Non-dominant hand detected: True (confidence: 0.993)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 35/65 (timestamp: 00m35s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918005.765029    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.767647  323165 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.787925  323168 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.807028  323166 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918005.888229    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.891061  323181 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.891569    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918005.900173  323184 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918005.921793  323187 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918005.965417    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918005.967936  323197 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918005.991677  323198 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.008500  323206 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.905)\n",
      "Non-dominant hand detected: True (confidence: 0.993)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 36/65 (timestamp: 00m36s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918006.074011    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.077663  323213 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.078237    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918006.085602  323215 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.105204  323224 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918006.141658    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.145453  323229 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.168111  323233 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.187931  323240 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.968)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918006.269739    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.273688  323245 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.274208    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918006.283186  323246 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.302155  323251 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918006.345087    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.348030  323261 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.369817  323262 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.385328  323263 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918006.456716    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.459068  323277 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.459863    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918006.464646  323278 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.480915  323283 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms_missing_NonDomHand.jpg\n",
      "Processing frame 37/65 (timestamp: 00m37s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918006.535630    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.538540  323293 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.563052  323294 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.581219  323301 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918006.703932    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.706083  323309 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.706676    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918006.714918  323312 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.730592  323319 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918006.746424    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.748699  323325 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.779408  323327 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.795034  323326 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.949)\n",
      "Non-dominant hand detected: True (confidence: 0.998)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 38/65 (timestamp: 00m38s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918006.860025    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.862808  323341 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.863228    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918006.867379  323348 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.884742  323343 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918006.927622    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918006.930707  323357 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918006.953009  323358 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918006.969781  323359 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.953)\n",
      "Non-dominant hand detected: True (confidence: 0.997)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 39/65 (timestamp: 00m39s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918007.063172    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.066435  323373 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.066818    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918007.071854  323374 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.087756  323376 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918007.145855    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.149663  323389 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.168340  323392 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.184382  323397 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918007.237875    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.240173  323405 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.240667    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918007.247915  323412 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.264140  323406 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918007.299619    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.302057  323421 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.329774  323430 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.350526  323427 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918007.408294    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.412255  323465 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.412709    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918007.418282  323470 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.435257  323466 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918007.466374    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.468821  323481 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.486824  323493 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.503010  323482 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.993)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms_missing_NonDomHand.jpg\n",
      "Processing frame 40/65 (timestamp: 00m40s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918007.573014    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.575308  323497 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.576172    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918007.582637  323499 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.601210  323501 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918007.652937    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.655628  323513 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.675673  323518 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.689784  323515 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918007.757359    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.760713  323529 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.761251    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918007.766448  323537 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.788504  323536 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918007.818625    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.821002  323545 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.850474  323553 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.867918  323550 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918007.940559    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.943583  323561 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918007.944022    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918007.948454  323565 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918007.964343  323562 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918007.996816    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918007.999061  323577 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918008.019735  323578 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918008.036198  323581 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.928)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms_missing_NonDomHand.jpg\n",
      "Processing frame 41/65 (timestamp: 00m41s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918008.133587    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918008.135404  323593 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918008.135789    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918008.140052  323597 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918008.166337  323599 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918008.218049    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918008.220593  323609 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918008.242486  323613 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918008.262724  323614 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918008.354679    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918008.356984  323625 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918008.357497    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918008.363117  323628 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918008.382455  323626 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918008.435481    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918008.438213  323641 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918008.466913  323643 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918008.484766  323644 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.992)\n",
      "Non-dominant hand detected: True (confidence: 0.997)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 42/65 (timestamp: 00m42s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918008.566422    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918008.569218  323657 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918008.569750    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918008.574112  323661 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918008.591801  323660 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918008.644966    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918008.649280  323673 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918008.681168  323676 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918008.700547  323675 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.991)\n",
      "Non-dominant hand detected: True (confidence: 0.990)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 43/65 (timestamp: 00m43s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918008.808365    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918008.811949  323689 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918008.813543    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918008.820904  323694 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918008.850560  323701 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918008.900760    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918008.903280  323705 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918008.925367  323713 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918008.947585  323712 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918009.021907    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.024115  323721 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.024634    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918009.030502  323724 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.049314  323723 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.990)\n",
      "Non-dominant hand detected: True (confidence: 0.992)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 44/65 (timestamp: 00m44s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.988)\n",
      "Non-dominant hand detected: True (confidence: 0.988)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 45/65 (timestamp: 00m45s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.992)\n",
      "Non-dominant hand detected: True (confidence: 0.990)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 46/65 (timestamp: 00m46s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918009.092812    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.096144  323737 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.119126  323739 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.134536  323747 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918009.191052    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.193402  323753 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.193921    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918009.199194  323760 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.223853  323754 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918009.270763    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.273050  323769 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.293081  323778 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.307704  323771 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.988)\n",
      "Non-dominant hand detected: True (confidence: 0.985)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 47/65 (timestamp: 00m47s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918009.401289    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.403108  323785 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.403628    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918009.408264  323786 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.427347  323791 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918009.474514    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.477930  323815 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.502641  323820 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.526257  323819 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918009.602930    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.605517  323844 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.606051    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918009.613833  323845 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.636407  323848 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918009.654064    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.656357  323860 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.678209  323866 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.693532  323870 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918009.756699    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.759462  323876 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.760129    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918009.769076  323885 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.785080  323877 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918009.831884    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering face thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918009.835875  323892 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.858508  323897 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.878063  323899 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918009.959602    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918009.963528  323908 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918009.963952    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918009.968827  323917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918009.986615  323911 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918010.042996    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.978)\n",
      "Non-dominant hand detected: True (confidence: 0.975)\n",
      "Face detected: False\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000047_00m47s000ms_missing_Face.jpg\n",
      "Processing frame 48/65 (timestamp: 00m48s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918010.047350  323924 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918010.070399  323930 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918010.091706  323926 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918010.185866    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918010.189668  323940 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918010.190210    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918010.198881  323946 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918010.220038  323951 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.966)\n",
      "Non-dominant hand detected: True (confidence: 0.993)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 49/65 (timestamp: 00m49s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918010.283926    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918010.286406  323956 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918010.307137  323965 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918010.330918  323959 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918010.431427    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918010.434476  323972 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918010.435188    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918010.443051  323975 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918010.458160  323981 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918010.513123    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918010.516469  323988 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918010.541846  323990 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918010.560652  323991 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.970)\n",
      "Non-dominant hand detected: True (confidence: 0.994)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 50/65 (timestamp: 00m50s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918010.627547    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918010.632297  324004 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918010.632941    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918010.640046  324008 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918010.660284  324012 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918010.712478    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918010.716357  324020 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918010.740471  324022 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918010.758415  324021 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918010.842128    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918010.845179  324036 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918010.846044    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918010.850996  324042 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918010.866102  324037 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.962)\n",
      "Non-dominant hand detected: True (confidence: 0.996)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 51/65 (timestamp: 00m51s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918010.902014    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918010.904380  324052 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918010.923579  324062 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918010.939136  324058 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918011.001838    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.004365  324068 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.004913    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918011.009824  324070 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.025177  324076 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918011.056050    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.058866  324084 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.082003  324087 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.097674  324092 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918011.184750    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.188339  324100 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.189108    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918011.199162  324102 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.215520  324101 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918011.279947    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.283767  324116 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.313136  324117 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.332411  324119 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.988)\n",
      "Non-dominant hand detected: True (confidence: 0.556)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n",
      "Processing frame 52/65 (timestamp: 00m52s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918011.390890    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.393973  324132 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.394980    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918011.400888  324138 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.417676  324141 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918011.453709    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.456423  324148 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.482784  324155 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.500855  324158 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918011.572363    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.574902  324177 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.575413    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918011.581528  324179 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.599292  324186 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.909)\n",
      "Non-dominant hand detected: True (confidence: 0.983)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 53/65 (timestamp: 00m53s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918011.642944    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.647750  324208 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.667654  324214 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.682963  324213 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918011.771729    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.774910  324224 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.775558    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918011.781892  324228 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.803202  324231 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918011.825761    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.830203  324240 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.857332  324242 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.875547  324247 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.980)\n",
      "Non-dominant hand detected: True (confidence: 0.992)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 54/65 (timestamp: 00m54s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918011.957151    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918011.960435  324256 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918011.961584    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918011.968119  324261 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918011.985864  324260 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918012.032243    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.035627  324272 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.064733  324275 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.085165  324274 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918012.150682    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.152891  324288 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.153410    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918012.158154  324291 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.174402  324289 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n",
      "Lowering face thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918012.213643    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.216236  324304 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.237884  324306 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.253393  324313 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918012.353270    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.355652  324320 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.356317    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918012.362747  324329 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.384865  324328 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918012.414456    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.417099  324336 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.435968  324340 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.451491  324339 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n",
      "Lowering face thresholds: 0.245, 0.245\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.967)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: False\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms_missing_NonDomHand_Face.jpg\n",
      "Processing frame 55/65 (timestamp: 00m55s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918012.537125    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.539289  324352 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.539783    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918012.545176  324357 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.566189  324358 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918012.629480    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.632330  324368 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.654216  324379 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.672540  324374 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.934)\n",
      "Non-dominant hand detected: True (confidence: 0.984)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 56/65 (timestamp: 00m56s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918012.760975    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.764891  324384 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.765485    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918012.769749  324388 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.787277  324385 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918012.832863    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.835257  324400 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.856647  324404 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.874649  324407 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918012.942928    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918012.946196  324416 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918012.947140    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918012.952740  324420 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918012.968212  324425 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.949)\n",
      "Non-dominant hand detected: True (confidence: 0.983)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 57/65 (timestamp: 00m57s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918013.014780    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.017579  324432 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.051072  324436 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.085056  324435 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918013.158142    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.161071  324448 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.161796    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918013.167017  324451 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.193977  324454 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918013.248173    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.955)\n",
      "Non-dominant hand detected: True (confidence: 0.972)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 58/65 (timestamp: 00m58s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918013.251218  324464 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.273442  324466 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.292199  324474 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918013.357907    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.361166  324480 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.361706    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918013.365923  324481 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.382453  324487 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918013.407576    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.410769  324496 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.431278  324500 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.444755  324497 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering face thresholds: 0.245, 0.245\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.947)\n",
      "Non-dominant hand detected: True (confidence: 0.712)\n",
      "Face detected: False\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000058_00m58s000ms_missing_Face.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918013.506415    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.509324  324512 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.509968    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918013.515344  324518 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.531556  324521 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918013.559457    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.563319  324528 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.583320  324530 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.599374  324538 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918013.665433    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.667835  324556 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.668338    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918013.673107  324562 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.693868  324559 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 59/65 (timestamp: 00m59s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: False\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=0, non-dominant hand=1, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918013.740018    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.743116  324586 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.765369  324589 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.779963  324594 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918013.831261    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.833367  324602 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.833880    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918013.838891  324606 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.854790  324610 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742918013.885339    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918013.888139  324618 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918013.908792  324619 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918013.933744  324621 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.979)\n",
      "Non-dominant hand detected: True (confidence: 0.992)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "\n",
      "===== PROCESSING SUMMARY =====\n",
      "Processed 30 frames\n",
      "Detection rates: Dom hand: 100.0%, Non-dom hand: 86.7%, Face: 86.7%\n",
      "All parts detected in 0.0% of frames\n",
      "Full statistics saved to: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/detection_statistics.json\n",
      "Results saved to: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742918014.006702    5550 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742918014.009730  324634 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742918014.010361    5550 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742918014.015836  324636 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742918014.031827  324643 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_video(video_path=video_path, adaptive_detect_func=adaptive_detect, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                 min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5,\n",
    "                 min_face_detection_confidence=0.5, min_face_presence_confidence=0.5,\n",
    "                 num_hands=2, output_face_blendshapes=True,\n",
    "                 max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2, \n",
    "                 frame_step=1, start_time_seconds=30.2, end_time_seconds=60.4,\n",
    "                 save_failure_screenshots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = load_frame_data(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[7][1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interpolation_frames(x, nums_list):\n",
    "    \"\"\"\n",
    "    Returns integers in the range [x-5, x+5] that are not equal to x\n",
    "    and are not in nums_list.\n",
    "    \n",
    "    Args:\n",
    "        x (int): The reference integer\n",
    "        nums_list (list): A list of integers\n",
    "        \n",
    "    Returns:\n",
    "        list: Integers in [x-5, x+5] excluding x and elements in nums_list\n",
    "    \"\"\"\n",
    "    # Create the set of all integers in the range [x-5, x+5]\n",
    "    all_range = set(range(x-5, x+6))  # +6 because range is exclusive at upper bound\n",
    "    \n",
    "    # Remove x itself\n",
    "    all_range.discard(x)\n",
    "    \n",
    "    # Remove numbers that are in the input list\n",
    "    result = all_range - set(nums_list)\n",
    "    \n",
    "    # Convert back to a list and return\n",
    "    return sorted(list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "def find_file_with_partial_name(partial_name, search_dir='.', recursive=False):\n",
    "    \"\"\"\n",
    "    Find files that start with the given partial name.\n",
    "    \n",
    "    Args:\n",
    "        partial_name (str): Partial file name to match\n",
    "        search_dir (str): Directory to search in (default: current directory)\n",
    "        recursive (bool): Whether to search in subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        list: Complete paths of all matching files\n",
    "    \"\"\"\n",
    "    # Create a search pattern for files starting with the partial name\n",
    "    search_pattern = os.path.join(search_dir, f\"{partial_name}*\")\n",
    "    \n",
    "    # Use recursive glob if requested\n",
    "    if recursive:\n",
    "        matches = []\n",
    "        for root, _, _ in os.walk(search_dir):\n",
    "            matches.extend(glob.glob(os.path.join(root, f\"{os.path.basename(partial_name)}*\")))\n",
    "        return matches\n",
    "    else:\n",
    "        return glob.glob(search_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers_on_both_sides(x, missing_numbers):\n",
    "    \"\"\"\n",
    "    Checks if the list of missing numbers has at least one number smaller than x\n",
    "    AND at least one number larger than x.\n",
    "    \n",
    "    Args:\n",
    "        x (int): The reference integer\n",
    "        missing_numbers (list): Output from find_missing_numbers(x, nums_list)\n",
    "        \n",
    "    Returns:\n",
    "        bool: False if all numbers are either all smaller or all larger than x.\n",
    "              True if there's at least one smaller and one larger number.\n",
    "    \"\"\"\n",
    "    has_smaller = False\n",
    "    has_larger = False\n",
    "    \n",
    "    for num in missing_numbers:\n",
    "        if num < x:\n",
    "            has_smaller = True\n",
    "        elif num > x:\n",
    "            has_larger = True\n",
    "            \n",
    "        # Early exit if we found both smaller and larger numbers\n",
    "        if has_smaller and has_larger:\n",
    "            return True\n",
    "    \n",
    "    # If we get here, we didn't find both smaller and larger numbers\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_npz_file(file_path, modifications):\n",
    "    \"\"\"\n",
    "    Load a .npz file, modify existing arrays and add new ones, then save it back.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .npz file\n",
    "        modifications (dict): Dictionary with keys as array names and values as new arrays\n",
    "                             or functions that take the original array and return a modified version\n",
    "    \"\"\"\n",
    "    # Load the npz file\n",
    "    with np.load(file_path) as data:\n",
    "        # Create a copy of all arrays\n",
    "        arrays = {name: data[name] for name in data.files}\n",
    "    \n",
    "    # Apply modifications and add new arrays\n",
    "    for name, modification in modifications.items():\n",
    "        if name in arrays:\n",
    "            if callable(modification):\n",
    "                # If the modification is a function, apply it to the original array\n",
    "                arrays[name] = modification(arrays[name])\n",
    "            else:\n",
    "                # Otherwise, replace the array\n",
    "                arrays[name] = modification\n",
    "        else:\n",
    "            # Add new array\n",
    "            arrays[name] = modification\n",
    "            print(f\"Adding new array '{name}' to the file\")\n",
    "    \n",
    "    # Save back to the file with same format\n",
    "    np.savez(file_path, **arrays)\n",
    "    \n",
    "    print(f\"Successfully modified/added {len(modifications)} arrays in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_undetected_hand_landmarks(directory_path):  \n",
    "    \"\"\"\n",
    "    Interpolate landmarks for frames where hand detection failed.\n",
    "    \"\"\"\n",
    "    print(f\"Starting interpolation for directory: {directory_path}\")\n",
    "    \n",
    "    # Load detection statistics JSON\n",
    "    with open(os.path.join(directory_path, 'detection_statistics.json')) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    first_frame_number = round(data['video_info']['fps'] * data['video_info']['start_time'])\n",
    "    final_frame_number = round(data['video_info']['fps'] * data['video_info']['end_time'])\n",
    "    \n",
    "    print(f\"Processing frames range: {first_frame_number} to {final_frame_number}\")\n",
    "    \n",
    "    # Maximum possible sum of weights for normalization (when all 10 frames are available)\n",
    "    MAX_WEIGHT_SUM = 2.92722222\n",
    "    \n",
    "    # Process non-dominant hand failures\n",
    "    print(\"Processing non-dominant hand failures...\")\n",
    "    missing_non_dominant_frame_list = [frame['frame'] for frame in data['failed_frames']['non_dominant_hand_failures']]\n",
    "    \n",
    "    non_dom_interpolated_count = 0\n",
    "    \n",
    "    for missing_frame in data['failed_frames']['non_dominant_hand_failures']:\n",
    "        frame_number = missing_frame['frame']\n",
    "        filepath = missing_frame['file']\n",
    "        \n",
    "        # Only interpolate frames not at the edges of the video\n",
    "        if (frame_number - 5) <= first_frame_number or (frame_number + 5) >= final_frame_number:\n",
    "            print(f\"Skipping frame {frame_number} - too close to video boundary\")\n",
    "            continue\n",
    "        \n",
    "        # Find frames with valid detections for interpolation\n",
    "        interpolation_frames = find_interpolation_frames(frame_number, missing_non_dominant_frame_list)\n",
    "        \n",
    "        if not interpolation_frames:\n",
    "            print(f\"No valid frames found for interpolating frame {frame_number}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate interpolated landmarks\n",
    "        interpolation_weights_sum = 0\n",
    "        interpolated_coordinates = np.zeros(shape=(20, 3))\n",
    "        interpolated_wrist_to_nose = np.zeros(2)\n",
    "        \n",
    "        for interp_frame in interpolation_frames:\n",
    "            weight = 1 / ((frame_number - interp_frame) ** 2)\n",
    "            interpolation_weights_sum += weight\n",
    "            \n",
    "            # Find and load the reference frame\n",
    "            interp_partial_filename = data['video_info']['name'] + f\"_frame{interp_frame:06d}\"\n",
    "            try:\n",
    "                interp_files = find_file_with_partial_name(\n",
    "                    interp_partial_filename, \n",
    "                    search_dir=directory_path, \n",
    "                    recursive=False\n",
    "                )\n",
    "                \n",
    "                if not interp_files:\n",
    "                    print(f\"Warning: Could not find file for frame {interp_frame}\")\n",
    "                    continue\n",
    "                    \n",
    "                interp_filepath = interp_files[0]\n",
    "                \n",
    "                # Load the frame data - index 1 for non-dominant hand landmarks\n",
    "                frame_data = load_frame_data(interp_filepath)\n",
    "                non_dom_landmarks = frame_data[1]  # Correct index for non-dominant hand\n",
    "                nose_to_wrist_non_dom = frame_data[7][1, :]\n",
    "                \n",
    "                \n",
    "                # Add weighted contribution\n",
    "                interpolated_coordinates += weight * non_dom_landmarks\n",
    "                interpolated_wrist_to_nose += weight * nose_to_wrist_non_dom\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {interp_frame}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize by sum of weights (crucial step!)\n",
    "        if interpolation_weights_sum > 0:\n",
    "            interpolated_coordinates /= interpolation_weights_sum\n",
    "            interpolated_wrist_to_nose /= interpolation_weights_sum\n",
    "            \n",
    "            # Calculate confidence based on weights and frame distribution\n",
    "            has_frames_on_both_sides = has_numbers_on_both_sides(frame_number, interpolation_frames)\n",
    "            \n",
    "            if has_frames_on_both_sides:\n",
    "                interpolation_confidence = interpolation_weights_sum / MAX_WEIGHT_SUM\n",
    "            else:\n",
    "                interpolation_confidence = (interpolation_weights_sum / MAX_WEIGHT_SUM) * 0.8\n",
    "                \n",
    "            print(f\"Frame {frame_number}: Interpolated with confidence {interpolation_confidence:.2f}\")\n",
    "            \n",
    "            # Update the file with interpolated data\n",
    "            def update_interp_scores(arr):\n",
    "                new_arr = arr.copy()\n",
    "                new_arr[1] = interpolation_confidence  # Index 1 for non-dominant hand\n",
    "                return new_arr\n",
    "            \n",
    "            def update_nose_to_wrist_scores(matrix):\n",
    "                new_matrix = matrix.copy()\n",
    "                new_matrix[1, :] = interpolated_wrist_to_nose\n",
    "                return new_matrix\n",
    "                \n",
    "            modifications = {\n",
    "                'non_dom_landmarks': interpolated_coordinates,\n",
    "                'interpolation_scores': update_interp_scores,\n",
    "                'nose_to_wrist_dist': update_nose_to_wrist_scores\n",
    "            }\n",
    "            \n",
    "            modify_npz_file(\n",
    "                file_path=os.path.join(directory_path, filepath),\n",
    "                modifications=modifications\n",
    "            )\n",
    "            \n",
    "            non_dom_interpolated_count += 1\n",
    "    \n",
    "    # Process dominant hand failures\n",
    "    print(f\"Interpolated {non_dom_interpolated_count} non-dominant hand frames\")\n",
    "    print(\"Processing dominant hand failures...\")\n",
    "    \n",
    "    missing_dominant_frame_list = [frame['frame'] for frame in data['failed_frames']['dominant_hand_failures']]\n",
    "    \n",
    "    dom_interpolated_count = 0\n",
    "    \n",
    "    for missing_frame in data['failed_frames']['dominant_hand_failures']:\n",
    "        frame_number = missing_frame['frame']\n",
    "        filepath = missing_frame['file']\n",
    "        \n",
    "        # Only interpolate frames not at the edges of the video\n",
    "        if (frame_number - 5) <= first_frame_number or (frame_number + 5) >= final_frame_number:\n",
    "            continue\n",
    "        \n",
    "        # Find frames with valid detections for interpolation\n",
    "        interpolation_frames = find_interpolation_frames(frame_number, missing_dominant_frame_list)\n",
    "        \n",
    "        if not interpolation_frames:\n",
    "            continue\n",
    "        \n",
    "        # Calculate interpolated landmarks\n",
    "        interpolation_weights_sum = 0\n",
    "        interpolated_coordinates = np.zeros(shape=(20, 3))\n",
    "        interpolated_wrist_to_nose = np.zeros(2)\n",
    "        \n",
    "        for interp_frame in interpolation_frames:\n",
    "            weight = 1 / ((frame_number - interp_frame) ** 2)\n",
    "            interpolation_weights_sum += weight\n",
    "            \n",
    "            # Find and load the reference frame\n",
    "            interp_partial_filename = data['video_info']['name'] + f\"_frame{interp_frame:06d}\"\n",
    "            try:\n",
    "                interp_files = find_file_with_partial_name(\n",
    "                    interp_partial_filename, \n",
    "                    search_dir=directory_path, \n",
    "                    recursive=False\n",
    "                )\n",
    "                \n",
    "                if not interp_files:\n",
    "                    continue\n",
    "                    \n",
    "                interp_filepath = interp_files[0]\n",
    "                \n",
    "                # Load the frame data - index 0 for dominant hand landmarks\n",
    "                frame_data = load_frame_data(interp_filepath)\n",
    "                dom_landmarks = frame_data[0]  # Correct index for dominant hand\n",
    "                nose_to_wrist_dom = frame_data[7][0, :]\n",
    "                \n",
    "                # Add weighted contribution\n",
    "                interpolated_coordinates += weight * dom_landmarks\n",
    "                interpolated_wrist_to_nose += weight * nose_to_wrist_non_dom\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {interp_frame}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize by sum of weights\n",
    "        if interpolation_weights_sum > 0:\n",
    "            interpolated_coordinates /= interpolation_weights_sum\n",
    "            interpolated_wrist_to_nose /= interpolation_weights_sum\n",
    "            # Calculate confidence based on weights and frame distribution\n",
    "            has_frames_on_both_sides = has_numbers_on_both_sides(frame_number, interpolation_frames)\n",
    "            \n",
    "            if has_frames_on_both_sides:\n",
    "                interpolation_confidence = interpolation_weights_sum / MAX_WEIGHT_SUM\n",
    "            else:\n",
    "                interpolation_confidence = (interpolation_weights_sum / MAX_WEIGHT_SUM) * 0.8\n",
    "            \n",
    "            # Update the file with interpolated data\n",
    "            def update_interp_scores(arr):\n",
    "                new_arr = arr.copy()\n",
    "                new_arr[0] = interpolation_confidence  # Index 0 for dominant hand\n",
    "                return new_arr\n",
    "            \n",
    "            def update_nose_to_wrist_scores(matrix):\n",
    "                new_matrix = matrix.copy()\n",
    "                new_matrix[0, :] = interpolated_wrist_to_nose\n",
    "                return new_matrix\n",
    "                \n",
    "            modifications = {\n",
    "                'dom_landmarks': interpolated_coordinates,\n",
    "                'interpolation_scores': update_interp_scores,\n",
    "                'nose_to_wrist_dist': update_nose_to_wrist_scores\n",
    "            }\n",
    "            \n",
    "\n",
    "            modify_npz_file(\n",
    "                file_path=os.path.join(directory_path, filepath),\n",
    "                modifications=modifications\n",
    "            )\n",
    "            \n",
    "            dom_interpolated_count += 1\n",
    "    \n",
    "    print(f\"Interpolated {dom_interpolated_count} dominant hand frames\")\n",
    "    print(f\"Total interpolated: {non_dom_interpolated_count + dom_interpolated_count} frames\")\n",
    "    \n",
    "    return non_dom_interpolated_count + dom_interpolated_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting interpolation for directory: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\n",
      "Processing frames range: 30 to 60\n",
      "Processing non-dominant hand failures...\n",
      "Frame 36: Interpolated with confidence 0.94\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\n",
      "Frame 39: Interpolated with confidence 0.62\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\n",
      "Frame 40: Interpolated with confidence 0.64\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms.npz\n",
      "Frame 54: Interpolated with confidence 1.00\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms.npz\n",
      "Interpolated 4 non-dominant hand frames\n",
      "Processing dominant hand failures...\n",
      "Interpolated 0 dominant hand frames\n",
      "Total interpolated: 4 frames\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolate_undetected_hand_landmarks(directory_path=\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = load_frame_data(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_valid_detection(frame_data, is_dominant_hand=None):\n",
    "    \"\"\"\n",
    "    Check if the frame has valid detection (not interpolated) for a specific hand.\n",
    "    \n",
    "    Args:\n",
    "        frame_data: The loaded frame data\n",
    "        is_dominant_hand: If True, check dominant hand; if False, check non-dominant hand;\n",
    "                         if None, check if either hand is detected\n",
    "    \n",
    "    Returns:\n",
    "        bool: Whether the specified hand(s) is/are detected\n",
    "    \"\"\"\n",
    "    detection_status = frame_data[4]\n",
    "    \n",
    "    if is_dominant_hand is None:\n",
    "        # Original behavior - check if either hand is detected\n",
    "        return detection_status[0] == 1 or detection_status[1] == 1\n",
    "    elif is_dominant_hand:\n",
    "        # Check specifically for dominant hand\n",
    "        return detection_status[0] == 1\n",
    "    else:\n",
    "        # Check specifically for non-dominant hand\n",
    "        return detection_status[1] == 1\n",
    "    \n",
    "\n",
    "\n",
    "def has_value(frame_data):\n",
    "    \"\"\"Check if the frame exists and has any value (detection or interpolation)\"\"\"\n",
    "    return frame_data is not None\n",
    "\n",
    "def cartesian_to_spherical(velocities):\n",
    "    \"\"\"\n",
    "    Convert Cartesian velocities (ux, uy, uz) to spherical coordinate features.\n",
    "    \n",
    "    Args:\n",
    "        velocities: NumPy array of shape (20, 3) with Cartesian velocities\n",
    "        \n",
    "    Returns:\n",
    "        NumPy array of shape (20, 5) with spherical features:\n",
    "            [vmagnitude, ϕsin, ϕcos, θsin, θcos]\n",
    "    \"\"\"\n",
    "    num_landmarks = velocities.shape[0]\n",
    "    spherical_features = np.zeros((num_landmarks, 5))\n",
    "    \n",
    "    for i in range(num_landmarks):\n",
    "        ux, uy, uz = velocities[i]\n",
    "        \n",
    "        # Calculate velocity magnitude\n",
    "        vmagnitude = np.sqrt(ux**2 + uy**2 + uz**2)\n",
    "        spherical_features[i, 0] = vmagnitude\n",
    "        \n",
    "        # Handle edge cases to avoid division by zero\n",
    "        if vmagnitude == 0:\n",
    "            # If velocity is zero, set all angles to zero\n",
    "            spherical_features[i, 1:] = 0\n",
    "            continue\n",
    "        \n",
    "        # Calculate azimuth angle (ϕ)\n",
    "        phi = np.arctan2(uy, ux)\n",
    "        spherical_features[i, 1] = np.sin(phi)  # ϕsin\n",
    "        spherical_features[i, 2] = np.cos(phi)  # ϕcos\n",
    "        \n",
    "        # Calculate elevation angle (θ)\n",
    "        # Clamp uz/vmagnitude to range [-1, 1] to avoid numerical errors\n",
    "        cos_theta = np.clip(uz / vmagnitude, -1.0, 1.0)\n",
    "        theta = np.arccos(cos_theta)\n",
    "        spherical_features[i, 3] = np.sin(theta)  # θsin\n",
    "        spherical_features[i, 4] = cos_theta      # θcos (already calculated)\n",
    "    \n",
    "    return spherical_features\n",
    "\n",
    "def compute_landmark_velocities(directory_path):\n",
    "    \"\"\"\n",
    "    Compute velocity features for hand landmarks using central differencing with two window sizes,\n",
    "    and convert to spherical coordinates.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing frame NPZ files\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of frames processed\n",
    "    \"\"\"\n",
    "    # List all NPZ files in the directory\n",
    "    npz_files = sorted(glob.glob(os.path.join(directory_path, \"*.npz\")))\n",
    "    \n",
    "    # Skip if no files found\n",
    "    if not npz_files:\n",
    "        print(f\"No NPZ files found in {directory_path}\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Computing velocities for {len(npz_files)} files...\")\n",
    "    \n",
    "    # Create a mapping of frame indices to file paths\n",
    "    frame_to_file = {}\n",
    "    for file_path in npz_files:\n",
    "        frame_data = load_frame_data(file_path)\n",
    "        frame_idx = frame_data[8]  # Index for frame_idx\n",
    "        frame_to_file[frame_idx] = file_path\n",
    "    \n",
    "    frame_indices = sorted(frame_to_file.keys())\n",
    "    processed_count = 0\n",
    "    \n",
    "\n",
    "    min_frame = min(frame_indices)\n",
    "    max_frame = max(frame_indices)\n",
    "    safe_margin = 5  # Skip processing frames within 5 frames of the edge\n",
    "    \n",
    "    # Process each frame\n",
    "    for i, curr_idx in enumerate(frame_indices):\n",
    "        if curr_idx < min_frame + safe_margin or curr_idx > max_frame - safe_margin:\n",
    "            dom_velocity_small = np.zeros((20, 5))\n",
    "            dom_velocity_large = np.zeros((20, 5))\n",
    "            non_dom_velocity_small = np.zeros((20, 5))\n",
    "            non_dom_velocity_large = np.zeros((20, 5))\n",
    "            \n",
    "            # Create zero confidence arrays\n",
    "            velocity_confidence = np.zeros(2)\n",
    "            velocity_calculation_confidence = np.zeros(2)\n",
    "            \n",
    "            # Save these zero arrays\n",
    "            modifications = {\n",
    "                'dom_velocity_small': dom_velocity_small,\n",
    "                'dom_velocity_large': dom_velocity_large,\n",
    "                'non_dom_velocity_small': non_dom_velocity_small,\n",
    "                'non_dom_velocity_large': non_dom_velocity_large,\n",
    "                'velocity_confidence': velocity_confidence,\n",
    "                'velocity_calculation_confidence': velocity_calculation_confidence\n",
    "            }\n",
    "            \n",
    "            # Get the file path for this frame\n",
    "            current_file_path = frame_to_file[curr_idx]\n",
    "            modify_npz_file(current_file_path, modifications)\n",
    "            processed_count += 1\n",
    "            \n",
    "            # Log that we're skipping calculation\n",
    "            print(f\"Frame {curr_idx} too close to video boundary - setting zero velocities\")\n",
    "            continue\n",
    "        # Load current frame\n",
    "        current_file_path = frame_to_file[curr_idx]\n",
    "        curr_frame_data = load_frame_data(current_file_path)\n",
    "        \n",
    "        # Store needed frames in a dictionary for easy access\n",
    "        frame_cache = {curr_idx: curr_frame_data}\n",
    "        \n",
    "        # Load all potentially needed frames in the -5 to +5 range\n",
    "        for offset in range(-5, 6):\n",
    "            if offset == 0:  # Skip current frame (already loaded)\n",
    "                continue\n",
    "            \n",
    "            check_idx = curr_idx + offset\n",
    "            if check_idx in frame_to_file:\n",
    "                frame_cache[check_idx] = load_frame_data(frame_to_file[check_idx])\n",
    "            else:\n",
    "                frame_cache[check_idx] = None  # Mark as not available\n",
    "        \n",
    "        # Extract dominant and non-dominant hand landmarks from current frame\n",
    "        dom_landmarks = curr_frame_data[0]\n",
    "        non_dom_landmarks = curr_frame_data[1]\n",
    "        \n",
    "        # Initialize velocity arrays in Cartesian coordinates\n",
    "        dom_velocity_small_cart = np.zeros_like(dom_landmarks)\n",
    "        dom_velocity_large_cart = np.zeros_like(dom_landmarks)\n",
    "        non_dom_velocity_small_cart = np.zeros_like(non_dom_landmarks)\n",
    "        non_dom_velocity_large_cart = np.zeros_like(non_dom_landmarks)\n",
    "        \n",
    "        # Initialize confidence and method weight tracking\n",
    "        dom_small_conf = 0.0\n",
    "        dom_large_conf = 0.0\n",
    "        non_dom_small_conf = 0.0\n",
    "        non_dom_large_conf = 0.0\n",
    "        \n",
    "        dom_small_method_weight = 0.0\n",
    "        dom_large_method_weight = 0.0\n",
    "        non_dom_small_method_weight = 0.0\n",
    "        non_dom_large_method_weight = 0.0\n",
    "        \n",
    "        dom_small_source_quality = 0.0\n",
    "        dom_large_source_quality = 0.0\n",
    "        non_dom_small_source_quality = 0.0\n",
    "        non_dom_large_source_quality = 0.0\n",
    "        \n",
    "        # ===== DOMINANT HAND VELOCITY CALCULATION =====\n",
    "        \n",
    "        # Small window [-1, +1] velocity with fallbacks\n",
    "        if (curr_idx + 1 in frame_cache and frame_cache[curr_idx + 1] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 1], True) and \n",
    "            curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "            has_value(frame_cache[curr_idx - 1])):\n",
    "            # Ideal case: (t+1, t-1)\n",
    "            dom_velocity_small_cart = (frame_cache[curr_idx + 1][0] - frame_cache[curr_idx - 1][0]) / 2.0\n",
    "            dom_small_conf = frame_cache[curr_idx + 1][2][0]  # Detection confidence of t+1 frame\n",
    "            dom_small_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor (average of interpolation confidences)\n",
    "            t_plus_1_interp = frame_cache[curr_idx + 1][3][0]\n",
    "            t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "            dom_small_source_quality = (t_plus_1_interp + t_minus_1_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "              is_valid_detection(frame_cache[curr_idx + 2], True) and \n",
    "              curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "              has_value(frame_cache[curr_idx - 2])):\n",
    "            # Fallback 1: (t+2, t-2)\n",
    "            dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - frame_cache[curr_idx - 2][0]) / 4.0\n",
    "            dom_small_conf = frame_cache[curr_idx + 2][2][0]  # Detection confidence of t+2 frame\n",
    "            dom_small_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "            t_minus_2_interp = frame_cache[curr_idx - 2][3][0]\n",
    "            dom_small_source_quality = (t_plus_2_interp + t_minus_2_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "              is_valid_detection(frame_cache[curr_idx + 2], True)):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1])):\n",
    "                # Fallback 2: (t+2, t-1)\n",
    "                dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - frame_cache[curr_idx - 1][0]) / 3.0\n",
    "                dom_small_conf = frame_cache[curr_idx + 2][2][0]  # Detection confidence of t+2 frame\n",
    "                dom_small_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "                dom_small_source_quality = (t_plus_2_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif is_valid_detection(curr_frame_data, True):\n",
    "                # Fallback 3: (t+2, t)\n",
    "                dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - curr_frame_data[0]) / 2.0\n",
    "                dom_small_conf = min(frame_cache[curr_idx + 2][2][0], curr_frame_data[2][0])\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                dom_small_source_quality = (t_plus_2_interp + t_interp) / 2.0\n",
    "                \n",
    "        elif is_valid_detection(curr_frame_data, True):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1])):\n",
    "                # Fallback 4: (t, t-1)\n",
    "                dom_velocity_small_cart = (curr_frame_data[0] - frame_cache[curr_idx - 1][0])\n",
    "                dom_small_conf = curr_frame_data[2][0]  # Detection confidence of current frame\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "                dom_small_source_quality = (t_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "                  has_value(frame_cache[curr_idx - 2])):\n",
    "                # Fallback 5: (t, t-2)\n",
    "                dom_velocity_small_cart = (curr_frame_data[0] - frame_cache[curr_idx - 2][0]) / 2.0\n",
    "                dom_small_conf = curr_frame_data[2][0]  # Detection confidence of current frame\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                t_minus_2_interp = frame_cache[curr_idx - 2][3][0]\n",
    "                dom_small_source_quality = (t_interp + t_minus_2_interp) / 2.0\n",
    "        \n",
    "        # Large window [-5, +5] velocity with fallbacks\n",
    "        # (Similar implementation to the small window, with appropriate scaling)\n",
    "        # ... (Code implementation for large window velocity calculation)\n",
    "        if (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], True) and \n",
    "            curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "            has_value(frame_cache[curr_idx - 5])):\n",
    "            # Ideal case: (t+5, t-5)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 5][0]) / 10.0\n",
    "            dom_large_conf = frame_cache[curr_idx + 5][2][0]  # Detection confidence of t+5 frame\n",
    "            dom_large_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "            t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "            dom_large_source_quality = (t_plus_5_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "        is_valid_detection(frame_cache[curr_idx + 4], True) and \n",
    "        curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "        has_value(frame_cache[curr_idx - 4])):\n",
    "        # Fallback 1: (t+4, t-4)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 4][0]) / 8.0\n",
    "            dom_large_conf = frame_cache[curr_idx + 4][2][0]  # Detection confidence of t+4 frame\n",
    "            dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "            dom_large_source_quality = (t_plus_4_interp + t_minus_4_interp) / 2.0\n",
    "    \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], True) and \n",
    "            curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "            has_value(frame_cache[curr_idx - 3])):\n",
    "            # Fallback 2: (t+3, t-3)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 3][0]) / 6.0\n",
    "            dom_large_conf = frame_cache[curr_idx + 3][2][0]  # Detection confidence of t+3 frame\n",
    "            dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "            t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "            dom_large_source_quality = (t_plus_3_interp + t_minus_3_interp) / 2.0\n",
    "            \n",
    "        # Asymmetric fallbacks for large window\n",
    "        elif (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], True)):\n",
    "            if (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4])):\n",
    "                # Fallback 3: (t+5, t-4)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 4][0]) / 9.0\n",
    "                dom_large_conf = frame_cache[curr_idx + 5][2][0]  # Detection confidence of t+5 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "                dom_large_source_quality = (t_plus_5_interp + t_minus_4_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3])):\n",
    "                # Fallback 4: (t+5, t-3)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 3][0]) / 8.0\n",
    "                dom_large_conf = frame_cache[curr_idx + 5][2][0]  # Detection confidence of t+5 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "                dom_large_source_quality = (t_plus_5_interp + t_minus_3_interp) / 2.0\n",
    "        \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], True)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5])):\n",
    "                # Fallback 5: (t+4, t-5)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 5][0]) / 9.0\n",
    "                dom_large_conf = frame_cache[curr_idx + 4][2][0]  # Detection confidence of t+4 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "                dom_large_source_quality = (t_plus_4_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3])):\n",
    "                # Fallback 6: (t+4, t-3)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 3][0]) / 7.0\n",
    "                dom_large_conf = frame_cache[curr_idx + 4][2][0]  # Detection confidence of t+4 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "                dom_large_source_quality = (t_plus_4_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], True)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5])):\n",
    "                # Fallback 7: (t+3, t-5)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 5][0]) / 8.0\n",
    "                dom_large_conf = frame_cache[curr_idx + 3][2][0]  # Detection confidence of t+3 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "                dom_large_source_quality = (t_plus_3_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4])):\n",
    "                # Fallback 8: (t+3, t-4)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 4][0]) / 7.0\n",
    "                dom_large_conf = frame_cache[curr_idx + 3][2][0]  # Detection confidence of t+3 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "                dom_large_source_quality = (t_plus_3_interp + t_minus_4_interp) / 2.0\n",
    "\n",
    "        # ===== NON-DOMINANT HAND VELOCITY CALCULATION =====\n",
    "\n",
    "    # Small window [-1, +1] velocity with fallbacks for non-dominant hand\n",
    "        if (curr_idx + 1 in frame_cache and frame_cache[curr_idx + 1] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 1], False) and \n",
    "            curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "            has_value(frame_cache[curr_idx - 1])):\n",
    "            # Ideal case: (t+1, t-1)\n",
    "            non_dom_velocity_small_cart = (frame_cache[curr_idx + 1][1] - frame_cache[curr_idx - 1][1]) / 2.0\n",
    "            non_dom_small_conf = frame_cache[curr_idx + 1][2][1]  # Detection confidence of t+1 frame\n",
    "            non_dom_small_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_1_interp = frame_cache[curr_idx + 1][3][1]\n",
    "            t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "            non_dom_small_source_quality = (t_plus_1_interp + t_minus_1_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 2], False) and \n",
    "            curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "            has_value(frame_cache[curr_idx - 2])):\n",
    "            # Fallback 1: (t+2, t-2)\n",
    "            non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - frame_cache[curr_idx - 2][1]) / 4.0\n",
    "            non_dom_small_conf = frame_cache[curr_idx + 2][2][1]  # Detection confidence of t+2 frame\n",
    "            non_dom_small_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "            t_minus_2_interp = frame_cache[curr_idx - 2][3][1]\n",
    "            non_dom_small_source_quality = (t_plus_2_interp + t_minus_2_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 2], False)):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1])):\n",
    "                # Fallback 2: (t+2, t-1)\n",
    "                non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - frame_cache[curr_idx - 1][1]) / 3.0\n",
    "                non_dom_small_conf = frame_cache[curr_idx + 2][2][1]  # Detection confidence of t+2 frame\n",
    "                non_dom_small_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "                non_dom_small_source_quality = (t_plus_2_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif is_valid_detection(curr_frame_data, False):\n",
    "                # Fallback 3: (t+2, t)\n",
    "                non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - curr_frame_data[1]) / 2.0\n",
    "                non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], curr_frame_data[2][1])\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                non_dom_small_source_quality = (t_plus_2_interp + t_interp) / 2.0\n",
    "                \n",
    "        elif is_valid_detection(curr_frame_data, False):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1])):\n",
    "                # Fallback 4: (t, t-1)\n",
    "                non_dom_velocity_small_cart = (curr_frame_data[1] - frame_cache[curr_idx - 1][1])\n",
    "                non_dom_small_conf = curr_frame_data[2][1]  # Detection confidence of current frame\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "                non_dom_small_source_quality = (t_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "                has_value(frame_cache[curr_idx - 2])):\n",
    "                # Fallback 5: (t, t-2)\n",
    "                non_dom_velocity_small_cart = (curr_frame_data[1] - frame_cache[curr_idx - 2][1]) / 2.0\n",
    "                non_dom_small_conf = curr_frame_data[2][1]  # Detection confidence of current frame\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                t_minus_2_interp = frame_cache[curr_idx - 2][3][1]\n",
    "                non_dom_small_source_quality = (t_interp + t_minus_2_interp) / 2.0\n",
    "    \n",
    "        # Large window [-5, +5] velocity with fallbacks for non-dominant hand\n",
    "        if (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], False) and \n",
    "            curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "            has_value(frame_cache[curr_idx - 5])):\n",
    "            # Ideal case: (t+5, t-5)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 5][1]) / 10.0\n",
    "            non_dom_large_conf = frame_cache[curr_idx + 5][2][1]  # Detection confidence of t+5 frame\n",
    "            non_dom_large_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "            t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_5_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], False) and \n",
    "            curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "            has_value(frame_cache[curr_idx - 4])):\n",
    "            # Fallback 1: (t+4, t-4)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 4][1]) / 8.0\n",
    "            non_dom_large_conf = frame_cache[curr_idx + 4][2][1]  # Detection confidence of t+4 frame\n",
    "            non_dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_4_interp + t_minus_4_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], False) and \n",
    "            curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "            has_value(frame_cache[curr_idx - 3])):\n",
    "            # Fallback 2: (t+3, t-3)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 3][1]) / 6.0\n",
    "            non_dom_large_conf = frame_cache[curr_idx + 3][2][1]  # Detection confidence of t+3 frame\n",
    "            non_dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "            t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_3_interp + t_minus_3_interp) / 2.0\n",
    "            \n",
    "        # Asymmetric fallbacks for large window\n",
    "        elif (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], False)):\n",
    "            if (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4])):\n",
    "                # Fallback 3: (t+5, t-4)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 4][1]) / 9.0\n",
    "                non_dom_large_conf = frame_cache[curr_idx + 5][2][1]  # Detection confidence of t+5 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_5_interp + t_minus_4_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3])):\n",
    "                # Fallback 4: (t+5, t-3)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 3][1]) / 8.0\n",
    "                non_dom_large_conf = frame_cache[curr_idx + 5][2][1]  # Detection confidence of t+5 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_5_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], False)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5])):\n",
    "                # Fallback 5: (t+4, t-5)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 5][1]) / 9.0\n",
    "                non_dom_large_conf = frame_cache[curr_idx + 4][2][1]  # Detection confidence of t+4 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_4_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3])):\n",
    "                # Fallback 6: (t+4, t-3)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 3][1]) / 7.0\n",
    "                non_dom_large_conf = frame_cache[curr_idx + 4][2][1]  # Detection confidence of t+4 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_4_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], False)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5])):\n",
    "                # Fallback 7: (t+3, t-5)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 5][1]) / 8.0\n",
    "                non_dom_large_conf = frame_cache[curr_idx + 3][2][1]  # Detection confidence of t+3 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_3_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "            has_value(frame_cache[curr_idx - 4])):\n",
    "            # Fallback 8: (t+3, t-4)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 4][1]) / 7.0\n",
    "            non_dom_large_conf = frame_cache[curr_idx + 3][2][1]  # Detection confidence of t+3 frame\n",
    "            non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_3_interp + t_minus_4_interp) / 2.0\n",
    "            \n",
    "\n",
    "        \n",
    "        # Convert Cartesian velocities to spherical coordinates\n",
    "        dom_velocity_small = cartesian_to_spherical(dom_velocity_small_cart)\n",
    "        dom_velocity_large = cartesian_to_spherical(dom_velocity_large_cart)\n",
    "        non_dom_velocity_small = cartesian_to_spherical(non_dom_velocity_small_cart)\n",
    "        non_dom_velocity_large = cartesian_to_spherical(non_dom_velocity_large_cart)\n",
    "        \n",
    "        # Calculate average confidence for each hand across both windows\n",
    "        dom_avg_conf = (dom_small_conf + dom_large_conf) / 2.0\n",
    "        non_dom_avg_conf = (non_dom_small_conf + non_dom_large_conf) / 2.0\n",
    "        \n",
    "        # Calculate velocityCalculationConfidence using method weight and source quality\n",
    "        dom_small_vel_calc_conf = dom_small_method_weight * dom_small_source_quality\n",
    "        dom_large_vel_calc_conf = dom_large_method_weight * dom_large_source_quality\n",
    "        non_dom_small_vel_calc_conf = non_dom_small_method_weight * non_dom_small_source_quality\n",
    "        non_dom_large_vel_calc_conf = non_dom_large_method_weight * non_dom_large_source_quality\n",
    "        \n",
    "        # Average across windows for each hand\n",
    "        dom_vel_calc_conf = (dom_small_vel_calc_conf + dom_large_vel_calc_conf) / 2.0\n",
    "        non_dom_vel_calc_conf = (non_dom_small_vel_calc_conf + non_dom_large_vel_calc_conf) / 2.0\n",
    "        \n",
    "        # Prepare arrays\n",
    "        velocity_confidence = np.array([dom_avg_conf, non_dom_avg_conf])\n",
    "        velocity_calculation_confidence = np.array([dom_vel_calc_conf, non_dom_vel_calc_conf])\n",
    "        \n",
    "        # Save back to the NPZ file\n",
    "        modifications = {\n",
    "            'dom_velocity_small': dom_velocity_small,\n",
    "            'dom_velocity_large': dom_velocity_large,\n",
    "            'non_dom_velocity_small': non_dom_velocity_small,\n",
    "            'non_dom_velocity_large': non_dom_velocity_large,\n",
    "            'velocity_confidence': velocity_confidence,\n",
    "            'velocity_calculation_confidence': velocity_calculation_confidence\n",
    "        }\n",
    "        \n",
    "        modify_npz_file(current_file_path, modifications)\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 100 == 0 or i == len(frame_indices) - 1:\n",
    "            print(f\"Processed {i+1}/{len(frame_indices)} frames\")\n",
    "    \n",
    "    print(f\"Velocity computation complete. Processed {processed_count} frames.\")\n",
    "    return processed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.80464620e-01, -1.35821405e+00, -2.05060444e-03],\n",
       "        [ 7.60461811e-01, -2.28448913e+00, -8.07352830e-03],\n",
       "        [ 1.07784216e+00, -3.20284678e+00, -1.48512684e-02],\n",
       "        [ 1.11755885e+00, -3.95331839e+00, -2.20604353e-02],\n",
       "        [ 1.54198540e+00, -1.70081671e+00, -1.11612026e-02],\n",
       "        [ 2.32037256e+00, -2.02307761e+00, -1.90253891e-02],\n",
       "        [ 2.77555927e+00, -2.18607764e+00, -2.42254976e-02],\n",
       "        [ 3.10574441e+00, -2.24446425e+00, -2.68370043e-02],\n",
       "        [ 1.66682741e+00, -9.26352808e-01, -1.34856179e-02],\n",
       "        [ 2.51360365e+00, -9.92989577e-01, -1.63141210e-02],\n",
       "        [ 3.04343182e+00, -1.07903945e+00, -1.75543204e-02],\n",
       "        [ 3.42619300e+00, -1.07595485e+00, -1.86035652e-02],\n",
       "        [ 1.63099306e+00, -1.57934023e-01, -1.53115885e-02],\n",
       "        [ 2.16252309e+00, -3.44873163e-01, -1.62288360e-02],\n",
       "        [ 1.96919110e+00, -5.21400734e-01, -1.37625951e-02],\n",
       "        [ 1.69019105e+00, -5.81157821e-01, -1.13554159e-02],\n",
       "        [ 1.50395692e+00,  4.92684713e-01, -1.76404007e-02],\n",
       "        [ 1.81254573e+00,  2.03747653e-01, -1.62452683e-02],\n",
       "        [ 1.63865140e+00,  3.54211529e-02, -1.10736676e-02],\n",
       "        [ 1.40328721e+00,  5.11564631e-02, -6.40913704e-03]]),\n",
       " array([[-0.0507195 , -0.78922713, -0.0056356 ],\n",
       "        [-0.16229326, -1.66259728, -0.01123114],\n",
       "        [-0.18342772, -2.22638297, -0.01607217],\n",
       "        [-0.09932128, -2.50721372, -0.0213499 ],\n",
       "        [-0.20725507, -1.73661518, -0.01741128],\n",
       "        [-0.61886666, -2.29616412, -0.02567229],\n",
       "        [-0.82674491, -2.44265324, -0.03043887],\n",
       "        [-0.96190654, -2.57975991, -0.0333345 ],\n",
       "        [-0.3402211 , -1.42531477, -0.01871206],\n",
       "        [-0.87543982, -1.94450874, -0.02655811],\n",
       "        [-1.09250196, -2.08595867, -0.02966325],\n",
       "        [-1.22678118, -2.20644281, -0.03189318],\n",
       "        [-0.43062271, -1.13733474, -0.01963683],\n",
       "        [-0.87069471, -1.60198601, -0.02620915],\n",
       "        [-0.97545108, -1.68704989, -0.0264937 ],\n",
       "        [-1.02473524, -1.76611138, -0.02637169],\n",
       "        [-0.47143395, -0.8888043 , -0.02085327],\n",
       "        [-0.75712227, -1.28763621, -0.02518664],\n",
       "        [-0.82195759, -1.33912314, -0.02406802],\n",
       "        [-0.83957157, -1.37969721, -0.0226802 ]]),\n",
       " array([0.99306077, 0.        ]),\n",
       " array([1.        , 0.62042133]),\n",
       " array([1, 0], dtype=int32),\n",
       " array([1.78401410e-06, 2.84917699e-03, 2.87721562e-03, 3.46602410e-01,\n",
       "        1.15457296e-01, 3.98213454e-02, 9.71383088e-06, 1.51529136e-07,\n",
       "        1.03344512e-07, 2.92606264e-01, 3.06626529e-01, 4.86650199e-01,\n",
       "        4.49254721e-01, 5.12724183e-02, 1.45026267e-01, 1.25094503e-01,\n",
       "        3.69628109e-02, 2.74712052e-02, 1.63769256e-02, 2.41579041e-01,\n",
       "        1.45773515e-01, 5.25224581e-03, 2.94087012e-03, 9.90649255e-07,\n",
       "        2.45067570e-03, 2.06014030e-02, 1.49801388e-04, 2.36727111e-03,\n",
       "        5.15123047e-02, 1.10547975e-01, 4.75554225e-06, 7.77450350e-06,\n",
       "        2.02023264e-04, 2.39303289e-03, 1.29196133e-05, 2.37683707e-06,\n",
       "        6.44616559e-02, 1.46642908e-01, 1.77823789e-02, 9.02562495e-03,\n",
       "        2.82523513e-01, 5.00384092e-01, 9.31855798e-01, 2.66398340e-01,\n",
       "        4.18580894e-04, 3.39322840e-04, 7.35481226e-05, 5.71536875e-05,\n",
       "        4.66567115e-04, 6.98169519e-04, 2.16379775e-07, 9.51551442e-08]),\n",
       " 1,\n",
       " array([[-1.10679546, 10.45051649],\n",
       "        [ 1.87656627,  8.3773317 ]]),\n",
       " 39,\n",
       " 39000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_detection(lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing velocities for 30 files...\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000030_00m30s000ms.npz\n",
      "Frame 30 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000031_00m31s000ms.npz\n",
      "Frame 31 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000032_00m32s000ms.npz\n",
      "Frame 32 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000033_00m33s000ms.npz\n",
      "Frame 33 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000034_00m34s000ms.npz\n",
      "Frame 34 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000035_00m35s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000037_00m37s000ms.npz\n",
      "lol\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000038_00m38s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000041_00m41s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000042_00m42s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000043_00m43s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000044_00m44s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000045_00m45s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000046_00m46s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000047_00m47s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000048_00m48s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000049_00m49s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000050_00m50s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000051_00m51s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000052_00m52s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000053_00m53s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000055_00m55s000ms.npz\n",
      "Frame 55 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000056_00m56s000ms.npz\n",
      "Frame 56 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000057_00m57s000ms.npz\n",
      "Frame 57 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000058_00m58s000ms.npz\n",
      "Frame 58 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Successfully modified/added 6 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000059_00m59s000ms.npz\n",
      "Frame 59 too close to video boundary - setting zero velocities\n",
      "Velocity computation complete. Processed 30 frames.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_landmark_velocities(directory_path=\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data_with_velocities(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks = data['dom_landmarks']\n",
    "    non_dom_landmarks = data['non_dom_landmarks']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores = data['blendshape_scores']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist = data['nose_to_wrist_dist']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    dom_velocity_small = data['dom_velocity_small']\n",
    "    dom_velocity_large = data['dom_velocity_large']\n",
    "    non_dom_velocity_small = data['non_dom_velocity_small']\n",
    "    non_dom_velocity_large = data['non_dom_velocity_large']\n",
    "    velocity_confidence = data['velocity_confidence']\n",
    "    velocity_calculation_confidence = data['velocity_calculation_confidence']\n",
    "    \n",
    "    \n",
    "    return (dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores, face_detected, \n",
    "            nose_to_wrist_dist, frame_idx, timestamp_ms, dom_velocity_small, dom_velocity_large, non_dom_velocity_small, non_dom_velocity_large, velocity_confidence, velocity_calculation_confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000038_00m38s000ms.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.39363739, -1.0700265 , -0.00305143],\n",
       "        [ 0.97136423, -1.57644665, -0.00496877],\n",
       "        [ 1.46836774, -1.82744896, -0.00675347],\n",
       "        [ 1.72082063, -2.30542273, -0.00885207],\n",
       "        [ 1.42347892, -0.64228076, -0.00464257],\n",
       "        [ 2.07694584, -0.71003564, -0.00634263],\n",
       "        [ 2.40785752, -0.73022253, -0.00746003],\n",
       "        [ 2.64053433, -0.76056474, -0.00843428],\n",
       "        [ 1.42604454,  0.06243992, -0.00533265],\n",
       "        [ 2.13331808,  0.01204477, -0.0064797 ],\n",
       "        [ 2.48190462, -0.05758553, -0.00725395],\n",
       "        [ 2.69627888, -0.12335527, -0.00817924],\n",
       "        [ 1.32628764,  0.64773733, -0.00633961],\n",
       "        [ 2.0127147 ,  0.68863802, -0.00854984],\n",
       "        [ 2.37463584,  0.65607082, -0.01100428],\n",
       "        [ 2.61884072,  0.5814024 , -0.01273712],\n",
       "        [ 1.16708765,  1.14139571, -0.00744293],\n",
       "        [ 1.67512678,  1.31477232, -0.00905192],\n",
       "        [ 1.98586433,  1.31685697, -0.01020205],\n",
       "        [ 2.23698912,  1.25980217, -0.01104865]]),\n",
       " array([[ 0.34865543, -0.87300687, -0.00707294],\n",
       "        [ 0.54838102, -2.0176903 , -0.01385578],\n",
       "        [ 0.75482239, -2.84087421, -0.01872619],\n",
       "        [ 1.07385388, -3.11592461, -0.02441419],\n",
       "        [ 0.23762178, -1.73914104, -0.02609634],\n",
       "        [-0.06964816, -2.92250017, -0.03306105],\n",
       "        [-0.26934441, -3.62546663, -0.03589176],\n",
       "        [-0.43500119, -4.21350061, -0.03793716],\n",
       "        [-0.19614188, -1.39153813, -0.02630789],\n",
       "        [-0.66938883, -2.67249616, -0.03332304],\n",
       "        [-0.9346507 , -3.44896687, -0.03523948],\n",
       "        [-1.12731987, -4.00548555, -0.03729751],\n",
       "        [-0.55655376, -1.14360028, -0.02540007],\n",
       "        [-0.88340714, -2.38144348, -0.03128325],\n",
       "        [-0.97873322, -3.04917378, -0.03061079],\n",
       "        [-1.04668584, -3.50892218, -0.03013283],\n",
       "        [-0.81331501, -0.99086985, -0.02454476],\n",
       "        [-0.95576637, -2.02786858, -0.02820837],\n",
       "        [-0.96756491, -2.52999505, -0.02606366],\n",
       "        [-0.97740639, -2.87555604, -0.0242493 ]]),\n",
       " array([0.95309049, 0.9971112 ]),\n",
       " array([1., 1.]),\n",
       " array([1, 1], dtype=int32),\n",
       " array([2.61497888e-07, 6.09485805e-01, 6.66510224e-01, 2.24942032e-05,\n",
       "        7.44131161e-03, 1.73297513e-03, 4.66031634e-06, 8.77407835e-09,\n",
       "        1.30992719e-08, 6.46855608e-02, 1.12185769e-01, 3.14638764e-01,\n",
       "        3.40171665e-01, 9.66857895e-02, 5.59946671e-02, 9.27308500e-02,\n",
       "        1.85562074e-01, 5.01178950e-02, 2.72241458e-02, 3.09120268e-01,\n",
       "        2.04561323e-01, 4.23894227e-02, 9.95219313e-03, 1.95796997e-06,\n",
       "        1.18284533e-03, 7.62107957e-04, 5.49562392e-04, 2.63022550e-04,\n",
       "        1.88694909e-01, 7.09728122e-01, 6.87026969e-09, 2.01036787e-09,\n",
       "        2.76245363e-02, 6.80366382e-02, 5.55389533e-06, 8.82448887e-07,\n",
       "        2.63516209e-03, 8.82715173e-03, 9.83234763e-01, 1.76649801e-02,\n",
       "        1.45584904e-03, 1.08166113e-02, 6.40561759e-01, 1.80189207e-01,\n",
       "        1.66507132e-04, 1.57471412e-04, 7.28819941e-05, 7.99991824e-08,\n",
       "        2.52817455e-03, 7.07714492e-03, 1.63025192e-07, 2.04153183e-09]),\n",
       " 1,\n",
       " array([[-1.30196462, 13.72011242],\n",
       "        [ 1.9907047 ,  8.14935809]]),\n",
       " 38,\n",
       " 38000,\n",
       " array([[ 2.83245659e-01, -9.51130027e-01, -3.08790660e-01,\n",
       "          9.99950895e-01,  9.90994563e-03],\n",
       "        [ 5.74164601e-01, -9.70861583e-01, -2.39640954e-01,\n",
       "          9.99994101e-01,  3.43473542e-03],\n",
       "        [ 1.02338407e+00, -9.82220805e-01, -1.87729301e-01,\n",
       "          9.99999952e-01,  3.08340914e-04],\n",
       "        [ 1.49738820e+00, -9.72892054e-01, -2.31259704e-01,\n",
       "          9.99999493e-01, -1.00652856e-03],\n",
       "        [ 7.34282624e-01, -9.82831568e-01,  1.84505038e-01,\n",
       "          9.99998552e-01,  1.70154999e-03],\n",
       "        [ 1.06060788e+00, -9.69227653e-01,  2.46166118e-01,\n",
       "          9.99999862e-01,  5.25666486e-04],\n",
       "        [ 1.23166174e+00, -9.65766409e-01,  2.59413266e-01,\n",
       "          9.99999988e-01, -1.54835581e-04],\n",
       "        [ 1.29275020e+00, -9.64349520e-01,  2.64631827e-01,\n",
       "          9.99999992e-01, -1.28235408e-04],\n",
       "        [ 6.99699788e-01, -9.43598346e-01,  3.31092375e-01,\n",
       "          9.99999981e-01,  1.94870357e-04],\n",
       "        [ 8.67838267e-01, -9.01537987e-01,  4.32699963e-01,\n",
       "          9.99996693e-01,  2.57170094e-03],\n",
       "        [ 9.48520124e-01, -8.87552182e-01,  4.60707200e-01,\n",
       "          9.99990655e-01,  4.32320389e-03],\n",
       "        [ 9.58612493e-01, -8.75265422e-01,  4.83642886e-01,\n",
       "          9.99983639e-01,  5.72034945e-03],\n",
       "        [ 5.96605936e-01, -8.91307199e-01,  4.53399908e-01,\n",
       "          9.99999515e-01, -9.84561764e-04],\n",
       "        [ 7.81077264e-01, -9.41186138e-01,  3.37888524e-01,\n",
       "          9.99993361e-01,  3.64391548e-03],\n",
       "        [ 8.56953509e-01, -9.99706018e-01, -2.42461851e-02,\n",
       "          9.99970583e-01,  7.67031501e-03],\n",
       "        [ 9.35376599e-01, -9.42338945e-01, -3.34659995e-01,\n",
       "          9.99950956e-01,  9.90384928e-03],\n",
       "        [ 5.07241517e-01, -8.21240115e-01,  5.70582749e-01,\n",
       "          9.99996387e-01, -2.68803101e-03],\n",
       "        [ 7.70959937e-01, -9.49128690e-01,  3.14888439e-01,\n",
       "          9.99989567e-01,  4.56788658e-03],\n",
       "        [ 8.55486510e-01, -9.99944224e-01,  1.05616781e-02,\n",
       "          9.99955691e-01,  9.41356512e-03],\n",
       "        [ 8.56527180e-01, -9.61452232e-01, -2.74972008e-01,\n",
       "          9.99912471e-01,  1.32306595e-02]]),\n",
       " array([[ 0.08377601, -0.96634621,  0.25724501,  0.99998857, -0.00478157],\n",
       "        [ 0.12360296, -0.91206496,  0.41004573,  0.9999916 , -0.00409779],\n",
       "        [ 0.17568525, -0.90714081,  0.42082721,  0.99999484, -0.00321224],\n",
       "        [ 0.2135733 , -0.93015126,  0.36717657,  0.99999583, -0.00288659],\n",
       "        [ 0.13811865, -0.42147784,  0.9068387 ,  0.99999676,  0.00254528],\n",
       "        [ 0.18918996, -0.32559784,  0.94550835,  0.99999226,  0.0039347 ],\n",
       "        [ 0.2158913 , -0.30772958,  0.95147386,  0.99999225,  0.00393718],\n",
       "        [ 0.23491688, -0.29026248,  0.95694707,  0.99999285,  0.00378249],\n",
       "        [ 0.13301327, -0.02812173,  0.99960451,  0.99997424,  0.00717825],\n",
       "        [ 0.16743043,  0.00339658,  0.99999423,  0.99993237,  0.01162965],\n",
       "        [ 0.2102532 , -0.12761713,  0.99182351,  0.99995017,  0.00998335],\n",
       "        [ 0.25071537, -0.18979082,  0.98182455,  0.99997002,  0.00774355],\n",
       "        [ 0.12767858,  0.30027896,  0.95385143,  0.99992934,  0.01188744],\n",
       "        [ 0.13218646,  0.3304315 ,  0.94382998,  0.99983946,  0.01791824],\n",
       "        [ 0.10628995,  0.12643687,  0.99197466,  0.9998062 ,  0.01968669],\n",
       "        [ 0.09063633, -0.0790264 ,  0.99687252,  0.99981271,  0.01935321],\n",
       "        [ 0.12116634,  0.52616149,  0.85038467,  0.99986077,  0.01668674],\n",
       "        [ 0.11912041,  0.57853599,  0.81565686,  0.99975582,  0.02209764],\n",
       "        [ 0.09068029,  0.48386308,  0.87514372,  0.99959642,  0.02840762],\n",
       "        [ 0.0726602 ,  0.46357844,  0.88605588,  0.99940082,  0.03461226]]),\n",
       " array([[ 9.56830829e-01, -1.86014638e-01,  9.82546973e-01,\n",
       "          9.99968856e-01, -7.89225385e-03],\n",
       "        [ 2.20236999e+00, -6.02579976e-01,  7.98058502e-01,\n",
       "          9.99983114e-01, -5.81136963e-03],\n",
       "        [ 3.43859809e+00, -7.05730104e-01,  7.08480783e-01,\n",
       "          9.99988814e-01, -4.72989852e-03],\n",
       "        [ 4.11744769e+00, -6.58497079e-01,  7.52583283e-01,\n",
       "          9.99988849e-01, -4.72241644e-03],\n",
       "        [ 2.23677052e+00, -7.52123760e-01,  6.59021889e-01,\n",
       "          9.99983257e-01, -5.78674966e-03],\n",
       "        [ 3.84925454e+00, -8.87826105e-01,  4.60179104e-01,\n",
       "          9.99993203e-01, -3.68688040e-03],\n",
       "        [ 4.85942171e+00, -9.19606947e-01,  3.92839741e-01,\n",
       "          9.99995775e-01, -2.90693122e-03],\n",
       "        [ 5.69796744e+00, -9.36903148e-01,  3.49589032e-01,\n",
       "          9.99996926e-01, -2.47964532e-03],\n",
       "        [ 2.20205112e+00, -9.13955035e-01,  4.05815469e-01,\n",
       "          9.99989208e-01, -4.64582400e-03],\n",
       "        [ 4.11282283e+00, -9.67712856e-01,  2.52055209e-01,\n",
       "          9.99995367e-01, -3.04389029e-03],\n",
       "        [ 5.26457687e+00, -9.75764613e-01,  2.18822807e-01,\n",
       "          9.99997559e-01, -2.20971024e-03],\n",
       "        [ 6.07398683e+00, -9.78738705e-01,  2.05111063e-01,\n",
       "          9.99998458e-01, -1.75602218e-03],\n",
       "        [ 2.33922883e+00, -9.88859727e-01,  1.48850394e-01,\n",
       "          9.99995061e-01, -3.14287794e-03],\n",
       "        [ 4.32007071e+00, -9.91222080e-01,  1.32207371e-01,\n",
       "          9.99997530e-01, -2.22261884e-03],\n",
       "        [ 5.37355130e+00, -9.87980463e-01,  1.54578797e-01,\n",
       "          9.99999258e-01, -1.21856351e-03],\n",
       "        [ 6.07987366e+00, -9.85166127e-01,  1.71603328e-01,\n",
       "          9.99999846e-01, -5.54083498e-04],\n",
       "        [ 2.54748700e+00, -9.99166329e-01, -4.08245796e-02,\n",
       "          9.99998099e-01, -1.94995851e-03],\n",
       "        [ 4.26456064e+00, -9.99240400e-01,  3.89695050e-02,\n",
       "          9.99999475e-01, -1.02514303e-03],\n",
       "        [ 5.13103482e+00, -9.96577897e-01,  8.26589042e-02,\n",
       "          1.00000000e+00, -9.08264771e-07],\n",
       "        [ 5.73010774e+00, -9.93633896e-01,  1.12657360e-01,\n",
       "          9.99999814e-01,  6.10112737e-04]]),\n",
       " array([[ 5.11194686e-02, -1.49637873e-01, -9.88740869e-01,\n",
       "          9.99953717e-01, -9.62096098e-03],\n",
       "        [ 8.72495166e-02, -5.46486987e-01, -8.37467595e-01,\n",
       "          9.99973210e-01, -7.31982503e-03],\n",
       "        [ 1.13236928e-01, -7.18064392e-01, -6.95976672e-01,\n",
       "          9.99964405e-01, -8.43732225e-03],\n",
       "        [ 1.42806564e-01, -7.83088471e-01, -6.21910320e-01,\n",
       "          9.99975476e-01, -7.00336864e-03],\n",
       "        [ 1.92801439e-01, -9.91213165e-01, -1.32274187e-01,\n",
       "          9.99972574e-01,  7.40612942e-03],\n",
       "        [ 1.79505408e-01, -9.90645440e-01, -1.36461025e-01,\n",
       "          9.99995976e-01,  2.83698977e-03],\n",
       "        [ 8.27962234e-02, -9.99210422e-01, -3.97307467e-02,\n",
       "          9.99990304e-01, -4.40356822e-03],\n",
       "        [ 2.43797622e-02, -3.78294733e-01,  9.25685203e-01,\n",
       "          9.99579755e-01, -2.89881525e-02],\n",
       "        [ 2.31185451e-01, -9.96829570e-01,  7.95663738e-02,\n",
       "          9.99980411e-01,  6.25913475e-03],\n",
       "        [ 1.89673339e-01, -9.96693679e-01,  8.12509130e-02,\n",
       "          9.99994295e-01,  3.37776556e-03],\n",
       "        [ 1.06755975e-01, -9.77182287e-01,  2.12402397e-01,\n",
       "          9.99999962e-01,  2.76910542e-04],\n",
       "        [ 6.18383786e-02, -8.85154260e-01,  4.65297685e-01,\n",
       "          9.99999250e-01, -1.22507070e-03],\n",
       "        [ 2.60501703e-01, -9.74916509e-01,  2.22570887e-01,\n",
       "          9.99989583e-01,  4.56440892e-03],\n",
       "        [ 2.07590399e-01, -9.68628706e-01,  2.48512435e-01,\n",
       "          9.99996166e-01,  2.76902454e-03],\n",
       "        [ 1.29331418e-01, -9.04563765e-01,  4.26338358e-01,\n",
       "          9.99996998e-01,  2.45022932e-03],\n",
       "        [ 9.53071575e-02, -7.74550271e-01,  6.32512354e-01,\n",
       "          9.99990383e-01,  4.38566440e-03],\n",
       "        [ 2.76214022e-01, -9.40389830e-01,  3.40098468e-01,\n",
       "          9.99996097e-01,  2.79382724e-03],\n",
       "        [ 2.35593828e-01, -9.29174931e-01,  3.69640294e-01,\n",
       "          9.99995271e-01,  3.07529110e-03],\n",
       "        [ 1.72746624e-01, -8.84540005e-01,  4.66464339e-01,\n",
       "          9.99990826e-01,  4.28338044e-03],\n",
       "        [ 1.43187600e-01, -8.23815555e-01,  5.66857947e-01,\n",
       "          9.99981020e-01,  6.16120634e-03]]),\n",
       " array([0.99133155, 0.99454445]),\n",
       " array([1. , 0.7]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
