{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_model_path = \"hand_landmarker.task\"\n",
    "face_model_path = \"face_landmarker.task\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "import tempfile\n",
    "\n",
    "class SuppressOutput:\n",
    "    \"\"\"\n",
    "    A context manager that suppresses stdout and stderr from C/C++ libraries.\n",
    "    This is a more aggressive approach than just Python's logging or environment variables.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Create a temporary file to redirect output\n",
    "        self.null_fds = [tempfile.TemporaryFile(mode='w+b') for _ in range(2)]\n",
    "        # Save the original file descriptors to restore later\n",
    "        self.save_fds = [os.dup(1), os.dup(2)]\n",
    "        \n",
    "    def __enter__(self):\n",
    "        # Redirect stdout and stderr to the null files\n",
    "        os.dup2(self.null_fds[0].fileno(), 1)\n",
    "        os.dup2(self.null_fds[1].fileno(), 2)\n",
    "        # Also redirect Python-level stdout/stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        # Restore normal stdout and stderr\n",
    "        for fd in self.null_fds:\n",
    "            fd.close()\n",
    "        for fd in range(2):\n",
    "            os.dup2(self.save_fds[fd], fd + 1)\n",
    "        for fd in self.save_fds:\n",
    "            os.close(fd)\n",
    "        # Restore Python-level stdout/stderr\n",
    "        sys.stdout = sys.__stdout__\n",
    "        sys.stderr = sys.__stderr__\n",
    "\n",
    "try:\n",
    "    import ctypes\n",
    "    libc = ctypes.CDLL(None)\n",
    "    # Attempt to get C's stdout/stderr file descriptor\n",
    "    c_stdout = ctypes.c_void_p.in_dll(libc, 'stdout')\n",
    "    c_stderr = ctypes.c_void_p.in_dll(libc, 'stderr')\n",
    "    devnull = open(os.devnull, 'w')\n",
    "    os.dup2(devnull.fileno(), c_stdout.value)\n",
    "    os.dup2(devnull.fileno(), c_stderr.value)\n",
    "except:\n",
    "    # If this approach fails, continue with other methods\n",
    "    pass\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=INFO, 2=WARNING, 3=ERROR\n",
    "logging.getLogger(\"mediapipe\").setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
    "os.environ['GLOG_minloglevel'] = '3'      # Suppress Google logging (used by MediaPipe)\n",
    "os.environ['MEDIAPIPE_DISABLE_GPU'] = '1'  # Optional: Disable GPU logging messages\n",
    "\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = '0'\n",
    "# ABSL specific flags\n",
    "os.environ['ABSL_LOGGING_LEVEL'] = '50'  # Higher than any level that should be output\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "logging.getLogger(\"mediapipe\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"absl\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\\\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import glob\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, min_face_detection_confidence=0.5, min_face_presence_confidence=0.5, num_hands=2, dominand_hand='Right', visualize=False, output_face_blendshapes=True, adaptive_threshold=True, max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects hands and face in an image, extracts hand landmark coordinates and face blendshapes.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        min_hand_detection_confidence (float): Confidence threshold for hand detection (0.0-1.0)\n",
    "        min_hand_presence_confidence (float): Confidence threshold for hand presence (0.0-1.0)\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        visualize (bool): Whether to visualize the results\n",
    "        output_face_blendshapes (bool): Whether to detect and extract face blendshapes\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (dom_landmarks, non_dom_landmarks, wrists, confidence_scores, detection_status, \n",
    "                blendshape_scores, face_landmark_5, face_detected)\n",
    "               - dom_landmarks: NumPy array of shape [20, 3] with coordinates of dominant hand landmarks\n",
    "               - non_dom_landmarks: NumPy array of shape [20, 3] with coordinates of non-dominant hand landmarks\n",
    "               - wrists: NumPy array of shape [2, 2] with coordinates of both wrists [x, y]\n",
    "               - confidence_scores: NumPy array of shape [2] with confidence scores [dominant_hand, non_dominant_hand]\n",
    "               - detection_status: NumPy array of shape [2] with binary detection status [dominant_hand, non_dominant_hand]\n",
    "               - blendshape_scores: NumPy array of shape [26] with selected face blendshape scores\n",
    "               - face_landmark_5: NumPy array of shape [2] with coordinates of the 5th face landmark [x, y]\n",
    "               - face_detected: Binary value (1 if face detected, 0 if not)\n",
    "    \"\"\"\n",
    "    # Initialize output arrays for face detection\n",
    "    blendshape_scores = np.zeros(52)\n",
    "    nose_landmark = np.zeros(2)\n",
    "    left_eye_landmark = np.zeros(2)\n",
    "    right_eye_landmark = np.zeros(2)\n",
    "    face_detected = 0\n",
    "    \n",
    "    # PART 1: HAND LANDMARK DETECTION\n",
    "    # 1.1: Configure the hand landmarker\n",
    "    hand_base_options = python.BaseOptions(\n",
    "        model_asset_path=hand_model_path\n",
    "    )\n",
    "\n",
    "    VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "    # Configure detection options\n",
    "    hand_options = vision.HandLandmarkerOptions(\n",
    "        base_options=hand_base_options,\n",
    "        num_hands=num_hands,                             \n",
    "        min_hand_detection_confidence=min_hand_detection_confidence,       \n",
    "        min_hand_presence_confidence=min_hand_presence_confidence,        \n",
    "        min_tracking_confidence=0.5,             \n",
    "        running_mode=VisionRunningMode.IMAGE\n",
    "    )\n",
    "\n",
    "    # Create the hand detector\n",
    "    hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "\n",
    "    # 1.2: Load the input image\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "\n",
    "    # 1.3: Detect hand landmarks\n",
    "    hand_detection_result = hand_detector.detect(image)\n",
    "    \n",
    "    # Initialize hand output arrays with zeros\n",
    "    dom_landmarks = np.zeros((20, 3))       # 20 landmarks (excluding wrist), [x,y,z]\n",
    "    non_dom_landmarks = np.zeros((20, 3))   # 20 landmarks (excluding wrist), [x,y,z]\n",
    "    wrists = np.zeros((2, 2))               # 2 wrists, [x,y]\n",
    "    confidence_scores = np.zeros(2)         # Confidence scores for [dominant, non-dominant]\n",
    "    interpolation_scores = np.zeros(2) #Interpolation scores for [dominant, non-dominant]. Used later.\n",
    "    detection_status = np.zeros(2, dtype=np.int32)  # Binary detection status [dominant, non-dominant]\n",
    "    nose_to_wrist_dist = np.zeros((2, 2))\n",
    "    \n",
    "    # 1.4: Process hand landmarks if hands are detected\n",
    "    if hand_detection_result.hand_landmarks and hand_detection_result.handedness:\n",
    "        dom_hand_found = False\n",
    "        non_dom_hand_found = False\n",
    "        \n",
    "        # First, find the dominant and non-dominant hands in detection results\n",
    "        for idx, handedness in enumerate(hand_detection_result.handedness):\n",
    "            hand_type = handedness[0].category_name  # 'Left' or 'Right'\n",
    "            hand_score = handedness[0].score  # Confidence score for the handedness classification\n",
    "            \n",
    "            if hand_type == dominand_hand:\n",
    "                # This is the dominant hand\n",
    "                dom_hand_found = True\n",
    "                detection_status[0] = 1  # Set detection status to 1 (detected)\n",
    "                confidence_scores[0] = hand_score  # Store confidence score\n",
    "                interpolation_scores[0] = 1\n",
    "                \n",
    "                # Store dominant hand wrist coordinates [x,y]\n",
    "                dom_hand_landmarks = hand_detection_result.hand_landmarks[idx]\n",
    "                wrists[0, 0] = dom_hand_landmarks[0].x\n",
    "                wrists[0, 1] = dom_hand_landmarks[0].y\n",
    "                \n",
    "                # Store all other dominant hand landmarks (excluding wrist)\n",
    "                for i in range(1, 21):  # Landmarks 1-20 (skipping wrist which is index 0)\n",
    "                    dom_landmarks[i-1, 0] = dom_hand_landmarks[i].x\n",
    "                    dom_landmarks[i-1, 1] = dom_hand_landmarks[i].y\n",
    "                    dom_landmarks[i-1, 2] = dom_hand_landmarks[i].z\n",
    "                    \n",
    "            elif hand_type != dominand_hand:\n",
    "                # This is the non-dominant hand\n",
    "                non_dom_hand_found = True\n",
    "                detection_status[1] = 1  # Set detection status to 1 (detected)\n",
    "                confidence_scores[1] = hand_score  # Store confidence score\n",
    "                interpolation_scores[1] = 1\n",
    "                \n",
    "                # Store non-dominant hand wrist coordinates [x,y]\n",
    "                non_dom_hand_landmarks = hand_detection_result.hand_landmarks[idx]\n",
    "                wrists[1, 0] = non_dom_hand_landmarks[0].x\n",
    "                wrists[1, 1] = non_dom_hand_landmarks[0].y\n",
    "                \n",
    "                # Store all other non-dominant hand landmarks (excluding wrist)\n",
    "                for i in range(1, 21):  # Landmarks 1-20 (skipping wrist)\n",
    "                    non_dom_landmarks[i-1, 0] = non_dom_hand_landmarks[i].x\n",
    "                    non_dom_landmarks[i-1, 1] = non_dom_hand_landmarks[i].y\n",
    "                    non_dom_landmarks[i-1, 2] = non_dom_hand_landmarks[i].z\n",
    "                    \n",
    "        # Log information about which hands were found\n",
    "        print(f\"Dominant hand ({dominand_hand}) detected: {dom_hand_found}\")\n",
    "        print(f\"Non-dominant hand detected: {non_dom_hand_found}\")\n",
    "    \n",
    "\n",
    "   # PART 2: FACE LANDMARK DETECTION (If requested)\n",
    "    if output_face_blendshapes:\n",
    "        try:\n",
    "            # 2.1: Configure the face landmarker\n",
    "            face_base_options = python.BaseOptions(\n",
    "                model_asset_path=face_model_path\n",
    "            )\n",
    "            \n",
    "            # Configure face detection options\n",
    "            face_options = vision.FaceLandmarkerOptions(\n",
    "                base_options=face_base_options,\n",
    "                min_face_detection_confidence=min_face_detection_confidence,\n",
    "                min_face_presence_confidence=min_face_presence_confidence,\n",
    "                output_face_blendshapes=True,\n",
    "                num_faces=1,\n",
    "                running_mode=VisionRunningMode.IMAGE\n",
    "            )\n",
    "            \n",
    "            # Create the face detector\n",
    "            face_detector = vision.FaceLandmarker.create_from_options(face_options)\n",
    "            \n",
    "            # 2.2: Detect face landmarks (reuse the same image)\n",
    "            face_detection_result = face_detector.detect(image)\n",
    "            \n",
    "            # 2.3: Process face blendshapes if face is detected\n",
    "            if (face_detection_result.face_blendshapes and len(face_detection_result.face_blendshapes) > 0 and\n",
    "                face_detection_result.face_landmarks and len(face_detection_result.face_landmarks) > 0):\n",
    "                \n",
    "                # Set face detected flag to 1\n",
    "                face_detected = 1\n",
    "                \n",
    "                # Get all blendshapes from the first face\n",
    "                all_blendshapes = face_detection_result.face_blendshapes[0]\n",
    "                \n",
    "                # Initialize blendshape_scores with the correct size to hold all blendshapes\n",
    "                # Assuming MediaPipe returns all 52 blendshapes\n",
    "                blendshape_scores = np.zeros(len(all_blendshapes))\n",
    "                \n",
    "                # Fill the blendshape_scores array with ALL scores\n",
    "                for i in range(len(all_blendshapes)):\n",
    "                    blendshape_scores[i] = all_blendshapes[i].score\n",
    "                \n",
    "                # Get nose coordinates\n",
    "                nose = face_detection_result.face_landmarks[0][4]\n",
    "                nose_landmark[0] = nose.x\n",
    "                nose_landmark[1] = nose.y\n",
    "    \n",
    "                # Get eye coordinates\n",
    "                left_eye = face_detection_result.face_landmarks[0][473]\n",
    "                left_eye_landmark[0] = left_eye.x\n",
    "                left_eye_landmark[1] = left_eye.y\n",
    "    \n",
    "                right_eye = face_detection_result.face_landmarks[0][468]\n",
    "                right_eye_landmark[0] = right_eye.x\n",
    "                right_eye_landmark[1] = right_eye.y\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during face detection: {e}\")\n",
    "            # Keep default zero values for face outputs if detection fails\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PART 3: VISUALIZATION\n",
    "    if visualize:\n",
    "        # Load the image with OpenCV for visualization\n",
    "        img_cv = cv2.imread(image_path)\n",
    "        img_height, img_width, _ = img_cv.shape\n",
    "\n",
    "        # 3.1: Draw hand landmarks if hands are detected\n",
    "        if hand_detection_result.hand_landmarks:\n",
    "            print(f\"Visualizing {len(hand_detection_result.hand_landmarks)} hands\")\n",
    "            \n",
    "            # Define connections between landmarks for hand skeleton\n",
    "            connections = [\n",
    "                # Thumb connections\n",
    "                (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "                # Index finger connections\n",
    "                (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "                # Middle finger connections\n",
    "                (0, 9), (9, 10), (10, 11), (11, 12),\n",
    "                # Ring finger connections\n",
    "                (0, 13), (13, 14), (14, 15), (15, 16),\n",
    "                # Pinky finger connections\n",
    "                (0, 17), (17, 18), (18, 19), (19, 20),\n",
    "                # Palm connections\n",
    "                (0, 5), (5, 9), (9, 13), (13, 17)\n",
    "            ]\n",
    "            \n",
    "            for idx, hand_landmarks in enumerate(hand_detection_result.hand_landmarks):\n",
    "                # Determine if this is the dominant hand\n",
    "                is_dominant = False\n",
    "                if hand_detection_result.handedness:\n",
    "                    hand_type = hand_detection_result.handedness[idx][0].category_name\n",
    "                    is_dominant = (hand_type == dominand_hand)\n",
    "                \n",
    "                # Use different colors for dominant vs non-dominant hand\n",
    "                hand_color = (0, 0, 255) if is_dominant else (255, 0, 0)  # Blue for dominant, Red for non-dominant\n",
    "                \n",
    "                # Draw all landmark points\n",
    "                for landmark in hand_landmarks:\n",
    "                    # Convert normalized coordinates to pixel coordinates\n",
    "                    x = int(landmark.x * img_width)\n",
    "                    y = int(landmark.y * img_height)\n",
    "                    \n",
    "                    # Draw the landmark point\n",
    "                    cv2.circle(img_cv, (x, y), 5, hand_color, -1)\n",
    "                \n",
    "                # Draw connections between landmarks (hand skeleton)\n",
    "                for connection in connections:\n",
    "                    start_idx, end_idx = connection\n",
    "                    \n",
    "                    if start_idx < len(hand_landmarks) and end_idx < len(hand_landmarks):\n",
    "                        start_point = hand_landmarks[start_idx]\n",
    "                        end_point = hand_landmarks[end_idx]\n",
    "                        \n",
    "                        # Convert normalized coordinates to pixel coordinates\n",
    "                        start_x = int(start_point.x * img_width)\n",
    "                        start_y = int(start_point.y * img_height)\n",
    "                        end_x = int(end_point.x * img_width)\n",
    "                        end_y = int(end_point.y * img_height)\n",
    "                        \n",
    "                        # Draw the connection line\n",
    "                        cv2.line(img_cv, (start_x, start_y), (end_x, end_y), hand_color, 2)\n",
    "                \n",
    "                # Add hand type label (Left/Right, Dominant/Non-dominant)\n",
    "                if hand_detection_result.handedness:\n",
    "                    handedness = hand_detection_result.handedness[idx]\n",
    "                    hand_type = handedness[0].category_name  # 'Left' or 'Right'\n",
    "                    hand_score = handedness[0].score\n",
    "                    dom_status = \"Dominant\" if hand_type == dominand_hand else \"Non-dominant\"\n",
    "                    cv2.putText(img_cv, f\"{hand_type} Hand - {dom_status} ({hand_score:.2f})\", \n",
    "                            (10, 30 + idx * 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            0.8, hand_color, 2)\n",
    "                    \n",
    "                    # Calculate and draw a bounding box\n",
    "                    x_coords = [landmark.x for landmark in hand_landmarks]\n",
    "                    y_coords = [landmark.y for landmark in hand_landmarks]\n",
    "                    min_x, max_x = min(x_coords), max(x_coords)\n",
    "                    min_y, max_y = min(y_coords), max(y_coords)\n",
    "                    \n",
    "                    # Convert to pixel coordinates\n",
    "                    min_x, max_x = int(min_x * img_width), int(max_x * img_width)\n",
    "                    min_y, max_y = int(min_y * img_height), int(max_y * img_height)\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(img_cv, (min_x, min_y), (max_x, max_y), hand_color, 2)\n",
    "\n",
    "        # 3.2: Draw Nose if face was detected\n",
    "        if face_detected == 1:\n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            face_x = int(nose_landmark[0] * img_width)\n",
    "            face_y = int(nose_landmark[1] * img_height)\n",
    "            \n",
    "            # Draw the Nose with a distinctive color and size\n",
    "            cv2.circle(img_cv, (face_x, face_y), 8, (0, 255, 255), -1)  # Yellow circle\n",
    "            cv2.putText(img_cv, \"Nose\", (face_x + 10, face_y), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "            # Draw eyes\n",
    "            left_eye_x = int(left_eye_landmark[0] * img_width)\n",
    "            left_eye_y = int(left_eye_landmark[1] * img_height)\n",
    "            right_eye_x = int(right_eye_landmark[0] * img_width)\n",
    "            right_eye_y = int(right_eye_landmark[1] * img_height)\n",
    "            \n",
    "            cv2.circle(img_cv, (left_eye_x, left_eye_y), 6, (255, 255, 0), -1)  # Cyan circle\n",
    "            cv2.circle(img_cv, (right_eye_x, right_eye_y), 6, (255, 255, 0), -1)  # Cyan circle\n",
    "            cv2.line(img_cv, (left_eye_x, left_eye_y), (right_eye_x, right_eye_y), (255, 255, 0), 2)\n",
    "        # 3.3: Add detection status information to visualization\n",
    "        y_pos = img_height - 80\n",
    "        hand_status_text = f\"Hand Detection: Dom={detection_status[0]}, Non-Dom={detection_status[1]}\"\n",
    "        hand_conf_text = f\"Hand Confidence: Dom={confidence_scores[0]:.2f}, Non-Dom={confidence_scores[1]:.2f}\"\n",
    "        face_status_text = f\"Face Detection: {face_detected}\"\n",
    "        \n",
    "        cv2.putText(img_cv, hand_status_text, (10, y_pos), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img_cv, hand_conf_text, (10, y_pos + 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img_cv, face_status_text, (10, y_pos + 60), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # 3.4: Display the result\n",
    "        cv2.imshow('Hand and Face Landmarks', img_cv)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    if face_detected==1:\n",
    "        #Calculate distance between the eyes\n",
    "        eyes_diff = right_eye_landmark-left_eye_landmark\n",
    "        eyes_distance = np.sqrt(eyes_diff.dot(eyes_diff))\n",
    "        if detection_status[0]==1 and detection_status[1]==1:\n",
    "            nose_to_wrist_dist = (wrists-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / eyes_distance\n",
    "        elif detection_status[0]==1 and detection_status[1]==0:\n",
    "            nose_to_wrist_dist[0, :] = (wrists[0, :]-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "        elif detection_status[0]==0 and detection_status[1]==1:\n",
    "            nose_to_wrist_dist[1,:] = (wrists[1,:]-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "        \n",
    "    elif face_detected==0 and detection_status[0]==1:\n",
    "        #Calculate palm width distance as fallback scaling factor\n",
    "        palm_width_diff = dom_landmarks[5, :]- dom_landmarks[17, :]\n",
    "        palm_width_dist = np.sqrt(palm_width_diff.dot(palm_width_diff))\n",
    "        if detection_status[1]==1:\n",
    "            nose_to_wrist_dist = (wrists-nose_landmark) / palm_width_dist\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / palm_width_dist\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / palm_width_dist\n",
    "        elif detection_status[1]==0:\n",
    "            nose_to_wrist_dist[0,:] = (wrists[0,:]-nose_landmark) / palm_width_dist\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / palm_width_dist\n",
    "    elif face_detected==0 and detection_status[0]==0 and detection_status[1]==1:\n",
    "        #Calculate palm width distance as fallback scaling factor\n",
    "        palm_width_diff = non_dom_landmarks[5, :]- non_dom_landmarks[17, :]\n",
    "        palm_width_dist = np.sqrt(palm_width_diff.dot(palm_width_diff))\n",
    "        nose_to_wrist_dist[1,:] = (wrists[1,:]-nose_landmark) / palm_width_dist\n",
    "        #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "        non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / palm_width_dist\n",
    "    \n",
    "\n",
    "    \n",
    "    # Return all requested outputs\n",
    "    return dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lol \u001b[38;5;241m=\u001b[39m \u001b[43mdetect\u001b[49m(grimace_2_path, hand_model_path, face_model_path, min_hand_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, min_hand_presence_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, num_hands\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dominand_hand\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeft\u001b[39m\u001b[38;5;124m'\u001b[39m, visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detect' is not defined"
     ]
    }
   ],
   "source": [
    "lol = detect(grimace_2_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, num_hands=2, dominand_hand='Left', visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.22157899, -0.47131054, -0.03073101],\n",
       "        [-0.17301513, -0.94633667, -0.04778747],\n",
       "        [ 0.09016798, -1.17645313, -0.06066544],\n",
       "        [ 0.44656346, -1.2285489 , -0.07052706],\n",
       "        [ 0.75239831, -1.00154878, -0.03440262],\n",
       "        [ 0.74546371, -0.32252427, -0.05854694],\n",
       "        [ 0.52926255,  0.01389155, -0.07399104],\n",
       "        [ 0.3284734 ,  0.18489803, -0.08165359],\n",
       "        [ 0.96142811, -0.83551715, -0.02785412],\n",
       "        [ 0.733497  , -0.12437514, -0.05066952],\n",
       "        [ 0.38909197,  0.19143662, -0.05764463],\n",
       "        [ 0.1054417 ,  0.3583336 , -0.05927849],\n",
       "        [ 1.04701376, -0.64099421, -0.02452194],\n",
       "        [ 0.80774103, -0.04213003, -0.04881671],\n",
       "        [ 0.46820677,  0.20524576, -0.04839684],\n",
       "        [ 0.19587368,  0.31181122, -0.04229698],\n",
       "        [ 1.07308434, -0.42637636, -0.02360797],\n",
       "        [ 1.01975656, -0.01941465, -0.04110365],\n",
       "        [ 0.78549988,  0.19136538, -0.03911975],\n",
       "        [ 0.56846131,  0.24917491, -0.03242829]]),\n",
       " array([[ 0.43443525, -0.28010929, -0.01027886],\n",
       "        [ 0.80353707, -0.54089751, -0.0247014 ],\n",
       "        [ 0.97556812, -0.65599555, -0.03816538],\n",
       "        [ 1.01942988, -0.56354025, -0.05229027],\n",
       "        [ 0.86045716, -0.13682166, -0.03400054],\n",
       "        [ 1.31591499,  0.24062976, -0.04829271],\n",
       "        [ 1.60218625,  0.43870974, -0.05713674],\n",
       "        [ 1.81017121,  0.57159162, -0.06226961],\n",
       "        [ 0.61163023,  0.19540783, -0.03670957],\n",
       "        [ 1.09116728,  0.50590183, -0.04773441],\n",
       "        [ 1.4089959 ,  0.66185109, -0.05110947],\n",
       "        [ 1.64211591,  0.76527366, -0.05398791],\n",
       "        [ 0.37204199,  0.52394618, -0.03922421],\n",
       "        [ 0.87258241,  0.80436906, -0.0487699 ],\n",
       "        [ 1.172205  ,  0.94312534, -0.04679542],\n",
       "        [ 1.3891072 ,  1.00824313, -0.04479555],\n",
       "        [ 0.14446114,  0.84810335, -0.04231404],\n",
       "        [ 0.57520175,  1.08297814, -0.05030103],\n",
       "        [ 0.84108896,  1.15069684, -0.04695994],\n",
       "        [ 1.05146618,  1.16495436, -0.04323559]]),\n",
       " array([0.84230059, 0.72651029]),\n",
       " array([1., 1.]),\n",
       " array([1, 1], dtype=int32),\n",
       " array([5.13106784e-07, 2.02846597e-03, 3.18804756e-03, 5.41452050e-01,\n",
       "        7.69849420e-02, 3.81750800e-02, 1.87551632e-05, 3.96643820e-08,\n",
       "        2.66600040e-07, 3.10787767e-01, 3.27007324e-01, 3.97320539e-01,\n",
       "        3.99153769e-01, 1.49953105e-02, 5.87362051e-01, 5.51258922e-01,\n",
       "        2.45020236e-03, 3.55305187e-02, 3.04265637e-02, 4.82355088e-01,\n",
       "        5.29462278e-01, 4.21624212e-03, 2.38157995e-03, 6.42919476e-05,\n",
       "        1.46195479e-03, 3.30839418e-02, 7.51869346e-04, 9.70096048e-03,\n",
       "        8.69681120e-01, 8.20557177e-01, 7.09664860e-09, 2.41063027e-08,\n",
       "        2.43527279e-03, 4.74228486e-02, 2.91958713e-04, 1.99188435e-04,\n",
       "        3.98596115e-02, 1.40069559e-01, 1.56948388e-01, 7.12577777e-04,\n",
       "        1.05726868e-01, 7.86331594e-01, 1.80265668e-03, 3.23777436e-04,\n",
       "        1.01934398e-04, 2.63166294e-04, 7.19477441e-07, 1.26338258e-04,\n",
       "        9.95448863e-06, 5.62861487e-06, 3.39345775e-07, 4.37951542e-08]),\n",
       " 1,\n",
       " array([[ 3.81955315,  4.40232786],\n",
       "        [-3.99970874,  4.21392517]]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_detect(image_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, \n",
    "                   min_face_detection_confidence=0.5, min_face_presence_confidence=0.5, \n",
    "                   num_hands=2, dominand_hand='Right', visualize=False, output_face_blendshapes=True,\n",
    "                   max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Adaptively detects hands and face by progressively lowering detection thresholds\n",
    "    for undetected body parts.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        min_hand_detection_confidence (float): Initial confidence threshold for hand detection\n",
    "        min_hand_presence_confidence (float): Initial confidence threshold for hand presence\n",
    "        min_face_detection_confidence (float): Initial confidence threshold for face detection\n",
    "        min_face_presence_confidence (float): Initial confidence threshold for face presence\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        visualize (bool): Whether to visualize the final results\n",
    "        output_face_blendshapes (bool): Whether to detect and extract face blendshapes\n",
    "        max_attempts (int): Maximum number of detection attempts with lowered thresholds\n",
    "        threshold_reduction_factor (float): Factor to multiply thresholds by on each attempt (0-1)\n",
    "        min_threshold (float): Minimum threshold to prevent excessive lowering\n",
    "        \n",
    "    Returns:\n",
    "        Same output as the detect() function\n",
    "    \"\"\"\n",
    "    # Import the original detect function\n",
    "    #from your_module import detect  # Replace with actual module name\n",
    "    \n",
    "    # Store original thresholds\n",
    "    orig_hand_detection_conf = min_hand_detection_confidence\n",
    "    orig_hand_presence_conf = min_hand_presence_confidence\n",
    "    orig_face_detection_conf = min_face_detection_confidence\n",
    "    orig_face_presence_conf = min_face_presence_confidence\n",
    "    \n",
    "    # Initialize best results and detection status\n",
    "    best_results = None\n",
    "    best_detection_status = [0, 0]  # [dom_hand, non_dom_hand]\n",
    "    best_face_detected = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    # Try detection with progressively lower thresholds\n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"\\n--- Attempt {attempt+1}/{max_attempts} ---\")\n",
    "        \n",
    "        # Calculate current thresholds\n",
    "        if attempt > 0:\n",
    "            # Only lower thresholds for undetected parts\n",
    "            # For hands\n",
    "            if best_detection_status[0] == 0:  # Dominant hand not detected\n",
    "                hand_detection_conf_dom = max(orig_hand_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                hand_presence_conf_dom = max(orig_hand_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering dominant hand thresholds: {hand_detection_conf_dom:.3f}, {hand_presence_conf_dom:.3f}\")\n",
    "            else:\n",
    "                hand_detection_conf_dom = orig_hand_detection_conf\n",
    "                hand_presence_conf_dom = orig_hand_presence_conf\n",
    "                \n",
    "            if best_detection_status[1] == 0:  # Non-dominant hand not detected\n",
    "                hand_detection_conf_non_dom = max(orig_hand_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                hand_presence_conf_non_dom = max(orig_hand_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering non-dominant hand thresholds: {hand_detection_conf_non_dom:.3f}, {hand_presence_conf_non_dom:.3f}\")\n",
    "            else:\n",
    "                hand_detection_conf_non_dom = orig_hand_detection_conf\n",
    "                hand_presence_conf_non_dom = orig_hand_presence_conf\n",
    "            \n",
    "            # Use the minimum of the two calculated thresholds (MediaPipe doesn't support per-hand thresholds)\n",
    "            current_hand_detection_conf = min(hand_detection_conf_dom, hand_detection_conf_non_dom)\n",
    "            current_hand_presence_conf = min(hand_presence_conf_dom, hand_presence_conf_non_dom)\n",
    "            \n",
    "            # For face\n",
    "            if output_face_blendshapes and best_face_detected == 0:  # Face not detected\n",
    "                current_face_detection_conf = max(orig_face_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                current_face_presence_conf = max(orig_face_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering face thresholds: {current_face_detection_conf:.3f}, {current_face_presence_conf:.3f}\")\n",
    "            else:\n",
    "                current_face_detection_conf = orig_face_detection_conf\n",
    "                current_face_presence_conf = orig_face_presence_conf\n",
    "        else:\n",
    "            # Use original thresholds for first attempt\n",
    "            current_hand_detection_conf = orig_hand_detection_conf\n",
    "            current_hand_presence_conf = orig_hand_presence_conf\n",
    "            current_face_detection_conf = orig_face_detection_conf\n",
    "            current_face_presence_conf = orig_face_presence_conf\n",
    "            print(f\"Using original thresholds: hands={current_hand_detection_conf}, face={current_face_detection_conf}\")\n",
    "        \n",
    "        # Call detect with current thresholds (don't visualize intermediate attempts)\n",
    "        results = detect(image_path,  hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                        min_hand_detection_confidence=current_hand_detection_conf,\n",
    "                        min_hand_presence_confidence=current_hand_presence_conf,\n",
    "                        min_face_detection_confidence=current_face_detection_conf,\n",
    "                        min_face_presence_confidence=current_face_presence_conf,\n",
    "                        num_hands=num_hands,\n",
    "                        dominand_hand=dominand_hand,\n",
    "                        visualize=False,\n",
    "                        output_face_blendshapes=output_face_blendshapes)\n",
    "        \n",
    "        # Unpack results\n",
    "        dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
    "        \n",
    "        # Compare with best results so far\n",
    "        current_detection_count = detection_status[0] + detection_status[1] + face_detected\n",
    "        best_detection_count = best_detection_status[0] + best_detection_status[1] + best_face_detected\n",
    "        \n",
    "        if best_results is None or current_detection_count > best_detection_count:\n",
    "            best_results = results\n",
    "            best_detection_status = [detection_status[0], detection_status[1]]\n",
    "            best_face_detected = face_detected\n",
    "            \n",
    "            print(f\"New best detection: dominant hand={detection_status[0]}, \"\n",
    "                  f\"non-dominant hand={detection_status[1]}, face={face_detected}\")\n",
    "            \n",
    "            # If everything is detected, we can stop early\n",
    "            if detection_status[0] == 1 and detection_status[1] == 1 and (face_detected == 1 or not output_face_blendshapes):\n",
    "                print(\"All body parts detected. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"No improvement in detection. Continuing to next attempt.\")\n",
    "    \n",
    "    # Run final detection with visualization if requested\n",
    "    if visualize:\n",
    "        print(\"\\n--- Visualizing final results ---\")\n",
    "        # Call detect one more time with the parameters that gave best results, but with visualize=True\n",
    "        # For simplicity, we'll just use the best thresholds we found\n",
    "        # This is slightly inefficient (one extra detection) but keeps the code clean\n",
    "        \n",
    "        # Determine which thresholds gave the best results\n",
    "        if best_detection_status[0] == 0:  # If dominant hand not detected in best result\n",
    "            hand_detection_conf = min_threshold\n",
    "            hand_presence_conf = min_threshold\n",
    "        else:\n",
    "            hand_detection_conf = orig_hand_detection_conf\n",
    "            hand_presence_conf = orig_hand_presence_conf\n",
    "            \n",
    "        if output_face_blendshapes and best_face_detected == 0:  # If face not detected in best result\n",
    "            face_detection_conf = min_threshold\n",
    "            face_presence_conf = min_threshold\n",
    "        else:\n",
    "            face_detection_conf = orig_face_detection_conf\n",
    "            face_presence_conf = orig_face_presence_conf\n",
    "        \n",
    "        # Run final detection with visualization\n",
    "        final_results = detect(image_path, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                              min_hand_detection_confidence=hand_detection_conf,\n",
    "                              min_hand_presence_confidence=hand_presence_conf, \n",
    "                              min_face_detection_confidence=face_detection_conf,\n",
    "                              min_face_presence_confidence=face_presence_conf,\n",
    "                              num_hands=num_hands,\n",
    "                              dominand_hand=dominand_hand,\n",
    "                              visualize=True,\n",
    "                              output_face_blendshapes=output_face_blendshapes)\n",
    "        \n",
    "        # Use these results if they're better than our best so far\n",
    "        dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = final_results\n",
    "        current_detection_count = detection_status[0] + detection_status[1] + face_detected\n",
    "        best_detection_count = best_detection_status[0] + best_detection_status[1] + best_face_detected\n",
    "        \n",
    "        if current_detection_count > best_detection_count:\n",
    "            best_results = final_results\n",
    "    \n",
    "    # Print final detection summary\n",
    "    print(\"\\n=== Detection Summary ===\")\n",
    "    dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = best_results\n",
    "    print(f\"Dominant hand detected: {detection_status[0] == 1} (confidence: {confidence_scores[0]:.3f})\")\n",
    "    print(f\"Non-dominant hand detected: {detection_status[1] == 1} (confidence: {confidence_scores[1]:.3f})\")\n",
    "    if output_face_blendshapes:\n",
    "        print(f\"Face detected: {face_detected == 1}\")\n",
    "    print(f\"Total detection attempts: {attempt+1}\")\n",
    "    return best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=0, non-dominant hand=1, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824986.936870    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824986.940818  284930 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.039888  284934 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.056971  284940 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.133708    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.135590  284946 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.136250    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.142800  284947 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.160368  284950 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.199556    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.202131  284962 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.238721  284964 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.252189  284966 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering dominant hand thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824987.304140    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.307504  284978 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.308167    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.314875  284980 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.507001  284979 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.543464    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.547164  284994 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.568495  284996 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.686856  285000 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Visualizing final results ---\n",
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824987.738748    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.741043  285010 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.741756    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.750388  285011 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.779803  285022 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.819273    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.822941  285026 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.853940  285027 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.874366  285035 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.927914    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.931627  285064 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.932330    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.944897  285072 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.965721  285078 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 1 hands\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: False (confidence: 0.000)\n",
      "Non-dominant hand detected: True (confidence: 0.948)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n"
     ]
    }
   ],
   "source": [
    "best_results = adaptive_detect(without_left_hand_path, hand_model_path=hand_model_path, face_model_path=face_model_path,  min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, num_hands=2, dominand_hand='Left', visualize=True,output_face_blendshapes=True,max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dom_landmarks, non_dom_landmarks, confidence_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[ 4.4302598 ,  5.79258936, -0.01529764],\n",
       "        [ 4.97843194,  5.60716573, -0.03023903],\n",
       "        [ 5.4227688 ,  5.474167  , -0.04531926],\n",
       "        [ 5.6396736 ,  5.29133956, -0.06161126],\n",
       "        [ 5.46952226,  5.95203357, -0.02546122],\n",
       "        [ 6.14746077,  5.96317265, -0.03837759],\n",
       "        [ 6.5033938 ,  5.98504281, -0.04724246],\n",
       "        [ 6.77242064,  5.99687042, -0.05398038],\n",
       "        [ 5.45199885,  6.19957023, -0.02953801],\n",
       "        [ 6.20066676,  6.22105431, -0.03755009],\n",
       "        [ 6.60851154,  6.23570749, -0.04424638],\n",
       "        [ 6.9013028 ,  6.24974116, -0.05119639],\n",
       "        [ 5.33187311,  6.40705239, -0.03495561],\n",
       "        [ 6.02315108,  6.45053573, -0.04479358],\n",
       "        [ 6.42136374,  6.46895391, -0.05380662],\n",
       "        [ 6.71558972,  6.47136295, -0.0608963 ],\n",
       "        [ 5.14666546,  6.58524524, -0.04154579],\n",
       "        [ 5.66482483,  6.65936167, -0.04902381],\n",
       "        [ 5.98583324,  6.67587705, -0.05188949],\n",
       "        [ 6.25919546,  6.67568786, -0.0540517 ]]),\n",
       " array([0.       , 0.9484275]),\n",
       " array([0., 1.]),\n",
       " array([0, 1], dtype=int32),\n",
       " array([2.92115374e-07, 1.33861089e-02, 1.24196718e-02, 2.88109779e-01,\n",
       "        5.75697683e-02, 3.07354219e-02, 1.32326995e-05, 1.58878031e-08,\n",
       "        8.72999664e-08, 3.63642573e-01, 4.11365360e-01, 6.62605762e-01,\n",
       "        6.69030905e-01, 2.73421267e-03, 5.51178098e-01, 5.63342750e-01,\n",
       "        1.03895655e-02, 1.23140477e-02, 8.84756818e-03, 2.24526033e-01,\n",
       "        2.50775576e-01, 4.66695800e-03, 1.89312676e-03, 3.73961666e-05,\n",
       "        1.19879853e-03, 7.60920672e-03, 4.57036338e-04, 1.92889536e-03,\n",
       "        8.89772832e-01, 8.59930277e-01, 8.38296987e-10, 1.49412671e-09,\n",
       "        1.53606525e-02, 1.33757144e-02, 1.26740182e-04, 8.43084490e-05,\n",
       "        7.94648603e-02, 9.92558002e-02, 8.17362487e-01, 1.14337606e-02,\n",
       "        1.67473480e-02, 3.32397074e-01, 3.43104475e-03, 1.09493383e-03,\n",
       "        1.44784761e-04, 2.09721227e-04, 8.35517653e-07, 5.79597008e-06,\n",
       "        3.15646721e-05, 2.39465880e-05, 5.11401367e-07, 3.26468843e-08]),\n",
       " 1,\n",
       " array([[ 0.        ,  0.        ],\n",
       "        [-1.39355698,  4.85907125]]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_video(video_path, adaptive_detect_func=adaptive_detect, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                 min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5,\n",
    "                 min_face_detection_confidence=0.5, min_face_presence_confidence=0.5,\n",
    "                 num_hands=2, output_face_blendshapes=True,\n",
    "                 max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2, \n",
    "                 frame_step=1, start_time_seconds=0, end_time_seconds=None,\n",
    "                 save_failure_screenshots=False):\n",
    "    \"\"\"\n",
    "    Process a video frame-by-frame using the adaptive_detect function and save results.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file\n",
    "        adaptive_detect_func: The adaptive detection function to use\n",
    "        min_hand_detection_confidence (float): Initial confidence threshold for hand detection\n",
    "        min_hand_presence_confidence (float): Initial confidence threshold for hand presence\n",
    "        min_face_detection_confidence (float): Initial confidence threshold for face detection\n",
    "        min_face_presence_confidence (float): Initial confidence threshold for face presence\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        output_face_blendshapes (bool): Whether to detect face blendshapes\n",
    "        max_attempts (int): Maximum detection attempts for adaptive detection\n",
    "        threshold_reduction_factor (float): Factor to reduce thresholds by\n",
    "        min_threshold (float): Minimum threshold limit\n",
    "        frame_step (int): Process every Nth frame (1 = all frames)\n",
    "        start_time_seconds (float): Time in seconds to start processing from\n",
    "        end_time_seconds (float): Time in seconds to end processing (None = process until end)\n",
    "        save_failure_screenshots (bool): Save screenshots for all frames with any detection failures\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the directory containing saved frame results\n",
    "    \"\"\"\n",
    "    # Extract video name for directory creation\n",
    "    video_path = Path(video_path)\n",
    "    video_name = video_path.stem  # Get filename without extension\n",
    "    \n",
    "    # Extract dominant hand information from filename\n",
    "    if video_name.endswith(\"_R\"):\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "    elif video_name.endswith(\"_L\"):\n",
    "        extracted_dominant_hand = \"Left\"\n",
    "    else:\n",
    "        # Default if not specified in filename\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "        print(f\"Warning: Could not determine dominant hand from filename, using default: {extracted_dominant_hand}\")\n",
    "\n",
    "    # Use the extracted dominant hand instead of the parameter\n",
    "    dominand_hand = extracted_dominant_hand\n",
    "    print(f\"Detected dominant hand from filename: {dominand_hand}\")\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(f\"{video_name}_landmarks\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create screenshots directory if screenshot option is enabled\n",
    "    screenshots_dir = None\n",
    "    if save_failure_screenshots:\n",
    "        screenshots_dir = output_dir / \"failure_screenshots\"\n",
    "        screenshots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a log file to track processing\n",
    "    log_file = output_dir / \"processing_log.txt\"\n",
    "    \n",
    "    # Create a detailed statistics file\n",
    "    stats_file = output_dir / \"detection_statistics.json\"\n",
    "\n",
    "    # Initialize statistics tracking\n",
    "    stats = {\n",
    "        \"video_info\": {\n",
    "            \"name\": video_name,\n",
    "            \"path\": str(video_path),\n",
    "            \"total_frames\": 0,\n",
    "            \"processed_frames\": 0,\n",
    "            \"fps\": 0,\n",
    "            \"duration_seconds\": 0,\n",
    "            \"start_time\": start_time_seconds,\n",
    "            \"end_time\": end_time_seconds,\n",
    "            \"dominant_hand\": dominand_hand,\n",
    "            \"processing_started\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"processing_completed\": None\n",
    "        },\n",
    "        \"detection_rates\": {\n",
    "            \"dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"non_dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"face\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"overall\": {\n",
    "                \"all_detected\": 0,\n",
    "                \"partial_detections\": 0,\n",
    "                \"no_detections\": 0,\n",
    "                \"success_rate\": 0\n",
    "            }\n",
    "        },\n",
    "        \"failed_frames\": {\n",
    "            \"dominant_hand_failures\": [],\n",
    "            \"non_dominant_hand_failures\": [],\n",
    "            \"face_failures\": [],\n",
    "            \"all_failures\": []\n",
    "        },\n",
    "        \"processing_performance\": {\n",
    "            \"average_processing_time_ms\": 0,\n",
    "            \"total_processing_time_seconds\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(log_file, \"w\") as log:\n",
    "        log.write(f\"Processing video: {video_path}\\n\")\n",
    "        log.write(f\"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        log.write(f\"Parameters:\\n\")\n",
    "        log.write(f\"  - frame_step: {frame_step}\\n\")\n",
    "        log.write(f\"  - start_time: {start_time_seconds} seconds\\n\")\n",
    "        if end_time_seconds is not None:\n",
    "            log.write(f\"  - end_time: {end_time_seconds} seconds\\n\")\n",
    "        log.write(f\"  - dominand_hand: {dominand_hand}\\n\")\n",
    "        log.write(f\"  - num_hands: {num_hands}\\n\")\n",
    "        log.write(f\"  - detection confidence thresholds: {min_hand_detection_confidence}, {min_face_detection_confidence}\\n\")\n",
    "        log.write(\"\\n--- Frame processing log ---\\n\")\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration_seconds = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    # Update stats with video info\n",
    "    stats[\"video_info\"][\"total_frames\"] = total_frames\n",
    "    stats[\"video_info\"][\"fps\"] = fps\n",
    "    stats[\"video_info\"][\"duration_seconds\"] = duration_seconds\n",
    "    if end_time_seconds==None:\n",
    "        stats[\"video_info\"][\"end_time\"] = duration_seconds\n",
    "    \n",
    "    # Convert time to frame indices\n",
    "    start_frame = int(max(0, start_time_seconds * fps))\n",
    "    \n",
    "    # Set end frame if specified\n",
    "    if end_time_seconds is not None:\n",
    "        end_frame = min(total_frames, int(end_time_seconds * fps))\n",
    "    else:\n",
    "        end_frame = total_frames\n",
    "    \n",
    "    print(f\"Video: {video_name}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"Duration: {duration_seconds:.2f} seconds\")\n",
    "    print(f\"Processing frames {start_frame} to {end_frame} (time {start_time_seconds:.2f}s to {end_time_seconds if end_time_seconds is not None else duration_seconds:.2f}s)\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Process frames\n",
    "    frame_idx = 0\n",
    "    processed_count = 0\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    # Skip to start_frame\n",
    "    if start_frame > 0:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        frame_idx = start_frame\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        while frame_idx < end_frame:\n",
    "            # Read the next frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "            \n",
    "            # Only process every frame_step frames\n",
    "            if (frame_idx - start_frame) % frame_step != 0:\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "                \n",
    "            # Get timestamp in milliseconds\n",
    "            timestamp_ms = int(frame_idx * 1000 / fps)\n",
    "            timestamp_formatted = f\"{timestamp_ms//60000:02d}m{(timestamp_ms//1000)%60:02d}s{timestamp_ms%1000:03d}ms\"\n",
    "            \n",
    "            # Temporary frame path\n",
    "            temp_frame_path = Path(temp_dir) / f\"temp_frame_{frame_idx}.jpg\"\n",
    "            \n",
    "            # Save the current frame as an image\n",
    "            cv2.imwrite(str(temp_frame_path), frame)\n",
    "            \n",
    "            # Process the frame with adaptive_detect\n",
    "            print(f\"Processing frame {frame_idx}/{total_frames} (timestamp: {timestamp_formatted})\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                with SuppressOutput():\n",
    "                # Use adaptive_detect on the frame\n",
    "                    results = adaptive_detect_func(\n",
    "                        str(temp_frame_path), hand_model_path, face_model_path,\n",
    "                        min_hand_detection_confidence=min_hand_detection_confidence,\n",
    "                        min_hand_presence_confidence=min_hand_presence_confidence,\n",
    "                        min_face_detection_confidence=min_face_detection_confidence,\n",
    "                        min_face_presence_confidence=min_face_presence_confidence,\n",
    "                        num_hands=num_hands,\n",
    "                        dominand_hand=dominand_hand,\n",
    "                        visualize=False,\n",
    "                        output_face_blendshapes=output_face_blendshapes,\n",
    "                        max_attempts=max_attempts,\n",
    "                        threshold_reduction_factor=threshold_reduction_factor,\n",
    "                        min_threshold=min_threshold\n",
    "                    )\n",
    "                \n",
    "                # Calculate processing time\n",
    "                proc_time = time.time() - start_time\n",
    "                total_processing_time += proc_time\n",
    "                \n",
    "                # Unpack results\n",
    "                dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
    "                \n",
    "                # Update detection statistics\n",
    "                dom_hand_detected = detection_status[0] == 1\n",
    "                non_dom_hand_detected = detection_status[1] == 1\n",
    "                face_was_detected = face_detected == 1\n",
    "                \n",
    "                if dom_hand_detected:\n",
    "                    stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"dominant_hand\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"dominant_hand_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                if non_dom_hand_detected:\n",
    "                    stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"non_dominant_hand\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"non_dominant_hand_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                if face_was_detected:\n",
    "                    stats[\"detection_rates\"][\"face\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"face\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"face_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                # Track combined detection status\n",
    "                detection_count = dom_hand_detected + non_dom_hand_detected + face_was_detected\n",
    "                \n",
    "                if detection_count == 3:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"all_detected\"] += 1\n",
    "                elif detection_count == 0:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"no_detections\"] += 1\n",
    "                    stats[\"failed_frames\"][\"all_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"partial_detections\"] += 1\n",
    "                \n",
    "                # Save screenshot if any detection failed and screenshots are enabled\n",
    "                if save_failure_screenshots and (not dom_hand_detected or not non_dom_hand_detected or not face_was_detected):\n",
    "                    # Create a detailed failure type description for the filename\n",
    "                    failure_type = []\n",
    "                    if not dom_hand_detected:\n",
    "                        failure_type.append(\"DomHand\")\n",
    "                    if not non_dom_hand_detected:\n",
    "                        failure_type.append(\"NonDomHand\")\n",
    "                    if not face_was_detected:\n",
    "                        failure_type.append(\"Face\")\n",
    "                    \n",
    "                    failure_str = \"_\".join(failure_type)\n",
    "                    screenshot_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}_missing_{failure_str}.jpg\"\n",
    "                    screenshot_path = screenshots_dir / screenshot_filename\n",
    "                    \n",
    "                    # Copy the frame to the screenshots directory\n",
    "                    cv2.imwrite(str(screenshot_path), frame)\n",
    "                    print(f\"Saved failure screenshot: {screenshot_filename}\")\n",
    "                \n",
    "                # Create output filename with frame info\n",
    "                output_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                output_path = output_dir / output_filename\n",
    "                \n",
    "                # Save all results in a single .npz file\n",
    "                np.savez(\n",
    "                    output_path,\n",
    "                    dom_landmarks=dom_landmarks,\n",
    "                    non_dom_landmarks=non_dom_landmarks,\n",
    "                    confidence_scores=confidence_scores,\n",
    "                    interpolation_scores=interpolation_scores,\n",
    "                    detection_status=detection_status,\n",
    "                    blendshape_scores=blendshape_scores,\n",
    "                    face_detected=face_detected,\n",
    "                    nose_to_wrist_dist=nose_to_wrist_dist,\n",
    "                    frame_idx=np.array([frame_idx]),\n",
    "                    timestamp_ms=np.array([timestamp_ms])\n",
    "                )\n",
    "                \n",
    "                # Update processing log\n",
    "                detection_summary = f\"Dom: {detection_status[0]}, Non-dom: {detection_status[1]}, Face: {face_detected}\"\n",
    "                log_entry = f\"Frame {frame_idx}: {detection_summary} (proc time: {proc_time:.2f}s)\\n\"\n",
    "                \n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(log_entry)\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {frame_idx}: {e}\")\n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(f\"Error on frame {frame_idx}: {str(e)}\\n\")\n",
    "            \n",
    "            # Clean up temporary frame file\n",
    "            if temp_frame_path.exists():\n",
    "                temp_frame_path.unlink()\n",
    "                \n",
    "            frame_idx += 1\n",
    "    \n",
    "    # Close the video file\n",
    "    cap.release()\n",
    "    \n",
    "    # Update final statistics\n",
    "    stats[\"video_info\"][\"processed_frames\"] = processed_count\n",
    "    stats[\"video_info\"][\"processing_completed\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Calculate detection rates\n",
    "    if processed_count > 0:\n",
    "        stats[\"detection_rates\"][\"dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"non_dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"face\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"face\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"overall\"][\"success_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"overall\"][\"all_detected\"] / processed_count * 100\n",
    "        )\n",
    "    \n",
    "    # Calculate processing performance\n",
    "    if processed_count > 0:\n",
    "        stats[\"processing_performance\"][\"average_processing_time_ms\"] = (\n",
    "            total_processing_time / processed_count * 1000\n",
    "        )\n",
    "    stats[\"processing_performance\"][\"total_processing_time_seconds\"] = total_processing_time\n",
    "    \n",
    "    # Save statistics to JSON file\n",
    "    with open(stats_file, \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # Add summary statistics to log file\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"\\n\\n===== PROCESSING SUMMARY =====\\n\")\n",
    "        log.write(f\"Completed at: {stats['video_info']['processing_completed']}\\n\")\n",
    "        log.write(f\"Frames processed: {processed_count} from {start_frame} to {min(end_frame, frame_idx-1)}\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION RATES:\\n\")\n",
    "        log.write(f\"  Dominant hand ({dominand_hand}): {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Non-dominant hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  All parts detected: {stats['detection_rates']['overall']['success_rate']:.1f}%\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION FAILURES:\\n\")\n",
    "        log.write(f\"  Frames with dominant hand failures: {len(stats['failed_frames']['dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with non-dominant hand failures: {len(stats['failed_frames']['non_dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with face failures: {len(stats['failed_frames']['face_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with all parts missing: {len(stats['failed_frames']['all_failures'])}\\n\\n\")\n",
    "        \n",
    "        log.write(\"PERFORMANCE:\\n\")\n",
    "        log.write(f\"  Average processing time per frame: {stats['processing_performance']['average_processing_time_ms']:.2f} ms\\n\")\n",
    "        log.write(f\"  Total processing time: {stats['processing_performance']['total_processing_time_seconds']:.2f} seconds\\n\")\n",
    "    \n",
    "    print(f\"\\n===== PROCESSING SUMMARY =====\")\n",
    "    print(f\"Processed {processed_count} frames\")\n",
    "    print(f\"Detection rates: Dom hand: {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "          f\"Non-dom hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "          f\"Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\")\n",
    "    print(f\"All parts detected in {stats['detection_rates']['overall']['success_rate']:.1f}% of frames\")\n",
    "    print(f\"Full statistics saved to: {stats_file}\")\n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "    \n",
    "    return str(output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks = data['dom_landmarks']\n",
    "    non_dom_landmarks = data['non_dom_landmarks']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores = data['blendshape_scores']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist = data['nose_to_wrist_dist']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    \n",
    "    return (dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores, face_detected, \n",
    "            nose_to_wrist_dist, frame_idx, timestamp_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video(video_path=video_path, adaptive_detect_func=adaptive_detect, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                 min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5,\n",
    "                 min_face_detection_confidence=0.5, min_face_presence_confidence=0.5,\n",
    "                 num_hands=2, output_face_blendshapes=True,\n",
    "                 max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2, \n",
    "                 frame_step=1, start_time_seconds=30.2, end_time_seconds=60.4,\n",
    "                 save_failure_screenshots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = load_frame_data(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[7][1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interpolation_frames(x, nums_list):\n",
    "    \"\"\"\n",
    "    Returns integers in the range [x-5, x+5] that are not equal to x\n",
    "    and are not in nums_list.\n",
    "    \n",
    "    Args:\n",
    "        x (int): The reference integer\n",
    "        nums_list (list): A list of integers\n",
    "        \n",
    "    Returns:\n",
    "        list: Integers in [x-5, x+5] excluding x and elements in nums_list\n",
    "    \"\"\"\n",
    "    # Create the set of all integers in the range [x-5, x+5]\n",
    "    all_range = set(range(x-5, x+6))  # +6 because range is exclusive at upper bound\n",
    "    \n",
    "    # Remove x itself\n",
    "    all_range.discard(x)\n",
    "    \n",
    "    # Remove numbers that are in the input list\n",
    "    result = all_range - set(nums_list)\n",
    "    \n",
    "    # Convert back to a list and return\n",
    "    return sorted(list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def find_file_with_partial_name(partial_name, search_dir='.', recursive=False):\n",
    "    \"\"\"\n",
    "    Find files that start with the given partial name.\n",
    "    \n",
    "    Args:\n",
    "        partial_name (str): Partial file name to match\n",
    "        search_dir (str): Directory to search in (default: current directory)\n",
    "        recursive (bool): Whether to search in subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        list: Complete paths of all matching files\n",
    "    \"\"\"\n",
    "    # Create a search pattern for files starting with the partial name\n",
    "    search_pattern = os.path.join(search_dir, f\"{partial_name}*\")\n",
    "    \n",
    "    # Use recursive glob if requested\n",
    "    if recursive:\n",
    "        matches = []\n",
    "        for root, _, _ in os.walk(search_dir):\n",
    "            matches.extend(glob.glob(os.path.join(root, f\"{os.path.basename(partial_name)}*\")))\n",
    "        return matches\n",
    "    else:\n",
    "        return glob.glob(search_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers_on_both_sides(x, missing_numbers):\n",
    "    \"\"\"\n",
    "    Checks if the list of missing numbers has at least one number smaller than x\n",
    "    AND at least one number larger than x.\n",
    "    \n",
    "    Args:\n",
    "        x (int): The reference integer\n",
    "        missing_numbers (list): Output from find_missing_numbers(x, nums_list)\n",
    "        \n",
    "    Returns:\n",
    "        bool: False if all numbers are either all smaller or all larger than x.\n",
    "              True if there's at least one smaller and one larger number.\n",
    "    \"\"\"\n",
    "    has_smaller = False\n",
    "    has_larger = False\n",
    "    \n",
    "    for num in missing_numbers:\n",
    "        if num < x:\n",
    "            has_smaller = True\n",
    "        elif num > x:\n",
    "            has_larger = True\n",
    "            \n",
    "        # Early exit if we found both smaller and larger numbers\n",
    "        if has_smaller and has_larger:\n",
    "            return True\n",
    "    \n",
    "    # If we get here, we didn't find both smaller and larger numbers\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_npz_file(file_path, modifications):\n",
    "    \"\"\"\n",
    "    Load a .npz file, modify existing arrays and add new ones, then save it back.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .npz file\n",
    "        modifications (dict): Dictionary with keys as array names and values as new arrays\n",
    "                             or functions that take the original array and return a modified version\n",
    "    \"\"\"\n",
    "    # Load the npz file\n",
    "    with np.load(file_path) as data:\n",
    "        # Create a copy of all arrays\n",
    "        arrays = {name: data[name] for name in data.files}\n",
    "    \n",
    "    # Apply modifications and add new arrays\n",
    "    for name, modification in modifications.items():\n",
    "        if name in arrays:\n",
    "            if callable(modification):\n",
    "                # If the modification is a function, apply it to the original array\n",
    "                arrays[name] = modification(arrays[name])\n",
    "            else:\n",
    "                # Otherwise, replace the array\n",
    "                arrays[name] = modification\n",
    "        else:\n",
    "            # Add new array\n",
    "            arrays[name] = modification\n",
    "            print(f\"Adding new array '{name}' to the file\")\n",
    "    \n",
    "    # Save back to the file with same format\n",
    "    np.savez(file_path, **arrays)\n",
    "    \n",
    "    print(f\"Successfully modified/added {len(modifications)} arrays in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_undetected_hand_landmarks(directory_path):  \n",
    "    \"\"\"\n",
    "    Interpolate landmarks for frames where hand detection failed.\n",
    "    \"\"\"\n",
    "    print(f\"Starting interpolation for directory: {directory_path}\")\n",
    "    \n",
    "    # Load detection statistics JSON\n",
    "    with open(os.path.join(directory_path, 'detection_statistics.json')) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    first_frame_number = round(data['video_info']['fps'] * data['video_info']['start_time'])\n",
    "    final_frame_number = round(data['video_info']['fps'] * data['video_info']['end_time'])\n",
    "    \n",
    "    print(f\"Processing frames range: {first_frame_number} to {final_frame_number}\")\n",
    "    \n",
    "    # Maximum possible sum of weights for normalization (when all 10 frames are available)\n",
    "    MAX_WEIGHT_SUM = 2.92722222\n",
    "    \n",
    "    # Process non-dominant hand failures\n",
    "    print(\"Processing non-dominant hand failures...\")\n",
    "    missing_non_dominant_frame_list = [frame['frame'] for frame in data['failed_frames']['non_dominant_hand_failures']]\n",
    "    \n",
    "    non_dom_interpolated_count = 0\n",
    "    \n",
    "    for missing_frame in data['failed_frames']['non_dominant_hand_failures']:\n",
    "        frame_number = missing_frame['frame']\n",
    "        filepath = missing_frame['file']\n",
    "        \n",
    "        # Only interpolate frames not at the edges of the video\n",
    "        if (frame_number - 5) <= first_frame_number or (frame_number + 5) >= final_frame_number:\n",
    "            print(f\"Skipping frame {frame_number} - too close to video boundary\")\n",
    "            continue\n",
    "        \n",
    "        # Find frames with valid detections for interpolation\n",
    "        interpolation_frames = find_interpolation_frames(frame_number, missing_non_dominant_frame_list)\n",
    "        \n",
    "        if not interpolation_frames:\n",
    "            print(f\"No valid frames found for interpolating frame {frame_number}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate interpolated landmarks\n",
    "        interpolation_weights_sum = 0\n",
    "        interpolated_coordinates = np.zeros(shape=(20, 3))\n",
    "        interpolated_wrist_to_nose = np.zeros(2)\n",
    "        \n",
    "        for interp_frame in interpolation_frames:\n",
    "            weight = 1 / ((frame_number - interp_frame) ** 2)\n",
    "            interpolation_weights_sum += weight\n",
    "            \n",
    "            # Find and load the reference frame\n",
    "            interp_partial_filename = data['video_info']['name'] + f\"_frame{interp_frame:06d}\"\n",
    "            try:\n",
    "                interp_files = find_file_with_partial_name(\n",
    "                    interp_partial_filename, \n",
    "                    search_dir=directory_path, \n",
    "                    recursive=False\n",
    "                )\n",
    "                \n",
    "                if not interp_files:\n",
    "                    print(f\"Warning: Could not find file for frame {interp_frame}\")\n",
    "                    continue\n",
    "                    \n",
    "                interp_filepath = interp_files[0]\n",
    "                \n",
    "                # Load the frame data - index 1 for non-dominant hand landmarks\n",
    "                frame_data = load_frame_data(interp_filepath)\n",
    "                non_dom_landmarks = frame_data[1]  # Correct index for non-dominant hand\n",
    "                nose_to_wrist_non_dom = frame_data[7][1, :]\n",
    "                \n",
    "                \n",
    "                # Add weighted contribution\n",
    "                interpolated_coordinates += weight * non_dom_landmarks\n",
    "                interpolated_wrist_to_nose += weight * nose_to_wrist_non_dom\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {interp_frame}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize by sum of weights (crucial step!)\n",
    "        if interpolation_weights_sum > 0:\n",
    "            interpolated_coordinates /= interpolation_weights_sum\n",
    "            interpolated_wrist_to_nose /= interpolation_weights_sum\n",
    "            \n",
    "            # Calculate confidence based on weights and frame distribution\n",
    "            has_frames_on_both_sides = has_numbers_on_both_sides(frame_number, interpolation_frames)\n",
    "            \n",
    "            if has_frames_on_both_sides:\n",
    "                interpolation_confidence = interpolation_weights_sum / MAX_WEIGHT_SUM\n",
    "            else:\n",
    "                interpolation_confidence = (interpolation_weights_sum / MAX_WEIGHT_SUM) * 0.8\n",
    "                \n",
    "            print(f\"Frame {frame_number}: Interpolated with confidence {interpolation_confidence:.2f}\")\n",
    "            \n",
    "            # Update the file with interpolated data\n",
    "            def update_interp_scores(arr):\n",
    "                new_arr = arr.copy()\n",
    "                new_arr[1] = interpolation_confidence  # Index 1 for non-dominant hand\n",
    "                return new_arr\n",
    "            \n",
    "            def update_nose_to_wrist_scores(matrix):\n",
    "                new_matrix = matrix.copy()\n",
    "                new_matrix[1, :] = interpolated_wrist_to_nose\n",
    "                return new_matrix\n",
    "                \n",
    "            modifications = {\n",
    "                'non_dom_landmarks': interpolated_coordinates,\n",
    "                'interpolation_scores': update_interp_scores,\n",
    "                'nose_to_wrist_dist': update_nose_to_wrist_scores\n",
    "            }\n",
    "            \n",
    "            modify_npz_file(\n",
    "                file_path=os.path.join(directory_path, filepath),\n",
    "                modifications=modifications\n",
    "            )\n",
    "            \n",
    "            non_dom_interpolated_count += 1\n",
    "    \n",
    "    # Process dominant hand failures\n",
    "    print(f\"Interpolated {non_dom_interpolated_count} non-dominant hand frames\")\n",
    "    print(\"Processing dominant hand failures...\")\n",
    "    \n",
    "    missing_dominant_frame_list = [frame['frame'] for frame in data['failed_frames']['dominant_hand_failures']]\n",
    "    \n",
    "    dom_interpolated_count = 0\n",
    "    \n",
    "    for missing_frame in data['failed_frames']['dominant_hand_failures']:\n",
    "        frame_number = missing_frame['frame']\n",
    "        filepath = missing_frame['file']\n",
    "        \n",
    "        # Only interpolate frames not at the edges of the video\n",
    "        if (frame_number - 5) <= first_frame_number or (frame_number + 5) >= final_frame_number:\n",
    "            continue\n",
    "        \n",
    "        # Find frames with valid detections for interpolation\n",
    "        interpolation_frames = find_interpolation_frames(frame_number, missing_dominant_frame_list)\n",
    "        \n",
    "        if not interpolation_frames:\n",
    "            continue\n",
    "        \n",
    "        # Calculate interpolated landmarks\n",
    "        interpolation_weights_sum = 0\n",
    "        interpolated_coordinates = np.zeros(shape=(20, 3))\n",
    "        interpolated_wrist_to_nose = np.zeros(2)\n",
    "        \n",
    "        for interp_frame in interpolation_frames:\n",
    "            weight = 1 / ((frame_number - interp_frame) ** 2)\n",
    "            interpolation_weights_sum += weight\n",
    "            \n",
    "            # Find and load the reference frame\n",
    "            interp_partial_filename = data['video_info']['name'] + f\"_frame{interp_frame:06d}\"\n",
    "            try:\n",
    "                interp_files = find_file_with_partial_name(\n",
    "                    interp_partial_filename, \n",
    "                    search_dir=directory_path, \n",
    "                    recursive=False\n",
    "                )\n",
    "                \n",
    "                if not interp_files:\n",
    "                    continue\n",
    "                    \n",
    "                interp_filepath = interp_files[0]\n",
    "                \n",
    "                # Load the frame data - index 0 for dominant hand landmarks\n",
    "                frame_data = load_frame_data(interp_filepath)\n",
    "                dom_landmarks = frame_data[0]  # Correct index for dominant hand\n",
    "                nose_to_wrist_dom = frame_data[7][0, :]\n",
    "                \n",
    "                # Add weighted contribution\n",
    "                interpolated_coordinates += weight * dom_landmarks\n",
    "                interpolated_wrist_to_nose += weight * nose_to_wrist_non_dom\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {interp_frame}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize by sum of weights\n",
    "        if interpolation_weights_sum > 0:\n",
    "            interpolated_coordinates /= interpolation_weights_sum\n",
    "            interpolated_wrist_to_nose /= interpolation_weights_sum\n",
    "            # Calculate confidence based on weights and frame distribution\n",
    "            has_frames_on_both_sides = has_numbers_on_both_sides(frame_number, interpolation_frames)\n",
    "            \n",
    "            if has_frames_on_both_sides:\n",
    "                interpolation_confidence = interpolation_weights_sum / MAX_WEIGHT_SUM\n",
    "            else:\n",
    "                interpolation_confidence = (interpolation_weights_sum / MAX_WEIGHT_SUM) * 0.8\n",
    "            \n",
    "            # Update the file with interpolated data\n",
    "            def update_interp_scores(arr):\n",
    "                new_arr = arr.copy()\n",
    "                new_arr[0] = interpolation_confidence  # Index 0 for dominant hand\n",
    "                return new_arr\n",
    "            \n",
    "            def update_nose_to_wrist_scores(matrix):\n",
    "                new_matrix = matrix.copy()\n",
    "                new_matrix[0, :] = interpolated_wrist_to_nose\n",
    "                return new_matrix\n",
    "                \n",
    "            modifications = {\n",
    "                'dom_landmarks': interpolated_coordinates,\n",
    "                'interpolation_scores': update_interp_scores,\n",
    "                'nose_to_wrist_dist': update_nose_to_wrist_scores\n",
    "            }\n",
    "            \n",
    "\n",
    "            modify_npz_file(\n",
    "                file_path=os.path.join(directory_path, filepath),\n",
    "                modifications=modifications\n",
    "            )\n",
    "            \n",
    "            dom_interpolated_count += 1\n",
    "    \n",
    "    print(f\"Interpolated {dom_interpolated_count} dominant hand frames\")\n",
    "    print(f\"Total interpolated: {non_dom_interpolated_count + dom_interpolated_count} frames\")\n",
    "    \n",
    "    return non_dom_interpolated_count + dom_interpolated_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting interpolation for directory: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\n",
      "Processing frames range: 30 to 60\n",
      "Processing non-dominant hand failures...\n",
      "Frame 36: Interpolated with confidence 0.94\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\n",
      "Frame 39: Interpolated with confidence 0.62\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\n",
      "Frame 40: Interpolated with confidence 0.64\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms.npz\n",
      "Frame 54: Interpolated with confidence 1.00\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms.npz\n",
      "Interpolated 4 non-dominant hand frames\n",
      "Processing dominant hand failures...\n",
      "Interpolated 0 dominant hand frames\n",
      "Total interpolated: 4 frames\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolate_undetected_hand_landmarks(directory_path=\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = load_frame_data(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_valid_detection(frame_data, is_dominant_hand):\n",
    "    \"\"\"\n",
    "    Check if the frame has valid detection (not interpolated) for a specific hand.\n",
    "    \n",
    "    Args:\n",
    "        frame_data: The loaded frame data\n",
    "        is_dominant_hand: If True, check dominant hand; if False, check non-dominant hand;\n",
    "                         if None, check if either hand is detected\n",
    "    \n",
    "    Returns:\n",
    "        bool: Whether the specified hand(s) is/are detected\n",
    "    \"\"\"\n",
    "    detection_status = frame_data[4]\n",
    "    \n",
    "    if is_dominant_hand:\n",
    "        # Check specifically for dominant hand\n",
    "        return detection_status[0] == 1\n",
    "    else:\n",
    "        # Check specifically for non-dominant hand\n",
    "        return detection_status[1] == 1\n",
    "    \n",
    "\n",
    "\n",
    "def has_value(frame_data, is_dominant_hand):\n",
    "    \"\"\"Check if the frame exists and has any value (detection or interpolation)\"\"\"\n",
    "    detection_status = frame_data[4]\n",
    "    interpolation_scores = frame_data[3]\n",
    "    if is_dominant_hand:\n",
    "        return (detection_status[0]==1) or (interpolation_scores[0]>0)\n",
    "    else:\n",
    "        return (detection_status[1]==1) or (interpolation_scores[1]>0)\n",
    "    \n",
    "        \n",
    "\n",
    "def cartesian_to_spherical(velocities):\n",
    "    \"\"\"\n",
    "    Convert Cartesian velocities (ux, uy, uz) to spherical coordinate features.\n",
    "    \n",
    "    Args:\n",
    "        velocities: NumPy array of shape (20, 3) with Cartesian velocities\n",
    "        \n",
    "    Returns:\n",
    "        NumPy array of shape (20, 5) with spherical features:\n",
    "            [vmagnitude, ϕsin, ϕcos, θsin, θcos]\n",
    "    \"\"\"\n",
    "    num_landmarks = velocities.shape[0]\n",
    "    spherical_features = np.zeros((num_landmarks, 5))\n",
    "    \n",
    "    for i in range(num_landmarks):\n",
    "        ux, uy, uz = velocities[i]\n",
    "        \n",
    "        # Calculate velocity magnitude\n",
    "        vmagnitude = np.sqrt(ux**2 + uy**2 + uz**2)\n",
    "        spherical_features[i, 0] = vmagnitude\n",
    "        \n",
    "        # Handle edge cases to avoid division by zero\n",
    "        if vmagnitude == 0:\n",
    "            # If velocity is zero, set all angles to zero\n",
    "            spherical_features[i, 1:] = 0\n",
    "            continue\n",
    "        \n",
    "        # Calculate azimuth angle (ϕ)\n",
    "        phi = np.arctan2(uy, ux)\n",
    "        spherical_features[i, 1] = np.sin(phi)  # ϕsin\n",
    "        spherical_features[i, 2] = np.cos(phi)  # ϕcos\n",
    "        \n",
    "        # Calculate elevation angle (θ)\n",
    "        # Clamp uz/vmagnitude to range [-1, 1] to avoid numerical errors\n",
    "        cos_theta = np.clip(uz / vmagnitude, -1.0, 1.0)\n",
    "        theta = np.arccos(cos_theta)\n",
    "        spherical_features[i, 3] = np.sin(theta)  # θsin\n",
    "        spherical_features[i, 4] = cos_theta      # θcos (already calculated)\n",
    "    \n",
    "    return spherical_features\n",
    "\n",
    "\n",
    "\n",
    "def cartesian_to_polar_features(velocities):\n",
    "    \"\"\"\n",
    "    Convert Cartesian velocity coordinates to polar features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    velocities : numpy.ndarray\n",
    "        Array of shape (2, 2) where each row represents an object's [Ux, Uy]\n",
    "\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of shape (2, 3) with columns [magnitude, sin(direction), cos(direction)]\n",
    "    \"\"\"\n",
    "    # Calculate magnitude\n",
    "    magnitude = np.sqrt(np.sum(velocities**2, axis=1))\n",
    "    \n",
    "    # Initialize result array\n",
    "    result = np.zeros((velocities.shape[0], 3))\n",
    "    result[:, 0] = magnitude  # Set first column to magnitude\n",
    "    \n",
    "    # Create a mask for non-zero magnitudes\n",
    "    non_zero = magnitude > 0\n",
    "    \n",
    "    # For non-zero magnitudes, calculate direction components\n",
    "    if np.any(non_zero):\n",
    "        # Get direction for non-zero magnitudes\n",
    "        direction = np.arctan2(velocities[non_zero, 1], velocities[non_zero, 0])\n",
    "        \n",
    "        # Calculate sin and cos\n",
    "        result[non_zero, 1] = np.sin(direction)  # sin(direction)\n",
    "        result[non_zero, 2] = np.cos(direction)  # cos(direction)\n",
    "    \n",
    "    # Handle zero magnitudes \n",
    "    zero_indices = ~non_zero\n",
    "    if np.any(zero_indices):\n",
    "        result[zero_indices, 1] = 0.0\n",
    "        result[zero_indices, 2] = 0.0\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_landmark_velocities(directory_path):\n",
    "    \"\"\"\n",
    "    Compute velocity features for hand landmarks using central differencing with two window sizes,\n",
    "    and convert to spherical coordinates.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing frame NPZ files\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of frames processed\n",
    "    \"\"\"\n",
    "    # List all NPZ files in the directory\n",
    "    npz_files = sorted(glob.glob(os.path.join(directory_path, \"*.npz\")))\n",
    "    \n",
    "    # Skip if no files found\n",
    "    if not npz_files:\n",
    "        print(f\"No NPZ files found in {directory_path}\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Computing velocities for {len(npz_files)} files...\")\n",
    "    \n",
    "    # Create a mapping of frame indices to file paths\n",
    "    frame_to_file = {}\n",
    "    for file_path in npz_files:\n",
    "        frame_data = load_frame_data(file_path)\n",
    "        frame_idx = frame_data[8]  # Index for frame_idx\n",
    "        frame_to_file[frame_idx] = file_path\n",
    "    \n",
    "    frame_indices = sorted(frame_to_file.keys())\n",
    "    processed_count = 0\n",
    "    \n",
    "\n",
    "    min_frame = min(frame_indices)\n",
    "    max_frame = max(frame_indices)\n",
    "    safe_margin = 5  # Skip processing frames within 5 frames of the edge\n",
    "    \n",
    "    # Process each frame\n",
    "    for i, curr_idx in enumerate(frame_indices):\n",
    "        if curr_idx < min_frame + safe_margin or curr_idx > max_frame - safe_margin:\n",
    "            dom_velocity_small = np.zeros((20, 5))\n",
    "            dom_velocity_large = np.zeros((20, 5))\n",
    "            non_dom_velocity_small = np.zeros((20, 5))\n",
    "            non_dom_velocity_large = np.zeros((20, 5))\n",
    "            \n",
    "            wrist_velocity_small = np.zeros((2, 3))  \n",
    "            wrist_velocity_large = np.zeros((2, 3))\n",
    "            \n",
    "            # Create zero confidence arrays\n",
    "            velocity_confidence = np.zeros(2)\n",
    "            velocity_calculation_confidence = np.zeros(2)\n",
    "            \n",
    "            # Save these zero arrays\n",
    "            modifications = {\n",
    "                'dom_velocity_small': dom_velocity_small,\n",
    "                'dom_velocity_large': dom_velocity_large,\n",
    "                'non_dom_velocity_small': non_dom_velocity_small,\n",
    "                'non_dom_velocity_large': non_dom_velocity_large,\n",
    "                'velocity_confidence': velocity_confidence,\n",
    "                'velocity_calculation_confidence': velocity_calculation_confidence,\n",
    "                'wrist_velocity_small': wrist_velocity_small,\n",
    "                'wrist_velocity_large': wrist_velocity_large,\n",
    "            }\n",
    "            \n",
    "            # Get the file path for this frame\n",
    "            current_file_path = frame_to_file[curr_idx]\n",
    "            modify_npz_file(current_file_path, modifications)\n",
    "            processed_count += 1\n",
    "            \n",
    "            # Log that we're skipping calculation\n",
    "            print(f\"Frame {curr_idx} too close to video boundary - setting zero velocities\")\n",
    "            continue\n",
    "        # Load current frame\n",
    "        current_file_path = frame_to_file[curr_idx]\n",
    "        curr_frame_data = load_frame_data(current_file_path)\n",
    "        \n",
    "        # Store needed frames in a dictionary for easy access\n",
    "        frame_cache = {curr_idx: curr_frame_data}\n",
    "        \n",
    "        # Load all potentially needed frames in the -5 to +5 range\n",
    "        for offset in range(-5, 6):\n",
    "            if offset == 0:  # Skip current frame (already loaded)\n",
    "                continue\n",
    "            \n",
    "            check_idx = curr_idx + offset\n",
    "            if check_idx in frame_to_file:\n",
    "                frame_cache[check_idx] = load_frame_data(frame_to_file[check_idx])\n",
    "            else:\n",
    "                frame_cache[check_idx] = None  # Mark as not available\n",
    "        \n",
    "        # Extract dominant and non-dominant hand landmarks from current frame\n",
    "        dom_landmarks = curr_frame_data[0]\n",
    "        non_dom_landmarks = curr_frame_data[1]\n",
    "        \n",
    "        # Initialize velocity arrays in Cartesian coordinates\n",
    "        dom_velocity_small_cart = np.zeros_like(dom_landmarks)\n",
    "        dom_velocity_large_cart = np.zeros_like(dom_landmarks)\n",
    "        non_dom_velocity_small_cart = np.zeros_like(non_dom_landmarks)\n",
    "        non_dom_velocity_large_cart = np.zeros_like(non_dom_landmarks)\n",
    "        \n",
    "        wrist_velocity_small = np.zeros((2, 2))  # 2 hands × [x, y] coordinates\n",
    "        wrist_velocity_large = np.zeros((2, 2))  # 2 hands × [x, y] coordinates\n",
    "        \n",
    "        # Initialize confidence and method weight tracking\n",
    "        dom_small_conf = 0.0\n",
    "        dom_large_conf = 0.0\n",
    "        non_dom_small_conf = 0.0\n",
    "        non_dom_large_conf = 0.0\n",
    "        \n",
    "        dom_small_method_weight = 0.0\n",
    "        dom_large_method_weight = 0.0\n",
    "        non_dom_small_method_weight = 0.0\n",
    "        non_dom_large_method_weight = 0.0\n",
    "        \n",
    "        dom_small_source_quality = 0.0\n",
    "        dom_large_source_quality = 0.0\n",
    "        non_dom_small_source_quality = 0.0\n",
    "        non_dom_large_source_quality = 0.0\n",
    "        \n",
    "        # ===== DOMINANT HAND VELOCITY CALCULATION =====\n",
    "        \n",
    "        # Small window [-1, +1] velocity with fallbacks\n",
    "        if (curr_idx + 1 in frame_cache and frame_cache[curr_idx + 1] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 1], True) and \n",
    "            curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "            has_value(frame_cache[curr_idx - 1], True)):\n",
    "            # Ideal case: (t+1, t-1)\n",
    "            dom_velocity_small_cart = (frame_cache[curr_idx + 1][0] - frame_cache[curr_idx - 1][0]) / 2.0\n",
    "            wrist_velocity_small[0, :] = (frame_cache[curr_idx + 1][7][0, :] - frame_cache[curr_idx - 1][7][0, :]) / 2.0\n",
    "            dom_small_conf = min(frame_cache[curr_idx + 1][2][0], frame_cache[curr_idx - 1][2][0])  # Detection confidence of t+1 frame\n",
    "            dom_small_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor (average of interpolation confidences)\n",
    "            t_plus_1_interp = frame_cache[curr_idx + 1][3][0]\n",
    "            t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "            dom_small_source_quality = (t_plus_1_interp + t_minus_1_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "              is_valid_detection(frame_cache[curr_idx + 2], True) and \n",
    "              curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "              has_value(frame_cache[curr_idx - 2], True)):\n",
    "            # Fallback 1: (t+2, t-2)\n",
    "            dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - frame_cache[curr_idx - 2][0]) / 4.0\n",
    "            wrist_velocity_small[0, :] = (frame_cache[curr_idx + 2][7][0, :] - frame_cache[curr_idx - 2][7][0, :]) / 4.0\n",
    "            dom_small_conf = min(frame_cache[curr_idx + 2][2][0], frame_cache[curr_idx - 2][2][0])  # Detection confidence of t+2 frame\n",
    "            dom_small_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "            t_minus_2_interp = frame_cache[curr_idx - 2][3][0]\n",
    "            dom_small_source_quality = (t_plus_2_interp + t_minus_2_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "              is_valid_detection(frame_cache[curr_idx + 2], True)):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], True)):\n",
    "                # Fallback 2: (t+2, t-1)\n",
    "                dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - frame_cache[curr_idx - 1][0]) / 3.0\n",
    "                wrist_velocity_small[0, :] = (frame_cache[curr_idx + 2][7][0, :] - frame_cache[curr_idx - 1][7][0, :]) / 3.0\n",
    "                dom_small_conf = min(frame_cache[curr_idx + 2][2][0], frame_cache[curr_idx - 1][2][0])  # Detection confidence of t+2 frame\n",
    "                dom_small_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "                dom_small_source_quality = (t_plus_2_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif is_valid_detection(curr_frame_data, True):\n",
    "                # Fallback 3: (t+2, t)\n",
    "                dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - curr_frame_data[0]) / 2.0\n",
    "                wrist_velocity_small[0, :] = (frame_cache[curr_idx + 2][7][0, :] - curr_frame_data[7][0, :]) / 2.0\n",
    "                dom_small_conf = min(frame_cache[curr_idx + 2][2][0], curr_frame_data[2][0])\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                dom_small_source_quality = (t_plus_2_interp + t_interp) / 2.0\n",
    "                \n",
    "        elif is_valid_detection(curr_frame_data, True):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], True)):\n",
    "                # Fallback 4: (t, t-1)\n",
    "                dom_velocity_small_cart = (curr_frame_data[0] - frame_cache[curr_idx - 1][0])\n",
    "                wrist_velocity_small[0, :] = (curr_frame_data[7][0, :] - frame_cache[curr_idx - 1][7][0, :]) \n",
    "                dom_small_conf = min(curr_frame_data[2][0], frame_cache[curr_idx - 1][2][0])  # Detection confidence of current frame\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "                dom_small_source_quality = (t_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "                  has_value(frame_cache[curr_idx - 2], True)):\n",
    "                # Fallback 5: (t, t-2)\n",
    "                dom_velocity_small_cart = (curr_frame_data[0] - frame_cache[curr_idx - 2][0]) / 2.0\n",
    "                wrist_velocity_small[0, :] = (curr_frame_data[7][0, :] - frame_cache[curr_idx - 2][7][0, :]) / 2.0\n",
    "                dom_small_conf = min(curr_frame_data[2][0], frame_cache[curr_idx - 2][2][0])  # Detection confidence of current frame\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                t_minus_2_interp = frame_cache[curr_idx - 2][3][0]\n",
    "                dom_small_source_quality = (t_interp + t_minus_2_interp) / 2.0\n",
    "        \n",
    "        # Large window [-5, +5] velocity with fallbacks\n",
    "\n",
    "        if (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], True) and \n",
    "            curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "            has_value(frame_cache[curr_idx - 5], True)):\n",
    "            # Ideal case: (t+5, t-5)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 5][0]) / 10.0\n",
    "            wrist_velocity_large[0, :] = (frame_cache[curr_idx + 5][7][0, :] - frame_cache[curr_idx - 5][7][0, :]) / 10.0\n",
    "            dom_large_conf = min(frame_cache[curr_idx + 5][2][0], frame_cache[curr_idx - 5][2][0])  # Detection confidence of t+5 frame\n",
    "            dom_large_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "            t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "            dom_large_source_quality = (t_plus_5_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "        is_valid_detection(frame_cache[curr_idx + 4], True) and \n",
    "        curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "        has_value(frame_cache[curr_idx - 4], True)):\n",
    "        # Fallback 1: (t+4, t-4)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 4][0]) / 8.0\n",
    "            wrist_velocity_large[0, :] = (frame_cache[curr_idx + 4][7][0, :] - frame_cache[curr_idx - 4][7][0, :]) / 8.0\n",
    "            dom_large_conf = min(frame_cache[curr_idx + 4][2][0], frame_cache[curr_idx - 4][2][0]) # Detection confidence of t+4 frame\n",
    "            dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "            dom_large_source_quality = (t_plus_4_interp + t_minus_4_interp) / 2.0\n",
    "    \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], True) and \n",
    "            curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "            has_value(frame_cache[curr_idx - 3], True)):\n",
    "            # Fallback 2: (t+3, t-3)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 3][0]) / 6.0\n",
    "            wrist_velocity_large[0, :] = (frame_cache[curr_idx + 3][7][0, :] - frame_cache[curr_idx - 3][7][0, :]) / 6.0\n",
    "            dom_large_conf = min(frame_cache[curr_idx + 3][2][0], frame_cache[curr_idx - 3][2][0])  # Detection confidence of t+3 frame\n",
    "            dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "            t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "            dom_large_source_quality = (t_plus_3_interp + t_minus_3_interp) / 2.0\n",
    "            \n",
    "        # Asymmetric fallbacks for large window\n",
    "        elif (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], True)):\n",
    "            if (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4], True)):\n",
    "                # Fallback 3: (t+5, t-4)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 4][0]) / 9.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 5][7][0, :] - frame_cache[curr_idx - 4][7][0, :]) / 9.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 5][2][0], frame_cache[curr_idx - 4][2][0])  # Detection confidence of t+5 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "                dom_large_source_quality = (t_plus_5_interp + t_minus_4_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], True)):\n",
    "                # Fallback 4: (t+5, t-3)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 3][0]) / 8.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 5][7][0, :] - frame_cache[curr_idx - 3][7][0, :]) / 8.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 5][2][0], frame_cache[curr_idx - 3][2][0])  # Detection confidence of t+5 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "                dom_large_source_quality = (t_plus_5_interp + t_minus_3_interp) / 2.0\n",
    "        \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], True)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], True)):\n",
    "                # Fallback 5: (t+4, t-5)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 5][0]) / 9.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 4][7][0, :] - frame_cache[curr_idx - 5][7][0, :]) / 9.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 4][2][0], frame_cache[curr_idx - 5][2][0])  # Detection confidence of t+4 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "                dom_large_source_quality = (t_plus_4_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], True)):\n",
    "                # Fallback 6: (t+4, t-3)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 3][0]) / 7.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 4][7][0, :] - frame_cache[curr_idx - 3][7][0, :]) / 7.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 4][2][0], frame_cache[curr_idx - 3][2][0])  # Detection confidence of t+4 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "                dom_large_source_quality = (t_plus_4_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], True)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], True)):\n",
    "                # Fallback 7: (t+3, t-5)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 5][0]) / 8.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 3][7][0, :] - frame_cache[curr_idx - 5][7][0, :]) / 8.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 3][2][0], frame_cache[curr_idx - 5][2][0])  # Detection confidence of t+3 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "                dom_large_source_quality = (t_plus_3_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4], True)):\n",
    "                # Fallback 8: (t+3, t-4)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 4][0]) / 7.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 3][7][0, :] - frame_cache[curr_idx - 4][7][0, :]) / 7.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 3][2][0], frame_cache[curr_idx - 4][2][0])  # Detection confidence of t+3 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "                dom_large_source_quality = (t_plus_3_interp + t_minus_4_interp) / 2.0\n",
    "\n",
    "        # ===== NON-DOMINANT HAND VELOCITY CALCULATION =====\n",
    "\n",
    "    # Small window [-1, +1] velocity with fallbacks for non-dominant hand\n",
    "        if (curr_idx + 1 in frame_cache and frame_cache[curr_idx + 1] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 1], False) and \n",
    "            curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "            has_value(frame_cache[curr_idx - 1], False)):\n",
    "            # Ideal case: (t+1, t-1)\n",
    "            non_dom_velocity_small_cart = (frame_cache[curr_idx + 1][1] - frame_cache[curr_idx - 1][1]) / 2.0\n",
    "            wrist_velocity_small[1, :] = (frame_cache[curr_idx + 1][7][1, :] - frame_cache[curr_idx - 1][7][1, :]) / 2.0\n",
    "            non_dom_small_conf = min(frame_cache[curr_idx + 1][2][1], frame_cache[curr_idx - 1][2][1])  # Detection confidence of t+1 frame\n",
    "            non_dom_small_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_1_interp = frame_cache[curr_idx + 1][3][1]\n",
    "            t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "            non_dom_small_source_quality = (t_plus_1_interp + t_minus_1_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 2], False) and \n",
    "            curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "            has_value(frame_cache[curr_idx - 2], False)):\n",
    "            # Fallback 1: (t+2, t-2)\n",
    "            non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - frame_cache[curr_idx - 2][1]) / 4.0\n",
    "            wrist_velocity_small[1, :] = (frame_cache[curr_idx + 2][7][1, :] - frame_cache[curr_idx - 2][7][1, :]) / 4.0\n",
    "            non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], frame_cache[curr_idx - 2][2][1]) \n",
    "            non_dom_small_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "            t_minus_2_interp = frame_cache[curr_idx - 2][3][1]\n",
    "            non_dom_small_source_quality = (t_plus_2_interp + t_minus_2_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 2], False)):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], False)):\n",
    "                # Fallback 2: (t+2, t-1)\n",
    "                non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - frame_cache[curr_idx - 1][1]) / 3.0\n",
    "                wrist_velocity_small[1, :] = (frame_cache[curr_idx + 2][7][1, :] - frame_cache[curr_idx - 2][7][1, :]) / 3.0\n",
    "                non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], frame_cache[curr_idx - 1][2][1])  \n",
    "                non_dom_small_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "                non_dom_small_source_quality = (t_plus_2_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif is_valid_detection(curr_frame_data, False):\n",
    "                # Fallback 3: (t+2, t)\n",
    "                non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - curr_frame_data[1]) / 2.0\n",
    "                wrist_velocity_small[1, :] = (frame_cache[curr_idx + 2][7][1, :] - curr_frame_data[7][1, :]) / 2.0\n",
    "                non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], curr_frame_data[2][1])\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                non_dom_small_source_quality = (t_plus_2_interp + t_interp) / 2.0\n",
    "                \n",
    "        elif is_valid_detection(curr_frame_data, False):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], False)):\n",
    "                # Fallback 4: (t, t-1)\n",
    "                non_dom_velocity_small_cart = (curr_frame_data[1] - frame_cache[curr_idx - 1][1])\n",
    "                wrist_velocity_small[1, :] = (curr_frame_data[7][1, :] - frame_cache[curr_idx - 1][7][1, :]) \n",
    "                non_dom_small_conf = min(curr_frame_data[2][1], frame_cache[curr_idx - 1][2][1])  # Detection confidence of current frame\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "                non_dom_small_source_quality = (t_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "                has_value(frame_cache[curr_idx - 2], False)):\n",
    "                # Fallback 5: (t, t-2)\n",
    "                non_dom_velocity_small_cart = (curr_frame_data[1] - frame_cache[curr_idx - 2][1]) / 2.0\n",
    "                wrist_velocity_small[1, :] = (curr_frame_data[7][1, :] - frame_cache[curr_idx -21][7][1, :]) / 2.0\n",
    "                non_dom_small_conf = min(curr_frame_data[2][1], frame_cache[curr_idx -2][2][1])  # Detection confidence of current frame\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                t_minus_2_interp = frame_cache[curr_idx - 2][3][1]\n",
    "                non_dom_small_source_quality = (t_interp + t_minus_2_interp) / 2.0\n",
    "    \n",
    "        # Large window [-5, +5] velocity with fallbacks for non-dominant hand\n",
    "        if (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], False) and \n",
    "            curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "            has_value(frame_cache[curr_idx - 5], False)):\n",
    "            # Ideal case: (t+5, t-5)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 5][1]) / 10.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 5][7][1, :] - frame_cache[curr_idx - 5][7][1, :]) / 10.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 5][2][1], frame_cache[curr_idx - 5][2][1])  \n",
    "            non_dom_large_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "            t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_5_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], False) and \n",
    "            curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "            has_value(frame_cache[curr_idx - 4], False)):\n",
    "            # Fallback 1: (t+4, t-4)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 4][1]) / 8.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 4][7][1, :] - frame_cache[curr_idx - 4][7][1, :]) / 8.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 4][2][1], frame_cache[curr_idx - 4][2][1])  \n",
    "            non_dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_4_interp + t_minus_4_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], False) and \n",
    "            curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "            has_value(frame_cache[curr_idx - 3], False)):\n",
    "            # Fallback 2: (t+3, t-3)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 3][1]) / 6.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 3][7][1, :] - frame_cache[curr_idx - 3][7][1, :]) / 6.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 3][2][1], frame_cache[curr_idx - 3][2][1])  # Detection confidence of t+3 frame\n",
    "            non_dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "            t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_3_interp + t_minus_3_interp) / 2.0\n",
    "            \n",
    "        # Asymmetric fallbacks for large window\n",
    "        elif (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], False)):\n",
    "            if (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4], False)):\n",
    "                # Fallback 3: (t+5, t-4)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 4][1]) / 9.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 5][7][1, :] - frame_cache[curr_idx - 4][7][1, :]) / 9.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 5][2][1], frame_cache[curr_idx - 4][2][1])  # Detection confidence of t+5 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_5_interp + t_minus_4_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], False)):\n",
    "                # Fallback 4: (t+5, t-3)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 3][1]) / 8.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 5][7][1, :] - frame_cache[curr_idx - 3][7][1, :]) / 8.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 5][2][1], frame_cache[curr_idx - 3][2][1])  # Detection confidence of t+5 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_5_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], False)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], False)):\n",
    "                # Fallback 5: (t+4, t-5)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 5][1]) / 9.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 4][7][1, :] - frame_cache[curr_idx - 5][7][1, :]) / 9.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 4][2][1], frame_cache[curr_idx - 5][2][1])  # Detection confidence of t+4 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_4_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], False)):\n",
    "                # Fallback 6: (t+4, t-3)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 3][1]) / 7.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 4][7][1, :] - frame_cache[curr_idx - 3][7][1, :]) / 7.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 4][2][1], frame_cache[curr_idx - 3][2][1])  # Detection confidence of t+4 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_4_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], False)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], False)):\n",
    "                # Fallback 7: (t+3, t-5)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 5][1]) / 8.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 3][7][1, :] - frame_cache[curr_idx - 5][7][1, :]) / 8.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 3][2][1], frame_cache[curr_idx - 5][2][1])  # Detection confidence of t+3 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_3_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "            has_value(frame_cache[curr_idx - 4], False)):\n",
    "            # Fallback 8: (t+3, t-4)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 4][1]) / 7.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 3][7][1, :] - frame_cache[curr_idx - 4][7][1, :]) / 7.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 3][2][1], frame_cache[curr_idx - 4][2][1])  # Detection confidence of t+3 frame\n",
    "            non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_3_interp + t_minus_4_interp) / 2.0\n",
    "            \n",
    "\n",
    "        \n",
    "        # Convert Cartesian velocities to spherical/polar coordinates\n",
    "        dom_velocity_small = cartesian_to_spherical(dom_velocity_small_cart)\n",
    "        dom_velocity_large = cartesian_to_spherical(dom_velocity_large_cart)\n",
    "        non_dom_velocity_small = cartesian_to_spherical(non_dom_velocity_small_cart)\n",
    "        non_dom_velocity_large = cartesian_to_spherical(non_dom_velocity_large_cart)\n",
    "        \n",
    "        wrist_velocity_small_polar = cartesian_to_polar_features(wrist_velocity_small)\n",
    "        wrist_velocity_large_polar = cartesian_to_polar_features(wrist_velocity_large)\n",
    "        \n",
    "        # Calculate average confidence for each hand across both windows\n",
    "        dom_avg_conf = (dom_small_conf + dom_large_conf) / 2.0\n",
    "        non_dom_avg_conf = (non_dom_small_conf + non_dom_large_conf) / 2.0\n",
    "        \n",
    "        # Calculate velocityCalculationConfidence using method weight and source quality\n",
    "        dom_small_vel_calc_conf = dom_small_method_weight * dom_small_source_quality\n",
    "        dom_large_vel_calc_conf = dom_large_method_weight * dom_large_source_quality\n",
    "        non_dom_small_vel_calc_conf = non_dom_small_method_weight * non_dom_small_source_quality\n",
    "        non_dom_large_vel_calc_conf = non_dom_large_method_weight * non_dom_large_source_quality\n",
    "        \n",
    "        # Average across windows for each hand\n",
    "        dom_vel_calc_conf = (dom_small_vel_calc_conf + dom_large_vel_calc_conf) / 2.0\n",
    "        non_dom_vel_calc_conf = (non_dom_small_vel_calc_conf + non_dom_large_vel_calc_conf) / 2.0\n",
    "        \n",
    "        # Prepare arrays\n",
    "        velocity_confidence = np.array([dom_avg_conf, non_dom_avg_conf])\n",
    "        velocity_calculation_confidence = np.array([dom_vel_calc_conf, non_dom_vel_calc_conf])\n",
    "        \n",
    "        # Save back to the NPZ file\n",
    "        modifications = {\n",
    "            'dom_velocity_small': dom_velocity_small,\n",
    "            'dom_velocity_large': dom_velocity_large,\n",
    "            'non_dom_velocity_small': non_dom_velocity_small,\n",
    "            'non_dom_velocity_large': non_dom_velocity_large,\n",
    "            'velocity_confidence': velocity_confidence,\n",
    "            'velocity_calculation_confidence': velocity_calculation_confidence,\n",
    "            'wrist_velocity_small': wrist_velocity_small_polar,\n",
    "            'wrist_velocity_large': wrist_velocity_large_polar,\n",
    "            \n",
    "        }\n",
    "        \n",
    "        modify_npz_file(current_file_path, modifications)\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 100 == 0 or i == len(frame_indices) - 1:\n",
    "            print(f\"Processed {i+1}/{len(frame_indices)} frames\")\n",
    "    \n",
    "    print(f\"Velocity computation complete. Processed {processed_count} frames.\")\n",
    "    return processed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing velocities for 30 files...\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000030_00m30s000ms.npz\n",
      "Frame 30 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000031_00m31s000ms.npz\n",
      "Frame 31 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000032_00m32s000ms.npz\n",
      "Frame 32 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000033_00m33s000ms.npz\n",
      "Frame 33 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000034_00m34s000ms.npz\n",
      "Frame 34 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000035_00m35s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000037_00m37s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000038_00m38s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000041_00m41s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000042_00m42s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000043_00m43s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000044_00m44s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000045_00m45s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000046_00m46s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000047_00m47s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000048_00m48s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000049_00m49s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000050_00m50s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000051_00m51s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000052_00m52s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000053_00m53s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000055_00m55s000ms.npz\n",
      "Frame 55 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000056_00m56s000ms.npz\n",
      "Frame 56 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000057_00m57s000ms.npz\n",
      "Frame 57 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000058_00m58s000ms.npz\n",
      "Frame 58 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000059_00m59s000ms.npz\n",
      "Frame 59 too close to video boundary - setting zero velocities\n",
      "Velocity computation complete. Processed 30 frames.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_landmark_velocities(directory_path=\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data_with_velocities(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks = data['dom_landmarks']\n",
    "    non_dom_landmarks = data['non_dom_landmarks']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores = data['blendshape_scores']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist = data['nose_to_wrist_dist']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    dom_velocity_small = data['dom_velocity_small']\n",
    "    dom_velocity_large = data['dom_velocity_large']\n",
    "    non_dom_velocity_small = data['non_dom_velocity_small']\n",
    "    non_dom_velocity_large = data['non_dom_velocity_large']\n",
    "    velocity_confidence = data['velocity_confidence']\n",
    "    velocity_calculation_confidence = data['velocity_calculation_confidence']\n",
    "    nose_to_wrist_velocity_small = data['wrist_velocity_small']\n",
    "    nose_to_wrist_velocity_large = data['wrist_velocity_large']\n",
    "    \n",
    "    return (dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores, face_detected, \n",
    "            nose_to_wrist_dist, frame_idx, timestamp_ms, dom_velocity_small, dom_velocity_large, non_dom_velocity_small, non_dom_velocity_large, velocity_confidence, velocity_calculation_confidence, nose_to_wrist_velocity_small, nose_to_wrist_velocity_large)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_30 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000030_00m30s000ms.npz\")\n",
    "frame_37 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000037_00m37s000ms.npz\")\n",
    "frame_42 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000042_00m42s000ms.npz\")\n",
    "frame_32 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000032_00m32s000ms.npz\")\n",
    "frame_36 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\")\n",
    "frame_38 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000038_00m38s000ms.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_videos_df(directory_path):\n",
    "    \"\"\"\n",
    "    Process video files in a directory and return information in a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to the directory containing video files\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with video name, frame count, fps, Right/Left designation, and file path\n",
    "    \"\"\"\n",
    "    # Lists to store video information\n",
    "    data = []\n",
    "    \n",
    "    # Common video extensions\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv']\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            # Check if the file is a video\n",
    "            _, ext = os.path.splitext(file)\n",
    "            if ext.lower() in video_extensions:\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Extract video name (without extension)\n",
    "                video_name = os.path.splitext(file)[0]\n",
    "                \n",
    "                # Determine if it's Right or Left\n",
    "                if video_name.endswith(\"_R\"):\n",
    "                    dom_hand = \"Right\"\n",
    "                elif video_name.endswith(\"_L\"):\n",
    "                    dom_hand = \"Left\"\n",
    "                else:\n",
    "                    dom_hand = None\n",
    "                \n",
    "                # Open the video file\n",
    "                try:\n",
    "                    cap = cv2.VideoCapture(file_path)\n",
    "                    \n",
    "                    # Get frames per second\n",
    "                    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "                    \n",
    "                    # Get total number of frames\n",
    "                    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                    \n",
    "                    # Release the video capture object\n",
    "                    cap.release()\n",
    "                    \n",
    "                    # Add row to data\n",
    "                    data.append({\n",
    "                        'Video Name': video_name,\n",
    "                        'Frame Count': frame_count,\n",
    "                        'FPS': fps,\n",
    "                        'dom_hand': dom_hand,\n",
    "                        'file_path': file_path  # Added file_path to the DataFrame\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = make_videos_df(directory_path=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_videos(video_df, process_video_func, detection_threshold_dom, detection_threshold_non_dom, \n",
    "                         detection_threshold_dom_small_length, detection_threshold_non_dom_small_length, report_dir, \n",
    "                         start_time_seconds=0, end_time_seconds=None, delete_videos=False, \n",
    "                         start_from_row=0):\n",
    "    \"\"\"\n",
    "    Process multiple videos from a pandas dataframe with memory-efficient operation.\n",
    "    \n",
    "    This function processes videos one by one, storing detailed results on disk rather than in memory.\n",
    "    This approach prevents memory growth regardless of how many videos are processed, making it\n",
    "    suitable for processing thousands of videos in a single run.\n",
    "    \n",
    "\n",
    "    \n",
    "    Args:\n",
    "        video_df (pandas.DataFrame): DataFrame with 'file_path', 'FPS', and 'Frame Count' columns\n",
    "        process_video_func: The process_video function to use for processing each video\n",
    "        detection_threshold_dom (float): Minimum detection rate (%) for dominant hand\n",
    "        detection_threshold_non_dom (float): Minimum detection rate (%) for non-dominant hand\n",
    "        detection_threshold_dom_small_length (float): Threshold for short videos (dominant hand)\n",
    "        detection_threshold_non_dom_small_length (float): Threshold for short videos (non-dominant hand)\n",
    "        start_time_seconds (float): Start time for video processing\n",
    "        end_time_seconds (float): End time for video processing\n",
    "        delete_videos (bool): Whether to delete original videos after processing\n",
    "        start_from_row (int): Index to start processing from (for resuming previous runs)\n",
    "        report_dir (str): Directory to store reports and detailed results\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the generated report JSON file\n",
    "    \"\"\"\n",
    "    # Create report directory structure\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    details_dir = os.path.join(report_dir, \"video_details\")\n",
    "    os.makedirs(details_dir, exist_ok=True)\n",
    "    \n",
    "    # Define report file paths\n",
    "    current_report_path = os.path.join(report_dir, \"video_processing_report_current.json\")\n",
    "    temp_report_path = os.path.join(report_dir, \"video_processing_report_temp.json\")\n",
    "    \n",
    "    # Initialize statistics for logging\n",
    "    if start_from_row > 0 and os.path.exists(current_report_path):\n",
    "        # Load previous summary statistics if resuming\n",
    "        print(f\"Loading previous report from {current_report_path}\")\n",
    "        try:\n",
    "            with open(current_report_path, \"r\") as f:\n",
    "                stats = json.load(f)\n",
    "            \n",
    "            # Update resume information\n",
    "            stats[\"processing_info\"][\"resume_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            stats[\"processing_info\"][\"resumed_from_row\"] = start_from_row\n",
    "            \n",
    "            print(f\"Successfully loaded previous report summary\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading previous report: {e}\")\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        stats = None\n",
    "        \n",
    "    # Create new statistics if not resuming or if loading failed\n",
    "    if stats is None:\n",
    "        stats = {\n",
    "            \"processing_info\": {\n",
    "                \"start_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"end_time\": None,\n",
    "                \"total_videos\": len(video_df),\n",
    "                \"videos_processed\": 0,\n",
    "                \"directories_deleted\": 0,\n",
    "                \"videos_deleted\": 0,\n",
    "                \"detection_threshold_dom\": detection_threshold_dom,\n",
    "                \"detection_threshold_non_dom\": detection_threshold_non_dom,\n",
    "                \"detection_threshold_dom_small_length\": detection_threshold_dom_small_length,\n",
    "                \"detection_threshold_non_dom_small_length\": detection_threshold_non_dom_small_length,\n",
    "                \"last_processed_row\": -1,\n",
    "            },\n",
    "            \"deleted_directories_summary\": {\n",
    "                \"count\": 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "    if start_from_row > 0:\n",
    "        last_processed_row = stats[\"processing_info\"].get(\"last_processed_row\", -1)\n",
    "        if last_processed_row >= 0 and start_from_row <= last_processed_row:\n",
    "            print(f\"Resuming from row {start_from_row} (last processed row was {last_processed_row})\")\n",
    "        elif last_processed_row >= 0 and start_from_row > last_processed_row + 1:\n",
    "            print(f\"Warning: Skipping rows {last_processed_row+1} to {start_from_row-1}\") \n",
    "    \n",
    "    # Calculate the number of videos to process\n",
    "    total_videos = len(video_df)\n",
    "    remaining_videos = total_videos - start_from_row\n",
    "    \n",
    "    print(f\"Starting batch processing from row {start_from_row} ({remaining_videos} videos remaining)\")\n",
    "    print(f\"Detection thresholds (normal): Dom={detection_threshold_dom}%, Non-Dom={detection_threshold_non_dom}%\")\n",
    "    print(f\"Detection thresholds (short videos): Dom={detection_threshold_dom_small_length}%, Non-Dom={detection_threshold_non_dom_small_length}%\")\n",
    "    print(f\"Using memory-efficient processing - detailed results stored in: {details_dir}\")\n",
    "    \n",
    "    # Helper function to save the current report using atomic operations\n",
    "    def save_current_report():\n",
    "        stats[\"processing_info\"][\"last_updated\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Write to temporary file first to avoid corruption\n",
    "        with open(temp_report_path, \"w\") as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        \n",
    "        # Atomic rename to ensure file integrity\n",
    "        if os.path.exists(temp_report_path):\n",
    "            os.rename(temp_report_path, current_report_path)\n",
    "    \n",
    "    # Save initial report\n",
    "    save_current_report()\n",
    "    \n",
    "    # Process each video starting from the specified row\n",
    "    for idx in range(start_from_row, total_videos):\n",
    "        row = video_df.iloc[idx]\n",
    "        video_path = row['file_path']\n",
    "        video_fps = row['FPS']\n",
    "        video_framecount = row['Frame Count']\n",
    "        video_length = video_framecount / video_fps\n",
    "        \n",
    "\n",
    "        # Skip if file doesn't exist\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Video not found: {video_path}\")\n",
    "            \n",
    "            # Create video detail file for skipped video\n",
    "            video_detail = {\n",
    "                \"video_path\": video_path,\n",
    "                \"status\": \"skipped\",\n",
    "                \"reason\": \"file_not_found\",\n",
    "                \"processed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"row_index\": idx\n",
    "            }\n",
    "            \n",
    "            # Save detail to separate file\n",
    "            path_hash = hashlib.md5(video_path.encode()).hexdigest()[:8]\n",
    "            detail_filename = f\"video_detail_{idx}_{path_hash}.json\"\n",
    "            with open(os.path.join(details_dir, detail_filename), 'w') as f:\n",
    "                json.dump(video_detail, f, indent=2)\n",
    "                \n",
    "            # Mark as processed\n",
    "            stats[\"processing_info\"][\"last_processed_row\"] = idx\n",
    "            \n",
    "            # Save summary report\n",
    "            save_current_report()\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing video {idx-start_from_row+1}/{remaining_videos} (overall: {idx+1}/{total_videos}): {video_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Determine output directory path (before actually processing)\n",
    "            video_dir = os.path.dirname(video_path)\n",
    "            video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "            output_dir = os.path.join(video_dir, f\"{video_name}_landmarks\")\n",
    "            \n",
    "            # Process the video\n",
    "            process_start_time = datetime.now()\n",
    "            process_video_func(video_path, start_time_seconds=start_time_seconds, end_time_seconds=end_time_seconds)\n",
    "            process_end_time = datetime.now()\n",
    "            process_duration = (process_end_time - process_start_time).total_seconds()\n",
    "            \n",
    "            stats[\"processing_info\"][\"videos_processed\"] += 1\n",
    "            \n",
    "            # Check detection statistics\n",
    "            stats_file = os.path.join(output_dir, \"detection_statistics.json\")\n",
    "            \n",
    "            if not os.path.exists(stats_file):\n",
    "                print(f\"Warning: Statistics file not found for {video_path}\")\n",
    "                \n",
    "                # Create video detail file for error\n",
    "                video_detail = {\n",
    "                    \"video_path\": video_path,\n",
    "                    \"status\": \"error\",\n",
    "                    \"reason\": \"statistics_file_not_found\",\n",
    "                    \"processed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"processing_time_seconds\": process_duration,\n",
    "                    \"row_index\": idx\n",
    "                }\n",
    "                \n",
    "                # Save detail to separate file\n",
    "                path_hash = hashlib.md5(video_path.encode()).hexdigest()[:8]\n",
    "                detail_filename = f\"video_detail_{idx}_{path_hash}.json\"\n",
    "                with open(os.path.join(details_dir, detail_filename), 'w') as f:\n",
    "                    json.dump(video_detail, f, indent=2)\n",
    "                    \n",
    "                \n",
    "                stats[\"processing_info\"][\"last_processed_row\"] = idx\n",
    "                \n",
    "                # Save summary report\n",
    "                save_current_report()\n",
    "                continue\n",
    "                \n",
    "            # Read detection statistics\n",
    "            with open(stats_file, \"r\") as f:\n",
    "                detection_stats = json.load(f)\n",
    "            \n",
    "            # Check detection rates\n",
    "            dom_hand_rate = detection_stats[\"detection_rates\"][\"dominant_hand\"][\"detection_rate\"]\n",
    "            non_dom_hand_rate = detection_stats[\"detection_rates\"][\"non_dominant_hand\"][\"detection_rate\"]\n",
    "            face_rate = detection_stats[\"detection_rates\"][\"face\"][\"detection_rate\"]\n",
    "            \n",
    "            # Create video details\n",
    "            video_detail = {\n",
    "                \"video_path\": video_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"dominant_hand_rate\": dom_hand_rate,\n",
    "                \"non_dominant_hand_rate\": non_dom_hand_rate,\n",
    "                \"face_rate\": face_rate,\n",
    "                \"processed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"processing_time_seconds\": process_duration,\n",
    "                \"row_index\": idx,\n",
    "                \"video_length_seconds\": video_length\n",
    "            }\n",
    "            \n",
    "            # Apply appropriate thresholds based on video length\n",
    "            current_detection_threshold_dom = detection_threshold_dom\n",
    "            current_detection_threshold_non_dom = detection_threshold_non_dom\n",
    "            \n",
    "            if video_length < 10: \n",
    "                current_detection_threshold_dom = detection_threshold_dom_small_length\n",
    "                current_detection_threshold_non_dom = detection_threshold_non_dom_small_length\n",
    "                video_detail[\"thresholds_used\"] = \"short_video\"\n",
    "            else:\n",
    "                video_detail[\"thresholds_used\"] = \"normal\"\n",
    "                \n",
    "            # Check if detection rates are below threshold\n",
    "            if dom_hand_rate < current_detection_threshold_dom or non_dom_hand_rate < current_detection_threshold_non_dom:\n",
    "                print(f\"Low detection rate for {video_name}: Dom={dom_hand_rate:.1f}%, Non-Dom={non_dom_hand_rate:.1f}%\")\n",
    "                print(f\"Deleting directory: {output_dir}\")\n",
    "                \n",
    "                # Delete the directory\n",
    "                shutil.rmtree(output_dir)\n",
    "                \n",
    "                # Update statistics\n",
    "                stats[\"processing_info\"][\"directories_deleted\"] += 1\n",
    "                stats[\"deleted_directories_summary\"][\"count\"] += 1\n",
    "                \n",
    "                video_detail[\"status\"] = \"deleted\"\n",
    "                video_detail[\"reason\"] = \"low_detection_rate\"\n",
    "            else:\n",
    "                print(f\"Detection rates acceptable: Dom={dom_hand_rate:.1f}%, Non-Dom={non_dom_hand_rate:.1f}%\")\n",
    "                video_detail[\"status\"] = \"kept\"\n",
    "            \n",
    "            # Save detail to separate file\n",
    "            path_hash = hashlib.md5(video_path.encode()).hexdigest()[:8]\n",
    "            detail_filename = f\"video_detail_{idx}_{path_hash}.json\"\n",
    "            with open(os.path.join(details_dir, detail_filename), 'w') as f:\n",
    "                json.dump(video_detail, f, indent=2)\n",
    "            \n",
    "            stats[\"processing_info\"][\"last_processed_row\"] = idx\n",
    "            \n",
    "            # Delete original video if requested\n",
    "            if delete_videos:\n",
    "                os.remove(video_path)\n",
    "                stats[\"processing_info\"][\"videos_deleted\"] += 1\n",
    "                print(f\"Deleted original video: {video_path}\")\n",
    "            \n",
    "            # Save report after each video\n",
    "            save_current_report()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_path}: {str(e)}\")\n",
    "            \n",
    "            # Create video detail file for error\n",
    "            video_detail = {\n",
    "                \"video_path\": video_path,\n",
    "                \"status\": \"error\",\n",
    "                \"reason\": str(e),\n",
    "                \"processed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"row_index\": idx\n",
    "            }\n",
    "            \n",
    "            # Save detail to separate file\n",
    "            path_hash = hashlib.md5(video_path.encode()).hexdigest()[:8]\n",
    "            detail_filename = f\"video_detail_{idx}_{path_hash}.json\"\n",
    "            with open(os.path.join(details_dir, detail_filename), 'w') as f:\n",
    "                json.dump(video_detail, f, indent=2)\n",
    "                \n",
    "            \n",
    "            stats[\"processing_info\"][\"last_processed_row\"] = idx\n",
    "            \n",
    "            # Save report even when errors occur\n",
    "            save_current_report()\n",
    "    \n",
    "    # Complete statistics\n",
    "    stats[\"processing_info\"][\"end_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    \n",
    "    # Save final current report\n",
    "    save_current_report()\n",
    "    \n",
    "    # Also save a timestamped archive copy\n",
    "    archive_report_path = os.path.join(report_dir, f\"video_processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    shutil.copy(current_report_path, archive_report_path)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== PROCESSING SUMMARY =====\")\n",
    "    print(f\"Total videos: {stats['processing_info']['total_videos']}\")\n",
    "    print(f\"Videos processed (this run + previous): {stats['processing_info']['videos_processed']}\")\n",
    "    print(f\"Directories deleted (low detection rate): {stats['processing_info']['directories_deleted']}\")\n",
    "    if delete_videos:\n",
    "        print(f\"Original videos deleted: {stats['processing_info']['videos_deleted']}\")\n",
    "    print(f\"Current report saved to: {current_report_path}\")\n",
    "    print(f\"Archive report saved to: {archive_report_path}\")\n",
    "    print(f\"Detailed results stored in: {details_dir}\")\n",
    "    \n",
    "    return archive_report_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing from row 0 (2 videos remaining)\n",
      "Detection thresholds (normal): Dom=70%, Non-Dom=50%\n",
      "Detection thresholds (short videos): Dom=5%, Non-Dom=5%\n",
      "Using memory-efficient processing - detailed results stored in: ./report/video_details\n",
      "\n",
      "Processing video 1/2 (overall: 1/2): ./66221823911-CARRY.mp4\n",
      "Warning: Could not determine dominant hand from filename, using default: Right\n",
      "Detected dominant hand from filename: Right\n",
      "Video: 66221823911-CARRY\n",
      "Total frames: 182\n",
      "FPS: 29.968713979911083\n",
      "Duration: 6.07 seconds\n",
      "Processing frames 0 to 182 (time 0.00s to 6.07s)\n",
      "Output directory: 66221823911-CARRY_landmarks\n",
      "Processing frame 0/182 (timestamp: 00m00s000ms)\n",
      "Processing frame 1/182 (timestamp: 00m00s033ms)\n",
      "Processing frame 2/182 (timestamp: 00m00s066ms)\n",
      "Processing frame 3/182 (timestamp: 00m00s100ms)\n",
      "Processing frame 4/182 (timestamp: 00m00s133ms)\n",
      "Processing frame 5/182 (timestamp: 00m00s166ms)\n",
      "Processing frame 6/182 (timestamp: 00m00s200ms)\n",
      "Processing frame 7/182 (timestamp: 00m00s233ms)\n",
      "Processing frame 8/182 (timestamp: 00m00s266ms)\n",
      "Processing frame 9/182 (timestamp: 00m00s300ms)\n",
      "Processing frame 10/182 (timestamp: 00m00s333ms)\n",
      "Processing frame 11/182 (timestamp: 00m00s367ms)\n",
      "Processing frame 12/182 (timestamp: 00m00s400ms)\n",
      "Processing frame 13/182 (timestamp: 00m00s433ms)\n",
      "Processing frame 14/182 (timestamp: 00m00s467ms)\n",
      "Processing frame 15/182 (timestamp: 00m00s500ms)\n",
      "Processing frame 16/182 (timestamp: 00m00s533ms)\n",
      "Processing frame 17/182 (timestamp: 00m00s567ms)\n",
      "Processing frame 18/182 (timestamp: 00m00s600ms)\n",
      "Processing frame 19/182 (timestamp: 00m00s633ms)\n",
      "Processing frame 20/182 (timestamp: 00m00s667ms)\n",
      "Processing frame 21/182 (timestamp: 00m00s700ms)\n",
      "Processing frame 22/182 (timestamp: 00m00s734ms)\n",
      "Processing frame 23/182 (timestamp: 00m00s767ms)\n",
      "Processing frame 24/182 (timestamp: 00m00s800ms)\n",
      "Processing frame 25/182 (timestamp: 00m00s834ms)\n",
      "Processing frame 26/182 (timestamp: 00m00s867ms)\n",
      "Processing frame 27/182 (timestamp: 00m00s900ms)\n",
      "Processing frame 28/182 (timestamp: 00m00s934ms)\n",
      "Processing frame 29/182 (timestamp: 00m00s967ms)\n",
      "Processing frame 30/182 (timestamp: 00m01s001ms)\n",
      "Processing frame 31/182 (timestamp: 00m01s034ms)\n",
      "Processing frame 32/182 (timestamp: 00m01s067ms)\n",
      "Processing frame 33/182 (timestamp: 00m01s101ms)\n",
      "Processing frame 34/182 (timestamp: 00m01s134ms)\n",
      "Processing frame 35/182 (timestamp: 00m01s167ms)\n",
      "Processing frame 36/182 (timestamp: 00m01s201ms)\n",
      "Processing frame 37/182 (timestamp: 00m01s234ms)\n",
      "Processing frame 38/182 (timestamp: 00m01s267ms)\n",
      "Processing frame 39/182 (timestamp: 00m01s301ms)\n",
      "Processing frame 40/182 (timestamp: 00m01s334ms)\n",
      "Processing frame 41/182 (timestamp: 00m01s368ms)\n",
      "Processing frame 42/182 (timestamp: 00m01s401ms)\n",
      "Processing frame 43/182 (timestamp: 00m01s434ms)\n",
      "Processing frame 44/182 (timestamp: 00m01s468ms)\n",
      "Processing frame 45/182 (timestamp: 00m01s501ms)\n",
      "Processing frame 46/182 (timestamp: 00m01s534ms)\n",
      "Processing frame 47/182 (timestamp: 00m01s568ms)\n",
      "Processing frame 48/182 (timestamp: 00m01s601ms)\n",
      "Processing frame 49/182 (timestamp: 00m01s635ms)\n",
      "Processing frame 50/182 (timestamp: 00m01s668ms)\n",
      "Processing frame 51/182 (timestamp: 00m01s701ms)\n",
      "Processing frame 52/182 (timestamp: 00m01s735ms)\n",
      "Processing frame 53/182 (timestamp: 00m01s768ms)\n",
      "Processing frame 54/182 (timestamp: 00m01s801ms)\n",
      "Processing frame 55/182 (timestamp: 00m01s835ms)\n",
      "Processing frame 56/182 (timestamp: 00m01s868ms)\n",
      "Processing frame 57/182 (timestamp: 00m01s901ms)\n",
      "Processing frame 58/182 (timestamp: 00m01s935ms)\n",
      "Processing frame 59/182 (timestamp: 00m01s968ms)\n",
      "Processing frame 60/182 (timestamp: 00m02s002ms)\n",
      "Processing frame 61/182 (timestamp: 00m02s035ms)\n",
      "Processing frame 62/182 (timestamp: 00m02s068ms)\n",
      "Processing frame 63/182 (timestamp: 00m02s102ms)\n",
      "Processing frame 64/182 (timestamp: 00m02s135ms)\n",
      "Processing frame 65/182 (timestamp: 00m02s168ms)\n",
      "Processing frame 66/182 (timestamp: 00m02s202ms)\n",
      "Processing frame 67/182 (timestamp: 00m02s235ms)\n",
      "Processing frame 68/182 (timestamp: 00m02s269ms)\n",
      "Processing frame 69/182 (timestamp: 00m02s302ms)\n",
      "Processing frame 70/182 (timestamp: 00m02s335ms)\n",
      "Processing frame 71/182 (timestamp: 00m02s369ms)\n",
      "Processing frame 72/182 (timestamp: 00m02s402ms)\n",
      "Processing frame 73/182 (timestamp: 00m02s435ms)\n",
      "Processing frame 74/182 (timestamp: 00m02s469ms)\n",
      "Processing frame 75/182 (timestamp: 00m02s502ms)\n",
      "Processing frame 76/182 (timestamp: 00m02s535ms)\n",
      "Processing frame 77/182 (timestamp: 00m02s569ms)\n",
      "Processing frame 78/182 (timestamp: 00m02s602ms)\n",
      "Processing frame 79/182 (timestamp: 00m02s636ms)\n",
      "Processing frame 80/182 (timestamp: 00m02s669ms)\n",
      "Processing frame 81/182 (timestamp: 00m02s702ms)\n",
      "Processing frame 82/182 (timestamp: 00m02s736ms)\n",
      "Processing frame 83/182 (timestamp: 00m02s769ms)\n",
      "Processing frame 84/182 (timestamp: 00m02s802ms)\n",
      "Processing frame 85/182 (timestamp: 00m02s836ms)\n",
      "Processing frame 86/182 (timestamp: 00m02s869ms)\n",
      "Processing frame 87/182 (timestamp: 00m02s903ms)\n",
      "Processing frame 88/182 (timestamp: 00m02s936ms)\n",
      "Processing frame 89/182 (timestamp: 00m02s969ms)\n",
      "Processing frame 90/182 (timestamp: 00m03s003ms)\n",
      "Processing frame 91/182 (timestamp: 00m03s036ms)\n",
      "Processing frame 92/182 (timestamp: 00m03s069ms)\n",
      "Processing frame 93/182 (timestamp: 00m03s103ms)\n",
      "Processing frame 94/182 (timestamp: 00m03s136ms)\n",
      "Processing frame 95/182 (timestamp: 00m03s169ms)\n",
      "Processing frame 96/182 (timestamp: 00m03s203ms)\n",
      "Processing frame 97/182 (timestamp: 00m03s236ms)\n",
      "Processing frame 98/182 (timestamp: 00m03s270ms)\n",
      "Processing frame 99/182 (timestamp: 00m03s303ms)\n",
      "Processing frame 100/182 (timestamp: 00m03s336ms)\n",
      "Processing frame 101/182 (timestamp: 00m03s370ms)\n",
      "Processing frame 102/182 (timestamp: 00m03s403ms)\n",
      "Processing frame 103/182 (timestamp: 00m03s436ms)\n",
      "Processing frame 104/182 (timestamp: 00m03s470ms)\n",
      "Processing frame 105/182 (timestamp: 00m03s503ms)\n",
      "Processing frame 106/182 (timestamp: 00m03s537ms)\n",
      "Processing frame 107/182 (timestamp: 00m03s570ms)\n",
      "Processing frame 108/182 (timestamp: 00m03s603ms)\n",
      "Processing frame 109/182 (timestamp: 00m03s637ms)\n",
      "Processing frame 110/182 (timestamp: 00m03s670ms)\n",
      "Processing frame 111/182 (timestamp: 00m03s703ms)\n",
      "Processing frame 112/182 (timestamp: 00m03s737ms)\n",
      "Processing frame 113/182 (timestamp: 00m03s770ms)\n",
      "Processing frame 114/182 (timestamp: 00m03s803ms)\n",
      "Processing frame 115/182 (timestamp: 00m03s837ms)\n",
      "Processing frame 116/182 (timestamp: 00m03s870ms)\n",
      "Processing frame 117/182 (timestamp: 00m03s904ms)\n",
      "Processing frame 118/182 (timestamp: 00m03s937ms)\n",
      "Processing frame 119/182 (timestamp: 00m03s970ms)\n",
      "Processing frame 120/182 (timestamp: 00m04s004ms)\n",
      "Processing frame 121/182 (timestamp: 00m04s037ms)\n",
      "Processing frame 122/182 (timestamp: 00m04s070ms)\n",
      "Processing frame 123/182 (timestamp: 00m04s104ms)\n",
      "Processing frame 124/182 (timestamp: 00m04s137ms)\n",
      "Processing frame 125/182 (timestamp: 00m04s171ms)\n",
      "Processing frame 126/182 (timestamp: 00m04s204ms)\n",
      "Processing frame 127/182 (timestamp: 00m04s237ms)\n",
      "Processing frame 128/182 (timestamp: 00m04s271ms)\n",
      "Processing frame 129/182 (timestamp: 00m04s304ms)\n",
      "Processing frame 130/182 (timestamp: 00m04s337ms)\n",
      "Processing frame 131/182 (timestamp: 00m04s371ms)\n",
      "Processing frame 132/182 (timestamp: 00m04s404ms)\n",
      "Processing frame 133/182 (timestamp: 00m04s437ms)\n",
      "Processing frame 134/182 (timestamp: 00m04s471ms)\n",
      "Processing frame 135/182 (timestamp: 00m04s504ms)\n",
      "Processing frame 136/182 (timestamp: 00m04s538ms)\n",
      "Processing frame 137/182 (timestamp: 00m04s571ms)\n",
      "Processing frame 138/182 (timestamp: 00m04s604ms)\n",
      "Processing frame 139/182 (timestamp: 00m04s638ms)\n",
      "Processing frame 140/182 (timestamp: 00m04s671ms)\n",
      "Processing frame 141/182 (timestamp: 00m04s704ms)\n",
      "Processing frame 142/182 (timestamp: 00m04s738ms)\n",
      "Processing frame 143/182 (timestamp: 00m04s771ms)\n",
      "Processing frame 144/182 (timestamp: 00m04s805ms)\n",
      "Processing frame 145/182 (timestamp: 00m04s838ms)\n",
      "Processing frame 146/182 (timestamp: 00m04s871ms)\n",
      "Processing frame 147/182 (timestamp: 00m04s905ms)\n",
      "Processing frame 148/182 (timestamp: 00m04s938ms)\n",
      "Processing frame 149/182 (timestamp: 00m04s971ms)\n",
      "Processing frame 150/182 (timestamp: 00m05s005ms)\n",
      "Processing frame 151/182 (timestamp: 00m05s038ms)\n",
      "Processing frame 152/182 (timestamp: 00m05s071ms)\n",
      "Processing frame 153/182 (timestamp: 00m05s105ms)\n",
      "Processing frame 154/182 (timestamp: 00m05s138ms)\n",
      "Processing frame 155/182 (timestamp: 00m05s172ms)\n",
      "Processing frame 156/182 (timestamp: 00m05s205ms)\n",
      "Processing frame 157/182 (timestamp: 00m05s238ms)\n",
      "Processing frame 158/182 (timestamp: 00m05s272ms)\n",
      "Processing frame 159/182 (timestamp: 00m05s305ms)\n",
      "Processing frame 160/182 (timestamp: 00m05s338ms)\n",
      "Processing frame 161/182 (timestamp: 00m05s372ms)\n",
      "Processing frame 162/182 (timestamp: 00m05s405ms)\n",
      "Processing frame 163/182 (timestamp: 00m05s439ms)\n",
      "Processing frame 164/182 (timestamp: 00m05s472ms)\n",
      "Processing frame 165/182 (timestamp: 00m05s505ms)\n",
      "Processing frame 166/182 (timestamp: 00m05s539ms)\n",
      "Processing frame 167/182 (timestamp: 00m05s572ms)\n",
      "Processing frame 168/182 (timestamp: 00m05s605ms)\n",
      "Processing frame 169/182 (timestamp: 00m05s639ms)\n",
      "Processing frame 170/182 (timestamp: 00m05s672ms)\n",
      "Processing frame 171/182 (timestamp: 00m05s705ms)\n",
      "Processing frame 172/182 (timestamp: 00m05s739ms)\n",
      "Processing frame 173/182 (timestamp: 00m05s772ms)\n",
      "Processing frame 174/182 (timestamp: 00m05s806ms)\n",
      "Processing frame 175/182 (timestamp: 00m05s839ms)\n",
      "Processing frame 176/182 (timestamp: 00m05s872ms)\n",
      "Processing frame 177/182 (timestamp: 00m05s906ms)\n",
      "Processing frame 178/182 (timestamp: 00m05s939ms)\n",
      "Processing frame 179/182 (timestamp: 00m05s972ms)\n",
      "Processing frame 180/182 (timestamp: 00m06s006ms)\n",
      "Processing frame 181/182 (timestamp: 00m06s039ms)\n",
      "\n",
      "===== PROCESSING SUMMARY =====\n",
      "Processed 182 frames\n",
      "Detection rates: Dom hand: 26.9%, Non-dom hand: 17.0%, Face: 100.0%\n",
      "All parts detected in 0.0% of frames\n",
      "Full statistics saved to: 66221823911-CARRY_landmarks/detection_statistics.json\n",
      "Results saved to: 66221823911-CARRY_landmarks\n",
      "Detection rates acceptable: Dom=26.9%, Non-Dom=17.0%\n",
      "Deleted original video: ./66221823911-CARRY.mp4\n",
      "\n",
      "Processing video 2/2 (overall: 2/2): ./4.7299129501965353e-7-seedSOUR.mp4\n",
      "Warning: Could not determine dominant hand from filename, using default: Right\n",
      "Detected dominant hand from filename: Right\n",
      "Video: 4.7299129501965353e-7-seedSOUR\n",
      "Total frames: 98\n",
      "FPS: 30.406\n",
      "Duration: 3.22 seconds\n",
      "Processing frames 0 to 98 (time 0.00s to 3.22s)\n",
      "Output directory: 4.7299129501965353e-7-seedSOUR_landmarks\n",
      "Processing frame 0/98 (timestamp: 00m00s000ms)\n",
      "Processing frame 1/98 (timestamp: 00m00s032ms)\n",
      "Processing frame 2/98 (timestamp: 00m00s065ms)\n",
      "Processing frame 3/98 (timestamp: 00m00s098ms)\n",
      "Processing frame 4/98 (timestamp: 00m00s131ms)\n",
      "Processing frame 5/98 (timestamp: 00m00s164ms)\n",
      "Processing frame 6/98 (timestamp: 00m00s197ms)\n",
      "Processing frame 7/98 (timestamp: 00m00s230ms)\n",
      "Processing frame 8/98 (timestamp: 00m00s263ms)\n",
      "Processing frame 9/98 (timestamp: 00m00s295ms)\n",
      "Processing frame 10/98 (timestamp: 00m00s328ms)\n",
      "Processing frame 11/98 (timestamp: 00m00s361ms)\n",
      "Processing frame 12/98 (timestamp: 00m00s394ms)\n",
      "Processing frame 13/98 (timestamp: 00m00s427ms)\n",
      "Processing frame 14/98 (timestamp: 00m00s460ms)\n",
      "Processing frame 15/98 (timestamp: 00m00s493ms)\n",
      "Processing frame 16/98 (timestamp: 00m00s526ms)\n",
      "Processing frame 17/98 (timestamp: 00m00s559ms)\n",
      "Processing frame 18/98 (timestamp: 00m00s591ms)\n",
      "Processing frame 19/98 (timestamp: 00m00s624ms)\n",
      "Processing frame 20/98 (timestamp: 00m00s657ms)\n",
      "Processing frame 21/98 (timestamp: 00m00s690ms)\n",
      "Processing frame 22/98 (timestamp: 00m00s723ms)\n",
      "Processing frame 23/98 (timestamp: 00m00s756ms)\n",
      "Processing frame 24/98 (timestamp: 00m00s789ms)\n",
      "Processing frame 25/98 (timestamp: 00m00s822ms)\n",
      "Processing frame 26/98 (timestamp: 00m00s855ms)\n",
      "Processing frame 27/98 (timestamp: 00m00s887ms)\n",
      "Processing frame 28/98 (timestamp: 00m00s920ms)\n",
      "Processing frame 29/98 (timestamp: 00m00s953ms)\n",
      "Processing frame 30/98 (timestamp: 00m00s986ms)\n",
      "Processing frame 31/98 (timestamp: 00m01s019ms)\n",
      "Processing frame 32/98 (timestamp: 00m01s052ms)\n",
      "Processing frame 33/98 (timestamp: 00m01s085ms)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbatch_process_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideos_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocess_video_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocess_video\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_threshold_dom\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_threshold_non_dom\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_threshold_dom_small_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_threshold_non_dom_small_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_from_row\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_videos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mbatch_process_videos\u001b[39m\u001b[34m(video_df, process_video_func, detection_threshold_dom, detection_threshold_non_dom, detection_threshold_dom_small_length, detection_threshold_non_dom_small_length, report_dir, start_time_seconds, end_time_seconds, delete_videos, start_from_row)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Process the video\u001b[39;00m\n\u001b[32m    155\u001b[39m process_start_time = datetime.now()\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[43mprocess_video_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_time_seconds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_time_seconds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m process_end_time = datetime.now()\n\u001b[32m    158\u001b[39m process_duration = (process_end_time - process_start_time).total_seconds()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 201\u001b[39m, in \u001b[36mprocess_video\u001b[39m\u001b[34m(video_path, adaptive_detect_func, hand_model_path, face_model_path, min_hand_detection_confidence, min_hand_presence_confidence, min_face_detection_confidence, min_face_presence_confidence, num_hands, output_face_blendshapes, max_attempts, threshold_reduction_factor, min_threshold, frame_step, start_time_seconds, end_time_seconds, save_failure_screenshots)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m SuppressOutput():\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# Use adaptive_detect on the frame\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         results = \u001b[43madaptive_detect_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemp_frame_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhand_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_hand_detection_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_hand_detection_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_hand_presence_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_hand_presence_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_face_detection_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_face_detection_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_face_presence_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_face_presence_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_hands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_hands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdominand_hand\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdominand_hand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_face_blendshapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_face_blendshapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_attempts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_attempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthreshold_reduction_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold_reduction_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_threshold\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# Calculate processing time\u001b[39;00m\n\u001b[32m    217\u001b[39m     proc_time = time.time() - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36madaptive_detect\u001b[39m\u001b[34m(image_path, hand_model_path, face_model_path, min_hand_detection_confidence, min_hand_presence_confidence, min_face_detection_confidence, min_face_presence_confidence, num_hands, dominand_hand, visualize, output_face_blendshapes, max_attempts, threshold_reduction_factor, min_threshold)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing original thresholds: hands=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_hand_detection_conf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, face=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_face_detection_conf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Call detect with current thresholds (don't visualize intermediate attempts)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m results = \u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mhand_model_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhand_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_model_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mface_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmin_hand_detection_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_hand_detection_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmin_hand_presence_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_hand_presence_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmin_face_detection_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_face_detection_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmin_face_presence_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_face_presence_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m                \u001b[49m\u001b[43mnum_hands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_hands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdominand_hand\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdominand_hand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m                \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m                \u001b[49m\u001b[43moutput_face_blendshapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_face_blendshapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Unpack results\u001b[39;00m\n\u001b[32m     98\u001b[39m dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mdetect\u001b[39m\u001b[34m(image_path, hand_model_path, face_model_path, min_hand_detection_confidence, min_hand_presence_confidence, min_face_detection_confidence, min_face_presence_confidence, num_hands, dominand_hand, visualize, output_face_blendshapes, adaptive_threshold, max_attempts, threshold_reduction_factor, min_threshold)\u001b[39m\n\u001b[32m    128\u001b[39m face_options = vision.FaceLandmarkerOptions(\n\u001b[32m    129\u001b[39m     base_options=face_base_options,\n\u001b[32m    130\u001b[39m     min_face_detection_confidence=min_face_detection_confidence,\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m     running_mode=VisionRunningMode.IMAGE\n\u001b[32m    135\u001b[39m )\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Create the face detector\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m face_detector = \u001b[43mvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFaceLandmarker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# 2.2: Detect face landmarks (reuse the same image)\u001b[39;00m\n\u001b[32m    141\u001b[39m face_detection_result = face_detector.detect(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mediapipe-env/lib/python3.12/site-packages/mediapipe/tasks/python/vision/face_landmarker.py:3104\u001b[39m, in \u001b[36mFaceLandmarker.create_from_options\u001b[39m\u001b[34m(cls, options)\u001b[39m\n\u001b[32m   3091\u001b[39m   output_streams.append(\n\u001b[32m   3092\u001b[39m       \u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m.join([_FACE_GEOMETRY_TAG, _FACE_GEOMETRY_STREAM_NAME])\n\u001b[32m   3093\u001b[39m   )\n\u001b[32m   3095\u001b[39m task_info = _TaskInfo(\n\u001b[32m   3096\u001b[39m     task_graph=_TASK_GRAPH_NAME,\n\u001b[32m   3097\u001b[39m     input_streams=[\n\u001b[32m   (...)\u001b[39m\u001b[32m   3102\u001b[39m     task_options=options,\n\u001b[32m   3103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_graph_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_flow_limiting\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunning_mode\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m        \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43m_RunningMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLIVE_STREAM\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunning_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpackets_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3111\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mediapipe-env/lib/python3.12/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:70\u001b[39m, in \u001b[36mBaseVisionTaskApi.__init__\u001b[39m\u001b[34m(self, graph_config, running_mode, packet_callback)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m packet_callback:\n\u001b[32m     66\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     67\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     68\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mcallback should not be provided.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     69\u001b[39m   )\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28mself\u001b[39m._runner = \u001b[43m_TaskRunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m._running_mode = running_mode\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous report from ./report/video_processing_report_current.json\n",
      "Successfully loaded previous report summary\n",
      "Starting batch processing from row 1 (1 videos remaining)\n",
      "Detection thresholds (normal): Dom=70%, Non-Dom=50%\n",
      "Detection thresholds (short videos): Dom=5%, Non-Dom=5%\n",
      "Using memory-efficient processing - detailed results stored in: ./report/video_details\n",
      "\n",
      "Processing video 1/1 (overall: 2/2): ./4.7299129501965353e-7-seedSOUR.mp4\n",
      "Warning: Could not determine dominant hand from filename, using default: Right\n",
      "Detected dominant hand from filename: Right\n",
      "Video: 4.7299129501965353e-7-seedSOUR\n",
      "Total frames: 98\n",
      "FPS: 30.406\n",
      "Duration: 3.22 seconds\n",
      "Processing frames 0 to 98 (time 0.00s to 3.22s)\n",
      "Output directory: 4.7299129501965353e-7-seedSOUR_landmarks\n",
      "Processing frame 0/98 (timestamp: 00m00s000ms)\n",
      "Processing frame 1/98 (timestamp: 00m00s032ms)\n",
      "Processing frame 2/98 (timestamp: 00m00s065ms)\n",
      "Processing frame 3/98 (timestamp: 00m00s098ms)\n",
      "Processing frame 4/98 (timestamp: 00m00s131ms)\n",
      "Processing frame 5/98 (timestamp: 00m00s164ms)\n",
      "Processing frame 6/98 (timestamp: 00m00s197ms)\n",
      "Processing frame 7/98 (timestamp: 00m00s230ms)\n",
      "Processing frame 8/98 (timestamp: 00m00s263ms)\n",
      "Processing frame 9/98 (timestamp: 00m00s295ms)\n",
      "Processing frame 10/98 (timestamp: 00m00s328ms)\n",
      "Processing frame 11/98 (timestamp: 00m00s361ms)\n",
      "Processing frame 12/98 (timestamp: 00m00s394ms)\n",
      "Processing frame 13/98 (timestamp: 00m00s427ms)\n",
      "Processing frame 14/98 (timestamp: 00m00s460ms)\n",
      "Processing frame 15/98 (timestamp: 00m00s493ms)\n",
      "Processing frame 16/98 (timestamp: 00m00s526ms)\n",
      "Processing frame 17/98 (timestamp: 00m00s559ms)\n",
      "Processing frame 18/98 (timestamp: 00m00s591ms)\n",
      "Processing frame 19/98 (timestamp: 00m00s624ms)\n",
      "Processing frame 20/98 (timestamp: 00m00s657ms)\n",
      "Processing frame 21/98 (timestamp: 00m00s690ms)\n"
     ]
    }
   ],
   "source": [
    "batch_process_videos(\n",
    "    video_df=videos_df, \n",
    "    process_video_func=process_video,\n",
    "    detection_threshold_dom=70, \n",
    "    detection_threshold_non_dom=50,\n",
    "    detection_threshold_dom_small_length=5,\n",
    "    detection_threshold_non_dom_small_length=5,\n",
    "    start_from_row=0, report_dir=\"./report\", delete_videos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./report/video_processing_report_current.json\", \"r\") as f:\n",
    "    current_report = json.load(f)\n",
    "    last_row = current_report[\"processing_info\"][\"last_processed_row\"]\n",
    "    next_row = last_row + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbatch_process_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideos_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocess_video_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocess_video\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_threshold_dom\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_threshold_non_dom\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_threshold_dom_small_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdetection_threshold_non_dom_small_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_from_row\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./report\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mbatch_process_videos\u001b[39m\u001b[34m(video_df, process_video_func, detection_threshold_dom, detection_threshold_non_dom, detection_threshold_dom_small_length, detection_threshold_non_dom_small_length, report_dir, start_time_seconds, end_time_seconds, delete_videos, start_from_row)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Process the video\u001b[39;00m\n\u001b[32m    155\u001b[39m process_start_time = datetime.now()\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[43mprocess_video_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_time_seconds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_time_seconds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m process_end_time = datetime.now()\n\u001b[32m    158\u001b[39m process_duration = (process_end_time - process_start_time).total_seconds()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 201\u001b[39m, in \u001b[36mprocess_video\u001b[39m\u001b[34m(video_path, adaptive_detect_func, hand_model_path, face_model_path, min_hand_detection_confidence, min_hand_presence_confidence, min_face_detection_confidence, min_face_presence_confidence, num_hands, output_face_blendshapes, max_attempts, threshold_reduction_factor, min_threshold, frame_step, start_time_seconds, end_time_seconds, save_failure_screenshots)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m SuppressOutput():\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# Use adaptive_detect on the frame\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         results = \u001b[43madaptive_detect_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtemp_frame_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhand_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_hand_detection_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_hand_detection_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_hand_presence_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_hand_presence_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_face_detection_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_face_detection_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_face_presence_confidence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_face_presence_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_hands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_hands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdominand_hand\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdominand_hand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_face_blendshapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_face_blendshapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_attempts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_attempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthreshold_reduction_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold_reduction_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_threshold\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# Calculate processing time\u001b[39;00m\n\u001b[32m    217\u001b[39m     proc_time = time.time() - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36madaptive_detect\u001b[39m\u001b[34m(image_path, hand_model_path, face_model_path, min_hand_detection_confidence, min_hand_presence_confidence, min_face_detection_confidence, min_face_presence_confidence, num_hands, dominand_hand, visualize, output_face_blendshapes, max_attempts, threshold_reduction_factor, min_threshold)\u001b[39m\n\u001b[32m    106\u001b[39m best_detection_status = [detection_status[\u001b[32m0\u001b[39m], detection_status[\u001b[32m1\u001b[39m]]\n\u001b[32m    107\u001b[39m best_face_detected = face_detected\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNew best detection: dominant hand=\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdetection_status\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnon-dominant hand=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetection_status[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, face=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mface_detected\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# If everything is detected, we can stop early\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m detection_status[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m detection_status[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (face_detected == \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_face_blendshapes):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "batch_process_videos(\n",
    "    video_df=videos_df, \n",
    "    process_video_func=process_video,\n",
    "    detection_threshold_dom=70, \n",
    "    detection_threshold_non_dom=50,\n",
    "    detection_threshold_dom_small_length=5,\n",
    "    detection_threshold_non_dom_small_length=5,\n",
    "    start_from_row=1, report_dir=\"./report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
