{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_model_path = \"hand_landmarker.task\"\n",
    "face_model_path = \"face_landmarker.task\"\n",
    "photo_path = \"Screenshot from 2025-04-01 10-02-44.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "import tempfile\n",
    "\n",
    "class SuppressOutput:\n",
    "    \"\"\"\n",
    "    A context manager that suppresses stdout and stderr from C/C++ libraries.\n",
    "    This is a more aggressive approach than just Python's logging or environment variables.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Create a temporary file to redirect output\n",
    "        self.null_fds = [tempfile.TemporaryFile(mode='w+b') for _ in range(2)]\n",
    "        # Save the original file descriptors to restore later\n",
    "        self.save_fds = [os.dup(1), os.dup(2)]\n",
    "        \n",
    "    def __enter__(self):\n",
    "        # Redirect stdout and stderr to the null files\n",
    "        os.dup2(self.null_fds[0].fileno(), 1)\n",
    "        os.dup2(self.null_fds[1].fileno(), 2)\n",
    "        # Also redirect Python-level stdout/stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *args):\n",
    "        # Restore normal stdout and stderr\n",
    "        for fd in self.null_fds:\n",
    "            fd.close()\n",
    "        for fd in range(2):\n",
    "            os.dup2(self.save_fds[fd], fd + 1)\n",
    "        for fd in self.save_fds:\n",
    "            os.close(fd)\n",
    "        # Restore Python-level stdout/stderr\n",
    "        sys.stdout = sys.__stdout__\n",
    "        sys.stderr = sys.__stderr__\n",
    "\n",
    "try:\n",
    "    import ctypes\n",
    "    libc = ctypes.CDLL(None)\n",
    "    # Attempt to get C's stdout/stderr file descriptor\n",
    "    c_stdout = ctypes.c_void_p.in_dll(libc, 'stdout')\n",
    "    c_stderr = ctypes.c_void_p.in_dll(libc, 'stderr')\n",
    "    devnull = open(os.devnull, 'w')\n",
    "    os.dup2(devnull.fileno(), c_stdout.value)\n",
    "    os.dup2(devnull.fileno(), c_stderr.value)\n",
    "except:\n",
    "    # If this approach fails, continue with other methods\n",
    "    pass\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=INFO, 2=WARNING, 3=ERROR\n",
    "logging.getLogger(\"mediapipe\").setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
    "os.environ['GLOG_minloglevel'] = '3'      # Suppress Google logging (used by MediaPipe)\n",
    "os.environ['MEDIAPIPE_DISABLE_GPU'] = '1'  # Optional: Disable GPU logging messages\n",
    "\n",
    "os.environ['AUTOGRAPH_VERBOSITY'] = '0'\n",
    "# ABSL specific flags\n",
    "os.environ['ABSL_LOGGING_LEVEL'] = '50'  # Higher than any level that should be output\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "logging.getLogger(\"mediapipe\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"absl\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\\\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import glob\n",
    "import hashlib\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, min_face_detection_confidence=0.5, min_face_presence_confidence=0.5, num_hands=2, dominand_hand='Right', visualize=False, output_face_blendshapes=True, adaptive_threshold=True, max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects hands and face in an image, extracts hand landmark coordinates and face blendshapes.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        min_hand_detection_confidence (float): Confidence threshold for hand detection (0.0-1.0)\n",
    "        min_hand_presence_confidence (float): Confidence threshold for hand presence (0.0-1.0)\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        visualize (bool): Whether to visualize the results\n",
    "        output_face_blendshapes (bool): Whether to detect and extract face blendshapes\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (dom_landmarks, non_dom_landmarks, wrists, confidence_scores, detection_status, \n",
    "                blendshape_scores, face_landmark_5, face_detected)\n",
    "               - dom_landmarks: NumPy array of shape [20, 3] with coordinates of dominant hand landmarks\n",
    "               - non_dom_landmarks: NumPy array of shape [20, 3] with coordinates of non-dominant hand landmarks\n",
    "               - wrists: NumPy array of shape [2, 2] with coordinates of both wrists [x, y]\n",
    "               - confidence_scores: NumPy array of shape [2] with confidence scores [dominant_hand, non_dominant_hand]\n",
    "               - detection_status: NumPy array of shape [2] with binary detection status [dominant_hand, non_dominant_hand]\n",
    "               - blendshape_scores: NumPy array of shape [26] with selected face blendshape scores\n",
    "               - face_landmark_5: NumPy array of shape [2] with coordinates of the 5th face landmark [x, y]\n",
    "               - face_detected: Binary value (1 if face detected, 0 if not)\n",
    "    \"\"\"\n",
    "    # Initialize output arrays for face detection\n",
    "    blendshape_scores = np.zeros(52)\n",
    "    nose_landmark = np.zeros(2)\n",
    "    left_eye_landmark = np.zeros(2)\n",
    "    right_eye_landmark = np.zeros(2)\n",
    "    face_detected = 0\n",
    "    \n",
    "    # PART 1: HAND LANDMARK DETECTION\n",
    "    # 1.1: Configure the hand landmarker\n",
    "    hand_base_options = python.BaseOptions(\n",
    "        model_asset_path=hand_model_path\n",
    "    )\n",
    "\n",
    "    VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "    # Configure detection options\n",
    "    hand_options = vision.HandLandmarkerOptions(\n",
    "        base_options=hand_base_options,\n",
    "        num_hands=num_hands,                             \n",
    "        min_hand_detection_confidence=min_hand_detection_confidence,       \n",
    "        min_hand_presence_confidence=min_hand_presence_confidence,        \n",
    "        min_tracking_confidence=0.5,             \n",
    "        running_mode=VisionRunningMode.IMAGE\n",
    "    )\n",
    "\n",
    "    # Create the hand detector\n",
    "    hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "\n",
    "    # 1.2: Load the input image\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "\n",
    "    # 1.3: Detect hand landmarks\n",
    "    hand_detection_result = hand_detector.detect(image)\n",
    "    \n",
    "    # Initialize hand output arrays with zeros\n",
    "    dom_landmarks = np.zeros((20, 3))       # 20 landmarks (excluding wrist), [x,y,z]\n",
    "    non_dom_landmarks = np.zeros((20, 3))   # 20 landmarks (excluding wrist), [x,y,z]\n",
    "    wrists = np.zeros((2, 2))               # 2 wrists, [x,y]\n",
    "    confidence_scores = np.zeros(2)         # Confidence scores for [dominant, non-dominant]\n",
    "    interpolation_scores = np.zeros(2) #Interpolation scores for [dominant, non-dominant]. Used later.\n",
    "    detection_status = np.zeros(2, dtype=np.int32)  # Binary detection status [dominant, non-dominant]\n",
    "    nose_to_wrist_dist = np.zeros((2, 2))\n",
    "    \n",
    "    # 1.4: Process hand landmarks if hands are detected\n",
    "    if hand_detection_result.hand_landmarks and hand_detection_result.handedness:\n",
    "        dom_hand_found = False\n",
    "        non_dom_hand_found = False\n",
    "        \n",
    "        # First, find the dominant and non-dominant hands in detection results\n",
    "        for idx, handedness in enumerate(hand_detection_result.handedness):\n",
    "            hand_type = handedness[0].category_name  # 'Left' or 'Right'\n",
    "            hand_score = handedness[0].score  # Confidence score for the handedness classification\n",
    "            \n",
    "            if hand_type == dominand_hand:\n",
    "                # This is the dominant hand\n",
    "                dom_hand_found = True\n",
    "                detection_status[0] = 1  # Set detection status to 1 (detected)\n",
    "                confidence_scores[0] = hand_score  # Store confidence score\n",
    "                interpolation_scores[0] = 1\n",
    "                \n",
    "                # Store dominant hand wrist coordinates [x,y]\n",
    "                dom_hand_landmarks = hand_detection_result.hand_landmarks[idx]\n",
    "                wrists[0, 0] = dom_hand_landmarks[0].x\n",
    "                wrists[0, 1] = dom_hand_landmarks[0].y\n",
    "                \n",
    "                # Store all other dominant hand landmarks (excluding wrist)\n",
    "                for i in range(1, 21):  # Landmarks 1-20 (skipping wrist which is index 0)\n",
    "                    dom_landmarks[i-1, 0] = dom_hand_landmarks[i].x\n",
    "                    dom_landmarks[i-1, 1] = dom_hand_landmarks[i].y\n",
    "                    dom_landmarks[i-1, 2] = dom_hand_landmarks[i].z\n",
    "                    \n",
    "            elif hand_type != dominand_hand:\n",
    "                # This is the non-dominant hand\n",
    "                non_dom_hand_found = True\n",
    "                detection_status[1] = 1  # Set detection status to 1 (detected)\n",
    "                confidence_scores[1] = hand_score  # Store confidence score\n",
    "                interpolation_scores[1] = 1\n",
    "                \n",
    "                # Store non-dominant hand wrist coordinates [x,y]\n",
    "                non_dom_hand_landmarks = hand_detection_result.hand_landmarks[idx]\n",
    "                wrists[1, 0] = non_dom_hand_landmarks[0].x\n",
    "                wrists[1, 1] = non_dom_hand_landmarks[0].y\n",
    "                \n",
    "                # Store all other non-dominant hand landmarks (excluding wrist)\n",
    "                for i in range(1, 21):  # Landmarks 1-20 (skipping wrist)\n",
    "                    non_dom_landmarks[i-1, 0] = non_dom_hand_landmarks[i].x\n",
    "                    non_dom_landmarks[i-1, 1] = non_dom_hand_landmarks[i].y\n",
    "                    non_dom_landmarks[i-1, 2] = non_dom_hand_landmarks[i].z\n",
    "                    \n",
    "        # Log information about which hands were found\n",
    "        print(f\"Dominant hand ({dominand_hand}) detected: {dom_hand_found}\")\n",
    "        print(f\"Non-dominant hand detected: {non_dom_hand_found}\")\n",
    "    \n",
    "\n",
    "   # PART 2: FACE LANDMARK DETECTION (If requested)\n",
    "    if output_face_blendshapes:\n",
    "        try:\n",
    "            # 2.1: Configure the face landmarker\n",
    "            face_base_options = python.BaseOptions(\n",
    "                model_asset_path=face_model_path\n",
    "            )\n",
    "            \n",
    "            # Configure face detection options\n",
    "            face_options = vision.FaceLandmarkerOptions(\n",
    "                base_options=face_base_options,\n",
    "                min_face_detection_confidence=min_face_detection_confidence,\n",
    "                min_face_presence_confidence=min_face_presence_confidence,\n",
    "                output_face_blendshapes=True,\n",
    "                num_faces=1,\n",
    "                running_mode=VisionRunningMode.IMAGE\n",
    "            )\n",
    "            \n",
    "            # Create the face detector\n",
    "            face_detector = vision.FaceLandmarker.create_from_options(face_options)\n",
    "            \n",
    "            # 2.2: Detect face landmarks (reuse the same image)\n",
    "            face_detection_result = face_detector.detect(image)\n",
    "            \n",
    "            # 2.3: Process face blendshapes if face is detected\n",
    "            if (face_detection_result.face_blendshapes and len(face_detection_result.face_blendshapes) > 0 and\n",
    "                face_detection_result.face_landmarks and len(face_detection_result.face_landmarks) > 0):\n",
    "                \n",
    "                # Set face detected flag to 1\n",
    "                face_detected = 1\n",
    "                \n",
    "                # Get all blendshapes from the first face\n",
    "                all_blendshapes = face_detection_result.face_blendshapes[0]\n",
    "                \n",
    "                # Initialize blendshape_scores with the correct size to hold all blendshapes\n",
    "                # Assuming MediaPipe returns all 52 blendshapes\n",
    "                blendshape_scores = np.zeros(len(all_blendshapes))\n",
    "                \n",
    "                # Fill the blendshape_scores array with ALL scores\n",
    "                for i in range(len(all_blendshapes)):\n",
    "                    blendshape_scores[i] = all_blendshapes[i].score\n",
    "                \n",
    "                # Get nose coordinates\n",
    "                nose = face_detection_result.face_landmarks[0][4]\n",
    "                nose_landmark[0] = nose.x\n",
    "                nose_landmark[1] = nose.y\n",
    "    \n",
    "                # Get eye coordinates\n",
    "                left_eye = face_detection_result.face_landmarks[0][473]\n",
    "                left_eye_landmark[0] = left_eye.x\n",
    "                left_eye_landmark[1] = left_eye.y\n",
    "    \n",
    "                right_eye = face_detection_result.face_landmarks[0][468]\n",
    "                right_eye_landmark[0] = right_eye.x\n",
    "                right_eye_landmark[1] = right_eye.y\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during face detection: {e}\")\n",
    "            # Keep default zero values for face outputs if detection fails\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PART 3: VISUALIZATION\n",
    "    if visualize:\n",
    "        # Load the image with OpenCV for visualization\n",
    "        img_cv = cv2.imread(image_path)\n",
    "        img_height, img_width, _ = img_cv.shape\n",
    "\n",
    "        # 3.1: Draw hand landmarks if hands are detected\n",
    "        if hand_detection_result.hand_landmarks:\n",
    "            print(f\"Visualizing {len(hand_detection_result.hand_landmarks)} hands\")\n",
    "            \n",
    "            # Define connections between landmarks for hand skeleton\n",
    "            connections = [\n",
    "                # Thumb connections\n",
    "                (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "                # Index finger connections\n",
    "                (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "                # Middle finger connections\n",
    "                (0, 9), (9, 10), (10, 11), (11, 12),\n",
    "                # Ring finger connections\n",
    "                (0, 13), (13, 14), (14, 15), (15, 16),\n",
    "                # Pinky finger connections\n",
    "                (0, 17), (17, 18), (18, 19), (19, 20),\n",
    "                # Palm connections\n",
    "                (0, 5), (5, 9), (9, 13), (13, 17)\n",
    "            ]\n",
    "            \n",
    "            for idx, hand_landmarks in enumerate(hand_detection_result.hand_landmarks):\n",
    "                # Determine if this is the dominant hand\n",
    "                is_dominant = False\n",
    "                if hand_detection_result.handedness:\n",
    "                    hand_type = hand_detection_result.handedness[idx][0].category_name\n",
    "                    is_dominant = (hand_type == dominand_hand)\n",
    "                \n",
    "                # Use different colors for dominant vs non-dominant hand\n",
    "                hand_color = (0, 0, 255) if is_dominant else (255, 0, 0)  # Blue for dominant, Red for non-dominant\n",
    "                \n",
    "                # Draw all landmark points\n",
    "                for landmark in hand_landmarks:\n",
    "                    # Convert normalized coordinates to pixel coordinates\n",
    "                    x = int(landmark.x * img_width)\n",
    "                    y = int(landmark.y * img_height)\n",
    "                    \n",
    "                    # Draw the landmark point\n",
    "                    cv2.circle(img_cv, (x, y), 5, hand_color, -1)\n",
    "                \n",
    "                # Draw connections between landmarks (hand skeleton)\n",
    "                for connection in connections:\n",
    "                    start_idx, end_idx = connection\n",
    "                    \n",
    "                    if start_idx < len(hand_landmarks) and end_idx < len(hand_landmarks):\n",
    "                        start_point = hand_landmarks[start_idx]\n",
    "                        end_point = hand_landmarks[end_idx]\n",
    "                        \n",
    "                        # Convert normalized coordinates to pixel coordinates\n",
    "                        start_x = int(start_point.x * img_width)\n",
    "                        start_y = int(start_point.y * img_height)\n",
    "                        end_x = int(end_point.x * img_width)\n",
    "                        end_y = int(end_point.y * img_height)\n",
    "                        \n",
    "                        # Draw the connection line\n",
    "                        cv2.line(img_cv, (start_x, start_y), (end_x, end_y), hand_color, 2)\n",
    "                \n",
    "                # Add hand type label (Left/Right, Dominant/Non-dominant)\n",
    "                if hand_detection_result.handedness:\n",
    "                    handedness = hand_detection_result.handedness[idx]\n",
    "                    hand_type = handedness[0].category_name  # 'Left' or 'Right'\n",
    "                    hand_score = handedness[0].score\n",
    "                    dom_status = \"Dominant\" if hand_type == dominand_hand else \"Non-dominant\"\n",
    "                    cv2.putText(img_cv, f\"{hand_type} Hand - {dom_status} ({hand_score:.2f})\", \n",
    "                            (10, 30 + idx * 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            0.8, hand_color, 2)\n",
    "                    \n",
    "                    # Calculate and draw a bounding box\n",
    "                    x_coords = [landmark.x for landmark in hand_landmarks]\n",
    "                    y_coords = [landmark.y for landmark in hand_landmarks]\n",
    "                    min_x, max_x = min(x_coords), max(x_coords)\n",
    "                    min_y, max_y = min(y_coords), max(y_coords)\n",
    "                    \n",
    "                    # Convert to pixel coordinates\n",
    "                    min_x, max_x = int(min_x * img_width), int(max_x * img_width)\n",
    "                    min_y, max_y = int(min_y * img_height), int(max_y * img_height)\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(img_cv, (min_x, min_y), (max_x, max_y), hand_color, 2)\n",
    "\n",
    "        # 3.2: Draw Nose if face was detected\n",
    "        if face_detected == 1:\n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            face_x = int(nose_landmark[0] * img_width)\n",
    "            face_y = int(nose_landmark[1] * img_height)\n",
    "            \n",
    "            # Draw the Nose with a distinctive color and size\n",
    "            cv2.circle(img_cv, (face_x, face_y), 8, (0, 255, 255), -1)  # Yellow circle\n",
    "            cv2.putText(img_cv, \"Nose\", (face_x + 10, face_y), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "            # Draw eyes\n",
    "            left_eye_x = int(left_eye_landmark[0] * img_width)\n",
    "            left_eye_y = int(left_eye_landmark[1] * img_height)\n",
    "            right_eye_x = int(right_eye_landmark[0] * img_width)\n",
    "            right_eye_y = int(right_eye_landmark[1] * img_height)\n",
    "            \n",
    "            cv2.circle(img_cv, (left_eye_x, left_eye_y), 6, (255, 255, 0), -1)  # Cyan circle\n",
    "            cv2.circle(img_cv, (right_eye_x, right_eye_y), 6, (255, 255, 0), -1)  # Cyan circle\n",
    "            cv2.line(img_cv, (left_eye_x, left_eye_y), (right_eye_x, right_eye_y), (255, 255, 0), 2)\n",
    "        # 3.3: Add detection status information to visualization\n",
    "        y_pos = img_height - 80\n",
    "        hand_status_text = f\"Hand Detection: Dom={detection_status[0]}, Non-Dom={detection_status[1]}\"\n",
    "        hand_conf_text = f\"Hand Confidence: Dom={confidence_scores[0]:.2f}, Non-Dom={confidence_scores[1]:.2f}\"\n",
    "        face_status_text = f\"Face Detection: {face_detected}\"\n",
    "        \n",
    "        cv2.putText(img_cv, hand_status_text, (10, y_pos), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img_cv, hand_conf_text, (10, y_pos + 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img_cv, face_status_text, (10, y_pos + 60), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # 3.4: Display the result\n",
    "        cv2.imshow('Hand and Face Landmarks', img_cv)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    if face_detected==1:\n",
    "        #Calculate distance between the eyes\n",
    "        eyes_diff = right_eye_landmark-left_eye_landmark\n",
    "        eyes_distance = np.sqrt(eyes_diff.dot(eyes_diff))\n",
    "        if detection_status[0]==1 and detection_status[1]==1:\n",
    "            nose_to_wrist_dist = (wrists-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / eyes_distance\n",
    "        elif detection_status[0]==1 and detection_status[1]==0:\n",
    "            nose_to_wrist_dist[0, :] = (wrists[0, :]-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "        elif detection_status[0]==0 and detection_status[1]==1:\n",
    "            nose_to_wrist_dist[1,:] = (wrists[1,:]-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "        \n",
    "    elif face_detected==0 and detection_status[0]==1:\n",
    "        #Calculate palm width distance as fallback scaling factor\n",
    "        palm_width_diff = dom_landmarks[5, :]- dom_landmarks[17, :]\n",
    "        palm_width_dist = np.sqrt(palm_width_diff.dot(palm_width_diff))\n",
    "        if detection_status[1]==1:\n",
    "            nose_to_wrist_dist = (wrists-nose_landmark) / palm_width_dist\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / palm_width_dist\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / palm_width_dist\n",
    "        elif detection_status[1]==0:\n",
    "            nose_to_wrist_dist[0,:] = (wrists[0,:]-nose_landmark) / palm_width_dist\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / palm_width_dist\n",
    "    elif face_detected==0 and detection_status[0]==0 and detection_status[1]==1:\n",
    "        #Calculate palm width distance as fallback scaling factor\n",
    "        palm_width_diff = non_dom_landmarks[5, :]- non_dom_landmarks[17, :]\n",
    "        palm_width_dist = np.sqrt(palm_width_diff.dot(palm_width_diff))\n",
    "        nose_to_wrist_dist[1,:] = (wrists[1,:]-nose_landmark) / palm_width_dist\n",
    "        #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "        non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / palm_width_dist\n",
    "    \n",
    "\n",
    "    \n",
    "    # Return all requested outputs\n",
    "    return dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743496270.375183   15687 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743496270.474735   15725 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 560.28.03), renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1743496270.501183   15728 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743496270.518936   15734 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743496270.566415   15729 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "I0000 00:00:1743496270.566851   15687 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743496270.646873   15739 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 560.28.03), renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2\n",
      "W0000 00:00:1743496270.647184   15687 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1743496270.654167   15742 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743496270.678560   15752 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "Visualizing 1 hands\n"
     ]
    }
   ],
   "source": [
    "lol = detect(photo_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, num_hands=2, dominand_hand='Right', visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_detect(image_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, \n",
    "                   min_face_detection_confidence=0.5, min_face_presence_confidence=0.5, \n",
    "                   num_hands=2, dominand_hand='Right', visualize=False, output_face_blendshapes=True,\n",
    "                   max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Adaptively detects hands and face by progressively lowering detection thresholds\n",
    "    for undetected body parts.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        min_hand_detection_confidence (float): Initial confidence threshold for hand detection\n",
    "        min_hand_presence_confidence (float): Initial confidence threshold for hand presence\n",
    "        min_face_detection_confidence (float): Initial confidence threshold for face detection\n",
    "        min_face_presence_confidence (float): Initial confidence threshold for face presence\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        visualize (bool): Whether to visualize the final results\n",
    "        output_face_blendshapes (bool): Whether to detect and extract face blendshapes\n",
    "        max_attempts (int): Maximum number of detection attempts with lowered thresholds\n",
    "        threshold_reduction_factor (float): Factor to multiply thresholds by on each attempt (0-1)\n",
    "        min_threshold (float): Minimum threshold to prevent excessive lowering\n",
    "        \n",
    "    Returns:\n",
    "        Same output as the detect() function\n",
    "    \"\"\"\n",
    "    # Import the original detect function\n",
    "    #from your_module import detect  # Replace with actual module name\n",
    "    \n",
    "    # Store original thresholds\n",
    "    orig_hand_detection_conf = min_hand_detection_confidence\n",
    "    orig_hand_presence_conf = min_hand_presence_confidence\n",
    "    orig_face_detection_conf = min_face_detection_confidence\n",
    "    orig_face_presence_conf = min_face_presence_confidence\n",
    "    \n",
    "    # Initialize best results and detection status\n",
    "    best_results = None\n",
    "    best_detection_status = [0, 0]  # [dom_hand, non_dom_hand]\n",
    "    best_face_detected = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    # Try detection with progressively lower thresholds\n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"\\n--- Attempt {attempt+1}/{max_attempts} ---\")\n",
    "        \n",
    "        # Calculate current thresholds\n",
    "        if attempt > 0:\n",
    "            # Only lower thresholds for undetected parts\n",
    "            # For hands\n",
    "            if best_detection_status[0] == 0:  # Dominant hand not detected\n",
    "                hand_detection_conf_dom = max(orig_hand_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                hand_presence_conf_dom = max(orig_hand_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering dominant hand thresholds: {hand_detection_conf_dom:.3f}, {hand_presence_conf_dom:.3f}\")\n",
    "            else:\n",
    "                hand_detection_conf_dom = orig_hand_detection_conf\n",
    "                hand_presence_conf_dom = orig_hand_presence_conf\n",
    "                \n",
    "            if best_detection_status[1] == 0:  # Non-dominant hand not detected\n",
    "                hand_detection_conf_non_dom = max(orig_hand_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                hand_presence_conf_non_dom = max(orig_hand_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering non-dominant hand thresholds: {hand_detection_conf_non_dom:.3f}, {hand_presence_conf_non_dom:.3f}\")\n",
    "            else:\n",
    "                hand_detection_conf_non_dom = orig_hand_detection_conf\n",
    "                hand_presence_conf_non_dom = orig_hand_presence_conf\n",
    "            \n",
    "            # Use the minimum of the two calculated thresholds (MediaPipe doesn't support per-hand thresholds)\n",
    "            current_hand_detection_conf = min(hand_detection_conf_dom, hand_detection_conf_non_dom)\n",
    "            current_hand_presence_conf = min(hand_presence_conf_dom, hand_presence_conf_non_dom)\n",
    "            \n",
    "            # For face\n",
    "            if output_face_blendshapes and best_face_detected == 0:  # Face not detected\n",
    "                current_face_detection_conf = max(orig_face_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                current_face_presence_conf = max(orig_face_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering face thresholds: {current_face_detection_conf:.3f}, {current_face_presence_conf:.3f}\")\n",
    "            else:\n",
    "                current_face_detection_conf = orig_face_detection_conf\n",
    "                current_face_presence_conf = orig_face_presence_conf\n",
    "        else:\n",
    "            # Use original thresholds for first attempt\n",
    "            current_hand_detection_conf = orig_hand_detection_conf\n",
    "            current_hand_presence_conf = orig_hand_presence_conf\n",
    "            current_face_detection_conf = orig_face_detection_conf\n",
    "            current_face_presence_conf = orig_face_presence_conf\n",
    "            print(f\"Using original thresholds: hands={current_hand_detection_conf}, face={current_face_detection_conf}\")\n",
    "        \n",
    "        # Call detect with current thresholds (don't visualize intermediate attempts)\n",
    "        results = detect(image_path,  hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                        min_hand_detection_confidence=current_hand_detection_conf,\n",
    "                        min_hand_presence_confidence=current_hand_presence_conf,\n",
    "                        min_face_detection_confidence=current_face_detection_conf,\n",
    "                        min_face_presence_confidence=current_face_presence_conf,\n",
    "                        num_hands=num_hands,\n",
    "                        dominand_hand=dominand_hand,\n",
    "                        visualize=False,\n",
    "                        output_face_blendshapes=output_face_blendshapes)\n",
    "        \n",
    "        # Unpack results\n",
    "        dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
    "        \n",
    "        # Compare with best results so far\n",
    "        current_detection_count = detection_status[0] + detection_status[1] + face_detected\n",
    "        best_detection_count = best_detection_status[0] + best_detection_status[1] + best_face_detected\n",
    "        \n",
    "        if best_results is None or current_detection_count > best_detection_count:\n",
    "            best_results = results\n",
    "            best_detection_status = [detection_status[0], detection_status[1]]\n",
    "            best_face_detected = face_detected\n",
    "            \n",
    "            print(f\"New best detection: dominant hand={detection_status[0]}, \"\n",
    "                  f\"non-dominant hand={detection_status[1]}, face={face_detected}\")\n",
    "            \n",
    "            # If everything is detected, we can stop early\n",
    "            if detection_status[0] == 1 and detection_status[1] == 1 and (face_detected == 1 or not output_face_blendshapes):\n",
    "                print(\"All body parts detected. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"No improvement in detection. Continuing to next attempt.\")\n",
    "    \n",
    "    # Run final detection with visualization if requested\n",
    "    if visualize:\n",
    "        print(\"\\n--- Visualizing final results ---\")\n",
    "        # Call detect one more time with the parameters that gave best results, but with visualize=True\n",
    "        # For simplicity, we'll just use the best thresholds we found\n",
    "        # This is slightly inefficient (one extra detection) but keeps the code clean\n",
    "        \n",
    "        # Determine which thresholds gave the best results\n",
    "        if best_detection_status[0] == 0:  # If dominant hand not detected in best result\n",
    "            hand_detection_conf = min_threshold\n",
    "            hand_presence_conf = min_threshold\n",
    "        else:\n",
    "            hand_detection_conf = orig_hand_detection_conf\n",
    "            hand_presence_conf = orig_hand_presence_conf\n",
    "            \n",
    "        if output_face_blendshapes and best_face_detected == 0:  # If face not detected in best result\n",
    "            face_detection_conf = min_threshold\n",
    "            face_presence_conf = min_threshold\n",
    "        else:\n",
    "            face_detection_conf = orig_face_detection_conf\n",
    "            face_presence_conf = orig_face_presence_conf\n",
    "        \n",
    "        # Run final detection with visualization\n",
    "        final_results = detect(image_path, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                              min_hand_detection_confidence=hand_detection_conf,\n",
    "                              min_hand_presence_confidence=hand_presence_conf, \n",
    "                              min_face_detection_confidence=face_detection_conf,\n",
    "                              min_face_presence_confidence=face_presence_conf,\n",
    "                              num_hands=num_hands,\n",
    "                              dominand_hand=dominand_hand,\n",
    "                              visualize=True,\n",
    "                              output_face_blendshapes=output_face_blendshapes)\n",
    "        \n",
    "        # Use these results if they're better than our best so far\n",
    "        dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = final_results\n",
    "        current_detection_count = detection_status[0] + detection_status[1] + face_detected\n",
    "        best_detection_count = best_detection_status[0] + best_detection_status[1] + best_face_detected\n",
    "        \n",
    "        if current_detection_count > best_detection_count:\n",
    "            best_results = final_results\n",
    "    \n",
    "    # Print final detection summary\n",
    "    print(\"\\n=== Detection Summary ===\")\n",
    "    dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = best_results\n",
    "    print(f\"Dominant hand detected: {detection_status[0] == 1} (confidence: {confidence_scores[0]:.3f})\")\n",
    "    print(f\"Non-dominant hand detected: {detection_status[1] == 1} (confidence: {confidence_scores[1]:.3f})\")\n",
    "    if output_face_blendshapes:\n",
    "        print(f\"Face detected: {face_detected == 1}\")\n",
    "    print(f\"Total detection attempts: {attempt+1}\")\n",
    "    return best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=0, non-dominant hand=1, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824986.936870    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824986.940818  284930 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.039888  284934 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.056971  284940 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.133708    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.135590  284946 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.136250    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.142800  284947 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.160368  284950 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.199556    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.202131  284962 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.238721  284964 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.252189  284966 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering dominant hand thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824987.304140    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.307504  284978 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.308167    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.314875  284980 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.507001  284979 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.543464    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.547164  284994 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.568495  284996 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.686856  285000 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Visualizing final results ---\n",
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824987.738748    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.741043  285010 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.741756    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.750388  285011 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.779803  285022 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.819273    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.822941  285026 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.853940  285027 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.874366  285035 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.927914    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.931627  285064 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.932330    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.944897  285072 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.965721  285078 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 1 hands\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: False (confidence: 0.000)\n",
      "Non-dominant hand detected: True (confidence: 0.948)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n"
     ]
    }
   ],
   "source": [
    "best_results = adaptive_detect(without_left_hand_path, hand_model_path=hand_model_path, face_model_path=face_model_path,  min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, num_hands=2, dominand_hand='Left', visualize=True,output_face_blendshapes=True,max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dom_landmarks, non_dom_landmarks, confidence_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[ 4.4302598 ,  5.79258936, -0.01529764],\n",
       "        [ 4.97843194,  5.60716573, -0.03023903],\n",
       "        [ 5.4227688 ,  5.474167  , -0.04531926],\n",
       "        [ 5.6396736 ,  5.29133956, -0.06161126],\n",
       "        [ 5.46952226,  5.95203357, -0.02546122],\n",
       "        [ 6.14746077,  5.96317265, -0.03837759],\n",
       "        [ 6.5033938 ,  5.98504281, -0.04724246],\n",
       "        [ 6.77242064,  5.99687042, -0.05398038],\n",
       "        [ 5.45199885,  6.19957023, -0.02953801],\n",
       "        [ 6.20066676,  6.22105431, -0.03755009],\n",
       "        [ 6.60851154,  6.23570749, -0.04424638],\n",
       "        [ 6.9013028 ,  6.24974116, -0.05119639],\n",
       "        [ 5.33187311,  6.40705239, -0.03495561],\n",
       "        [ 6.02315108,  6.45053573, -0.04479358],\n",
       "        [ 6.42136374,  6.46895391, -0.05380662],\n",
       "        [ 6.71558972,  6.47136295, -0.0608963 ],\n",
       "        [ 5.14666546,  6.58524524, -0.04154579],\n",
       "        [ 5.66482483,  6.65936167, -0.04902381],\n",
       "        [ 5.98583324,  6.67587705, -0.05188949],\n",
       "        [ 6.25919546,  6.67568786, -0.0540517 ]]),\n",
       " array([0.       , 0.9484275]),\n",
       " array([0., 1.]),\n",
       " array([0, 1], dtype=int32),\n",
       " array([2.92115374e-07, 1.33861089e-02, 1.24196718e-02, 2.88109779e-01,\n",
       "        5.75697683e-02, 3.07354219e-02, 1.32326995e-05, 1.58878031e-08,\n",
       "        8.72999664e-08, 3.63642573e-01, 4.11365360e-01, 6.62605762e-01,\n",
       "        6.69030905e-01, 2.73421267e-03, 5.51178098e-01, 5.63342750e-01,\n",
       "        1.03895655e-02, 1.23140477e-02, 8.84756818e-03, 2.24526033e-01,\n",
       "        2.50775576e-01, 4.66695800e-03, 1.89312676e-03, 3.73961666e-05,\n",
       "        1.19879853e-03, 7.60920672e-03, 4.57036338e-04, 1.92889536e-03,\n",
       "        8.89772832e-01, 8.59930277e-01, 8.38296987e-10, 1.49412671e-09,\n",
       "        1.53606525e-02, 1.33757144e-02, 1.26740182e-04, 8.43084490e-05,\n",
       "        7.94648603e-02, 9.92558002e-02, 8.17362487e-01, 1.14337606e-02,\n",
       "        1.67473480e-02, 3.32397074e-01, 3.43104475e-03, 1.09493383e-03,\n",
       "        1.44784761e-04, 2.09721227e-04, 8.35517653e-07, 5.79597008e-06,\n",
       "        3.15646721e-05, 2.39465880e-05, 5.11401367e-07, 3.26468843e-08]),\n",
       " 1,\n",
       " array([[ 0.        ,  0.        ],\n",
       "        [-1.39355698,  4.85907125]]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(frame_idx, total_frames, timestamp_formatted):\n",
    "    # Get terminal width to clear the entire line\n",
    "    terminal_width = shutil.get_terminal_size().columns\n",
    "    \n",
    "    # Create progress message\n",
    "    progress = f\"Processing frame {frame_idx}/{total_frames} (timestamp: {timestamp_formatted})\"\n",
    "    \n",
    "    # Pad with spaces to ensure previous text is overwritten\n",
    "    padded_progress = progress.ljust(terminal_width)\n",
    "    \n",
    "    # Print with carriage return and no newline\n",
    "    print(f\"\\r{padded_progress}\", end='', flush=True)\n",
    "    \n",
    "    # Print a newline when done (call this separately at the end)\n",
    "    if frame_idx == total_frames:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_video(video_path, adaptive_detect_func=adaptive_detect, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                 min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5,\n",
    "                 min_face_detection_confidence=0.5, min_face_presence_confidence=0.5,\n",
    "                 num_hands=2, output_face_blendshapes=True,\n",
    "                 max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2, \n",
    "                 frame_step=1, start_time_seconds=0, end_time_seconds=None,\n",
    "                 save_failure_screenshots=False):\n",
    "    \"\"\"\n",
    "    Process a video frame-by-frame using the adaptive_detect function and save results.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file\n",
    "        adaptive_detect_func: The adaptive detection function to use\n",
    "        min_hand_detection_confidence (float): Initial confidence threshold for hand detection\n",
    "        min_hand_presence_confidence (float): Initial confidence threshold for hand presence\n",
    "        min_face_detection_confidence (float): Initial confidence threshold for face detection\n",
    "        min_face_presence_confidence (float): Initial confidence threshold for face presence\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        output_face_blendshapes (bool): Whether to detect face blendshapes\n",
    "        max_attempts (int): Maximum detection attempts for adaptive detection\n",
    "        threshold_reduction_factor (float): Factor to reduce thresholds by\n",
    "        min_threshold (float): Minimum threshold limit\n",
    "        frame_step (int): Process every Nth frame (1 = all frames)\n",
    "        start_time_seconds (float): Time in seconds to start processing from\n",
    "        end_time_seconds (float): Time in seconds to end processing (None = process until end)\n",
    "        save_failure_screenshots (bool): Save screenshots for all frames with any detection failures\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the directory containing saved frame results\n",
    "    \"\"\"\n",
    "    # Extract video name for directory creation\n",
    "    video_path = Path(video_path)\n",
    "    video_name = video_path.stem  # Get filename without extension\n",
    "    \n",
    "    # Extract dominant hand information from filename\n",
    "    if video_name.endswith(\"_R\"):\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "    elif video_name.endswith(\"_L\"):\n",
    "        extracted_dominant_hand = \"Left\"\n",
    "    else:\n",
    "        # Default if not specified in filename\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "        print(f\"Warning: Could not determine dominant hand from filename, using default: {extracted_dominant_hand}\")\n",
    "\n",
    "    # Use the extracted dominant hand instead of the parameter\n",
    "    dominand_hand = extracted_dominant_hand\n",
    "    print(f\"Detected dominant hand from filename: {dominand_hand}\")\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(f\"{video_name}_landmarks\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create screenshots directory if screenshot option is enabled\n",
    "    screenshots_dir = None\n",
    "    if save_failure_screenshots:\n",
    "        screenshots_dir = output_dir / \"failure_screenshots\"\n",
    "        screenshots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a log file to track processing\n",
    "    log_file = output_dir / \"processing_log.txt\"\n",
    "    \n",
    "    # Create a detailed statistics file\n",
    "    stats_file = output_dir / \"detection_statistics.json\"\n",
    "\n",
    "    # Initialize statistics tracking\n",
    "    stats = {\n",
    "        \"video_info\": {\n",
    "            \"name\": video_name,\n",
    "            \"path\": str(video_path),\n",
    "            \"total_frames\": 0,\n",
    "            \"processed_frames\": 0,\n",
    "            \"fps\": 0,\n",
    "            \"duration_seconds\": 0,\n",
    "            \"start_time\": start_time_seconds,\n",
    "            \"end_time\": end_time_seconds,\n",
    "            \"dominant_hand\": dominand_hand,\n",
    "            \"processing_started\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"processing_completed\": None\n",
    "        },\n",
    "        \"detection_rates\": {\n",
    "            \"dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"non_dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"face\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"overall\": {\n",
    "                \"all_detected\": 0,\n",
    "                \"partial_detections\": 0,\n",
    "                \"no_detections\": 0,\n",
    "                \"success_rate\": 0\n",
    "            }\n",
    "        },\n",
    "        \"failed_frames\": {\n",
    "            \"dominant_hand_failures\": [],\n",
    "            \"non_dominant_hand_failures\": [],\n",
    "            \"face_failures\": [],\n",
    "            \"all_failures\": []\n",
    "        },\n",
    "        \"processing_performance\": {\n",
    "            \"average_processing_time_ms\": 0,\n",
    "            \"total_processing_time_seconds\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(log_file, \"w\") as log:\n",
    "        log.write(f\"Processing video: {video_path}\\n\")\n",
    "        log.write(f\"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        log.write(f\"Parameters:\\n\")\n",
    "        log.write(f\"  - frame_step: {frame_step}\\n\")\n",
    "        log.write(f\"  - start_time: {start_time_seconds} seconds\\n\")\n",
    "        if end_time_seconds is not None:\n",
    "            log.write(f\"  - end_time: {end_time_seconds} seconds\\n\")\n",
    "        log.write(f\"  - dominand_hand: {dominand_hand}\\n\")\n",
    "        log.write(f\"  - num_hands: {num_hands}\\n\")\n",
    "        log.write(f\"  - detection confidence thresholds: {min_hand_detection_confidence}, {min_face_detection_confidence}\\n\")\n",
    "        log.write(\"\\n--- Frame processing log ---\\n\")\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration_seconds = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    # Update stats with video info\n",
    "    stats[\"video_info\"][\"total_frames\"] = total_frames\n",
    "    stats[\"video_info\"][\"fps\"] = fps\n",
    "    stats[\"video_info\"][\"duration_seconds\"] = duration_seconds\n",
    "    if end_time_seconds==None:\n",
    "        stats[\"video_info\"][\"end_time\"] = duration_seconds\n",
    "    \n",
    "    # Convert time to frame indices\n",
    "    start_frame = int(max(0, start_time_seconds * fps))\n",
    "    \n",
    "    # Set end frame if specified\n",
    "    if end_time_seconds is not None:\n",
    "        end_frame = min(total_frames, int(end_time_seconds * fps))\n",
    "    else:\n",
    "        end_frame = total_frames\n",
    "    \n",
    "    print(f\"Video: {video_name}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"Duration: {duration_seconds:.2f} seconds\")\n",
    "    print(f\"Processing frames {start_frame} to {end_frame} (time {start_time_seconds:.2f}s to {end_time_seconds if end_time_seconds is not None else duration_seconds:.2f}s)\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Process frames\n",
    "    frame_idx = 0\n",
    "    processed_count = 0\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    # Skip to start_frame\n",
    "    if start_frame > 0:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        frame_idx = start_frame\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        while frame_idx < end_frame:\n",
    "            # Read the next frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "            \n",
    "            # Only process every frame_step frames\n",
    "            if (frame_idx - start_frame) % frame_step != 0:\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "                \n",
    "            # Get timestamp in milliseconds\n",
    "            timestamp_ms = int(frame_idx * 1000 / fps)\n",
    "            timestamp_formatted = f\"{timestamp_ms//60000:02d}m{(timestamp_ms//1000)%60:02d}s{timestamp_ms%1000:03d}ms\"\n",
    "            \n",
    "            # Temporary frame path\n",
    "            temp_frame_path = Path(temp_dir) / f\"temp_frame_{frame_idx}.jpg\"\n",
    "            \n",
    "            # Save the current frame as an image\n",
    "            cv2.imwrite(str(temp_frame_path), frame)\n",
    "            \n",
    "            # Process the frame with adaptive_detect\n",
    "            update_progress(frame_idx, total_frames, timestamp_formatted)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                with SuppressOutput():\n",
    "                # Use adaptive_detect on the frame\n",
    "                    results = adaptive_detect_func(\n",
    "                        str(temp_frame_path), hand_model_path, face_model_path,\n",
    "                        min_hand_detection_confidence=min_hand_detection_confidence,\n",
    "                        min_hand_presence_confidence=min_hand_presence_confidence,\n",
    "                        min_face_detection_confidence=min_face_detection_confidence,\n",
    "                        min_face_presence_confidence=min_face_presence_confidence,\n",
    "                        num_hands=num_hands,\n",
    "                        dominand_hand=dominand_hand,\n",
    "                        visualize=False,\n",
    "                        output_face_blendshapes=output_face_blendshapes,\n",
    "                        max_attempts=max_attempts,\n",
    "                        threshold_reduction_factor=threshold_reduction_factor,\n",
    "                        min_threshold=min_threshold\n",
    "                    )\n",
    "                \n",
    "                # Calculate processing time\n",
    "                proc_time = time.time() - start_time\n",
    "                total_processing_time += proc_time\n",
    "                \n",
    "                # Unpack results\n",
    "                dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
    "                \n",
    "                # Update detection statistics\n",
    "                dom_hand_detected = detection_status[0] == 1\n",
    "                non_dom_hand_detected = detection_status[1] == 1\n",
    "                face_was_detected = face_detected == 1\n",
    "                \n",
    "                if dom_hand_detected:\n",
    "                    stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"dominant_hand\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"dominant_hand_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                if non_dom_hand_detected:\n",
    "                    stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"non_dominant_hand\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"non_dominant_hand_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                if face_was_detected:\n",
    "                    stats[\"detection_rates\"][\"face\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"face\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"face_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                # Track combined detection status\n",
    "                detection_count = dom_hand_detected + non_dom_hand_detected + face_was_detected\n",
    "                \n",
    "                if detection_count == 3:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"all_detected\"] += 1\n",
    "                elif detection_count == 0:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"no_detections\"] += 1\n",
    "                    stats[\"failed_frames\"][\"all_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"partial_detections\"] += 1\n",
    "                \n",
    "                # Save screenshot if any detection failed and screenshots are enabled\n",
    "                if save_failure_screenshots and (not dom_hand_detected or not non_dom_hand_detected or not face_was_detected):\n",
    "                    # Create a detailed failure type description for the filename\n",
    "                    failure_type = []\n",
    "                    if not dom_hand_detected:\n",
    "                        failure_type.append(\"DomHand\")\n",
    "                    if not non_dom_hand_detected:\n",
    "                        failure_type.append(\"NonDomHand\")\n",
    "                    if not face_was_detected:\n",
    "                        failure_type.append(\"Face\")\n",
    "                    \n",
    "                    failure_str = \"_\".join(failure_type)\n",
    "                    screenshot_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}_missing_{failure_str}.jpg\"\n",
    "                    screenshot_path = screenshots_dir / screenshot_filename\n",
    "                    \n",
    "                    # Copy the frame to the screenshots directory\n",
    "                    cv2.imwrite(str(screenshot_path), frame)\n",
    "                    print(f\"Saved failure screenshot: {screenshot_filename}\")\n",
    "                \n",
    "                # Create output filename with frame info\n",
    "                output_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                output_path = output_dir / output_filename\n",
    "                \n",
    "                # Save all results in a single .npz file\n",
    "                np.savez(\n",
    "                    output_path,\n",
    "                    dom_landmarks=dom_landmarks,\n",
    "                    non_dom_landmarks=non_dom_landmarks,\n",
    "                    confidence_scores=confidence_scores,\n",
    "                    interpolation_scores=interpolation_scores,\n",
    "                    detection_status=detection_status,\n",
    "                    blendshape_scores=blendshape_scores,\n",
    "                    face_detected=face_detected,\n",
    "                    nose_to_wrist_dist=nose_to_wrist_dist,\n",
    "                    frame_idx=np.array([frame_idx]),\n",
    "                    timestamp_ms=np.array([timestamp_ms])\n",
    "                )\n",
    "                \n",
    "                # Update processing log\n",
    "                detection_summary = f\"Dom: {detection_status[0]}, Non-dom: {detection_status[1]}, Face: {face_detected}\"\n",
    "                log_entry = f\"Frame {frame_idx}: {detection_summary} (proc time: {proc_time:.2f}s)\\n\"\n",
    "                \n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(log_entry)\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {frame_idx}: {e}\")\n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(f\"Error on frame {frame_idx}: {str(e)}\\n\")\n",
    "            \n",
    "            # Clean up temporary frame file\n",
    "            if temp_frame_path.exists():\n",
    "                temp_frame_path.unlink()\n",
    "                \n",
    "            frame_idx += 1\n",
    "    \n",
    "    # Close the video file\n",
    "    cap.release()\n",
    "    \n",
    "    # Update final statistics\n",
    "    stats[\"video_info\"][\"processed_frames\"] = processed_count\n",
    "    stats[\"video_info\"][\"processing_completed\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Calculate detection rates\n",
    "    if processed_count > 0:\n",
    "        stats[\"detection_rates\"][\"dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"non_dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"face\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"face\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"overall\"][\"success_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"overall\"][\"all_detected\"] / processed_count * 100\n",
    "        )\n",
    "    \n",
    "    # Calculate processing performance\n",
    "    if processed_count > 0:\n",
    "        stats[\"processing_performance\"][\"average_processing_time_ms\"] = (\n",
    "            total_processing_time / processed_count * 1000\n",
    "        )\n",
    "    stats[\"processing_performance\"][\"total_processing_time_seconds\"] = total_processing_time\n",
    "    \n",
    "    # Save statistics to JSON file\n",
    "    with open(stats_file, \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # Add summary statistics to log file\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"\\n\\n===== PROCESSING SUMMARY =====\\n\")\n",
    "        log.write(f\"Completed at: {stats['video_info']['processing_completed']}\\n\")\n",
    "        log.write(f\"Frames processed: {processed_count} from {start_frame} to {min(end_frame, frame_idx-1)}\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION RATES:\\n\")\n",
    "        log.write(f\"  Dominant hand ({dominand_hand}): {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Non-dominant hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  All parts detected: {stats['detection_rates']['overall']['success_rate']:.1f}%\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION FAILURES:\\n\")\n",
    "        log.write(f\"  Frames with dominant hand failures: {len(stats['failed_frames']['dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with non-dominant hand failures: {len(stats['failed_frames']['non_dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with face failures: {len(stats['failed_frames']['face_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with all parts missing: {len(stats['failed_frames']['all_failures'])}\\n\\n\")\n",
    "        \n",
    "        log.write(\"PERFORMANCE:\\n\")\n",
    "        log.write(f\"  Average processing time per frame: {stats['processing_performance']['average_processing_time_ms']:.2f} ms\\n\")\n",
    "        log.write(f\"  Total processing time: {stats['processing_performance']['total_processing_time_seconds']:.2f} seconds\\n\")\n",
    "    \n",
    "    print(f\"\\n===== PROCESSING SUMMARY =====\")\n",
    "    print(f\"Processed {processed_count} frames\")\n",
    "    print(f\"Detection rates: Dom hand: {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "          f\"Non-dom hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "          f\"Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\")\n",
    "    print(f\"All parts detected in {stats['detection_rates']['overall']['success_rate']:.1f}% of frames\")\n",
    "    print(f\"Full statistics saved to: {stats_file}\")\n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "    \n",
    "    return str(output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks = data['dom_landmarks']\n",
    "    non_dom_landmarks = data['non_dom_landmarks']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores = data['blendshape_scores']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist = data['nose_to_wrist_dist']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    \n",
    "    return (dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores, face_detected, \n",
    "            nose_to_wrist_dist, frame_idx, timestamp_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m process_video(video_path=\u001b[43mvideo_path\u001b[49m, adaptive_detect_func=adaptive_detect, hand_model_path=hand_model_path, face_model_path=face_model_path,\n\u001b[32m      2\u001b[39m                  min_hand_detection_confidence=\u001b[32m0.5\u001b[39m, min_hand_presence_confidence=\u001b[32m0.5\u001b[39m,\n\u001b[32m      3\u001b[39m                  min_face_detection_confidence=\u001b[32m0.5\u001b[39m, min_face_presence_confidence=\u001b[32m0.5\u001b[39m,\n\u001b[32m      4\u001b[39m                  num_hands=\u001b[32m2\u001b[39m, output_face_blendshapes=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m                  max_attempts=\u001b[32m3\u001b[39m, threshold_reduction_factor=\u001b[32m0.7\u001b[39m, min_threshold=\u001b[32m0.2\u001b[39m, \n\u001b[32m      6\u001b[39m                  frame_step=\u001b[32m1\u001b[39m, start_time_seconds=\u001b[32m30.2\u001b[39m, end_time_seconds=\u001b[32m60.4\u001b[39m,\n\u001b[32m      7\u001b[39m                  save_failure_screenshots=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'video_path' is not defined"
     ]
    }
   ],
   "source": [
    "process_video(video_path=video_path, adaptive_detect_func=adaptive_detect, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                 min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5,\n",
    "                 min_face_detection_confidence=0.5, min_face_presence_confidence=0.5,\n",
    "                 num_hands=2, output_face_blendshapes=True,\n",
    "                 max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2, \n",
    "                 frame_step=1, start_time_seconds=30.2, end_time_seconds=60.4,\n",
    "                 save_failure_screenshots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = load_frame_data(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[7][1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interpolation_frames(x, nums_list):\n",
    "    \"\"\"\n",
    "    Returns integers in the range [x-5, x+5] that are not equal to x\n",
    "    and are not in nums_list.\n",
    "    \n",
    "    Args:\n",
    "        x (int): The reference integer\n",
    "        nums_list (list): A list of integers\n",
    "        \n",
    "    Returns:\n",
    "        list: Integers in [x-5, x+5] excluding x and elements in nums_list\n",
    "    \"\"\"\n",
    "    # Create the set of all integers in the range [x-5, x+5]\n",
    "    all_range = set(range(x-5, x+6))  # +6 because range is exclusive at upper bound\n",
    "    \n",
    "    # Remove x itself\n",
    "    all_range.discard(x)\n",
    "    \n",
    "    # Remove numbers that are in the input list\n",
    "    result = all_range - set(nums_list)\n",
    "    \n",
    "    # Convert back to a list and return\n",
    "    return sorted(list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def find_file_with_partial_name(partial_name, search_dir='.', recursive=False):\n",
    "    \"\"\"\n",
    "    Find files that start with the given partial name.\n",
    "    \n",
    "    Args:\n",
    "        partial_name (str): Partial file name to match\n",
    "        search_dir (str): Directory to search in (default: current directory)\n",
    "        recursive (bool): Whether to search in subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        list: Complete paths of all matching files\n",
    "    \"\"\"\n",
    "    # Create a search pattern for files starting with the partial name\n",
    "    search_pattern = os.path.join(search_dir, f\"{partial_name}*\")\n",
    "    \n",
    "    # Use recursive glob if requested\n",
    "    if recursive:\n",
    "        matches = []\n",
    "        for root, _, _ in os.walk(search_dir):\n",
    "            matches.extend(glob.glob(os.path.join(root, f\"{os.path.basename(partial_name)}*\")))\n",
    "        return matches\n",
    "    else:\n",
    "        return glob.glob(search_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers_on_both_sides(x, missing_numbers):\n",
    "    \"\"\"\n",
    "    Checks if the list of missing numbers has at least one number smaller than x\n",
    "    AND at least one number larger than x.\n",
    "    \n",
    "    Args:\n",
    "        x (int): The reference integer\n",
    "        missing_numbers (list): Output from find_missing_numbers(x, nums_list)\n",
    "        \n",
    "    Returns:\n",
    "        bool: False if all numbers are either all smaller or all larger than x.\n",
    "              True if there's at least one smaller and one larger number.\n",
    "    \"\"\"\n",
    "    has_smaller = False\n",
    "    has_larger = False\n",
    "    \n",
    "    for num in missing_numbers:\n",
    "        if num < x:\n",
    "            has_smaller = True\n",
    "        elif num > x:\n",
    "            has_larger = True\n",
    "            \n",
    "        # Early exit if we found both smaller and larger numbers\n",
    "        if has_smaller and has_larger:\n",
    "            return True\n",
    "    \n",
    "    # If we get here, we didn't find both smaller and larger numbers\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_npz_file(file_path, modifications):\n",
    "    \"\"\"\n",
    "    Load a .npz file, modify existing arrays and add new ones, then save it back.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .npz file\n",
    "        modifications (dict): Dictionary with keys as array names and values as new arrays\n",
    "                             or functions that take the original array and return a modified version\n",
    "    \"\"\"\n",
    "    # Load the npz file\n",
    "    with np.load(file_path) as data:\n",
    "        # Create a copy of all arrays\n",
    "        arrays = {name: data[name] for name in data.files}\n",
    "    \n",
    "    # Apply modifications and add new arrays\n",
    "    for name, modification in modifications.items():\n",
    "        if name in arrays:\n",
    "            if callable(modification):\n",
    "                # If the modification is a function, apply it to the original array\n",
    "                arrays[name] = modification(arrays[name])\n",
    "            else:\n",
    "                # Otherwise, replace the array\n",
    "                arrays[name] = modification\n",
    "        else:\n",
    "            # Add new array\n",
    "            arrays[name] = modification\n",
    "            print(f\"Adding new array '{name}' to the file\")\n",
    "    \n",
    "    # Save back to the file with same format\n",
    "    np.savez(file_path, **arrays)\n",
    "    \n",
    "    print(f\"Successfully modified/added {len(modifications)} arrays in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_undetected_hand_landmarks(directory_path):  \n",
    "    \"\"\"\n",
    "    Interpolate landmarks for frames where hand detection failed.\n",
    "    \"\"\"\n",
    "    print(f\"Starting interpolation for directory: {directory_path}\")\n",
    "    \n",
    "    # Load detection statistics JSON\n",
    "    with open(os.path.join(directory_path, 'detection_statistics.json')) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    first_frame_number = round(data['video_info']['fps'] * data['video_info']['start_time'])\n",
    "    final_frame_number = round(data['video_info']['fps'] * data['video_info']['end_time'])\n",
    "    \n",
    "    print(f\"Processing frames range: {first_frame_number} to {final_frame_number}\")\n",
    "    \n",
    "    # Maximum possible sum of weights for normalization (when all 10 frames are available)\n",
    "    MAX_WEIGHT_SUM = 2.92722222\n",
    "    \n",
    "    # Process non-dominant hand failures\n",
    "    print(\"Processing non-dominant hand failures...\")\n",
    "    missing_non_dominant_frame_list = [frame['frame'] for frame in data['failed_frames']['non_dominant_hand_failures']]\n",
    "    \n",
    "    non_dom_interpolated_count = 0\n",
    "    \n",
    "    for missing_frame in data['failed_frames']['non_dominant_hand_failures']:\n",
    "        frame_number = missing_frame['frame']\n",
    "        filepath = missing_frame['file']\n",
    "        \n",
    "        # Only interpolate frames not at the edges of the video\n",
    "        if (frame_number - 5) <= first_frame_number or (frame_number + 5) >= final_frame_number:\n",
    "            print(f\"Skipping frame {frame_number} - too close to video boundary\")\n",
    "            continue\n",
    "        \n",
    "        # Find frames with valid detections for interpolation\n",
    "        interpolation_frames = find_interpolation_frames(frame_number, missing_non_dominant_frame_list)\n",
    "        \n",
    "        if not interpolation_frames:\n",
    "            print(f\"No valid frames found for interpolating frame {frame_number}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate interpolated landmarks\n",
    "        interpolation_weights_sum = 0\n",
    "        interpolated_coordinates = np.zeros(shape=(20, 3))\n",
    "        interpolated_wrist_to_nose = np.zeros(2)\n",
    "        \n",
    "        for interp_frame in interpolation_frames:\n",
    "            weight = 1 / ((frame_number - interp_frame) ** 2)\n",
    "            interpolation_weights_sum += weight\n",
    "            \n",
    "            # Find and load the reference frame\n",
    "            interp_partial_filename = data['video_info']['name'] + f\"_frame{interp_frame:06d}\"\n",
    "            try:\n",
    "                interp_files = find_file_with_partial_name(\n",
    "                    interp_partial_filename, \n",
    "                    search_dir=directory_path, \n",
    "                    recursive=False\n",
    "                )\n",
    "                \n",
    "                if not interp_files:\n",
    "                    print(f\"Warning: Could not find file for frame {interp_frame}\")\n",
    "                    continue\n",
    "                    \n",
    "                interp_filepath = interp_files[0]\n",
    "                \n",
    "                # Load the frame data - index 1 for non-dominant hand landmarks\n",
    "                frame_data = load_frame_data(interp_filepath)\n",
    "                non_dom_landmarks = frame_data[1]  # Correct index for non-dominant hand\n",
    "                nose_to_wrist_non_dom = frame_data[7][1, :]\n",
    "                \n",
    "                \n",
    "                # Add weighted contribution\n",
    "                interpolated_coordinates += weight * non_dom_landmarks\n",
    "                interpolated_wrist_to_nose += weight * nose_to_wrist_non_dom\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {interp_frame}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize by sum of weights (crucial step!)\n",
    "        if interpolation_weights_sum > 0:\n",
    "            interpolated_coordinates /= interpolation_weights_sum\n",
    "            interpolated_wrist_to_nose /= interpolation_weights_sum\n",
    "            \n",
    "            # Calculate confidence based on weights and frame distribution\n",
    "            has_frames_on_both_sides = has_numbers_on_both_sides(frame_number, interpolation_frames)\n",
    "            \n",
    "            if has_frames_on_both_sides:\n",
    "                interpolation_confidence = interpolation_weights_sum / MAX_WEIGHT_SUM\n",
    "            else:\n",
    "                interpolation_confidence = (interpolation_weights_sum / MAX_WEIGHT_SUM) * 0.8\n",
    "                \n",
    "            print(f\"Frame {frame_number}: Interpolated with confidence {interpolation_confidence:.2f}\")\n",
    "            \n",
    "            # Update the file with interpolated data\n",
    "            def update_interp_scores(arr):\n",
    "                new_arr = arr.copy()\n",
    "                new_arr[1] = interpolation_confidence  # Index 1 for non-dominant hand\n",
    "                return new_arr\n",
    "            \n",
    "            def update_nose_to_wrist_scores(matrix):\n",
    "                new_matrix = matrix.copy()\n",
    "                new_matrix[1, :] = interpolated_wrist_to_nose\n",
    "                return new_matrix\n",
    "                \n",
    "            modifications = {\n",
    "                'non_dom_landmarks': interpolated_coordinates,\n",
    "                'interpolation_scores': update_interp_scores,\n",
    "                'nose_to_wrist_dist': update_nose_to_wrist_scores\n",
    "            }\n",
    "            \n",
    "            modify_npz_file(\n",
    "                file_path=os.path.join(directory_path, filepath),\n",
    "                modifications=modifications\n",
    "            )\n",
    "            \n",
    "            non_dom_interpolated_count += 1\n",
    "    \n",
    "    # Process dominant hand failures\n",
    "    print(f\"Interpolated {non_dom_interpolated_count} non-dominant hand frames\")\n",
    "    print(\"Processing dominant hand failures...\")\n",
    "    \n",
    "    missing_dominant_frame_list = [frame['frame'] for frame in data['failed_frames']['dominant_hand_failures']]\n",
    "    \n",
    "    dom_interpolated_count = 0\n",
    "    \n",
    "    for missing_frame in data['failed_frames']['dominant_hand_failures']:\n",
    "        frame_number = missing_frame['frame']\n",
    "        filepath = missing_frame['file']\n",
    "        \n",
    "        # Only interpolate frames not at the edges of the video\n",
    "        if (frame_number - 5) <= first_frame_number or (frame_number + 5) >= final_frame_number:\n",
    "            continue\n",
    "        \n",
    "        # Find frames with valid detections for interpolation\n",
    "        interpolation_frames = find_interpolation_frames(frame_number, missing_dominant_frame_list)\n",
    "        \n",
    "        if not interpolation_frames:\n",
    "            continue\n",
    "        \n",
    "        # Calculate interpolated landmarks\n",
    "        interpolation_weights_sum = 0\n",
    "        interpolated_coordinates = np.zeros(shape=(20, 3))\n",
    "        interpolated_wrist_to_nose = np.zeros(2)\n",
    "        \n",
    "        for interp_frame in interpolation_frames:\n",
    "            weight = 1 / ((frame_number - interp_frame) ** 2)\n",
    "            interpolation_weights_sum += weight\n",
    "            \n",
    "            # Find and load the reference frame\n",
    "            interp_partial_filename = data['video_info']['name'] + f\"_frame{interp_frame:06d}\"\n",
    "            try:\n",
    "                interp_files = find_file_with_partial_name(\n",
    "                    interp_partial_filename, \n",
    "                    search_dir=directory_path, \n",
    "                    recursive=False\n",
    "                )\n",
    "                \n",
    "                if not interp_files:\n",
    "                    continue\n",
    "                    \n",
    "                interp_filepath = interp_files[0]\n",
    "                \n",
    "                # Load the frame data - index 0 for dominant hand landmarks\n",
    "                frame_data = load_frame_data(interp_filepath)\n",
    "                dom_landmarks = frame_data[0]  # Correct index for dominant hand\n",
    "                nose_to_wrist_dom = frame_data[7][0, :]\n",
    "                \n",
    "                # Add weighted contribution\n",
    "                interpolated_coordinates += weight * dom_landmarks\n",
    "                interpolated_wrist_to_nose += weight * nose_to_wrist_non_dom\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {interp_frame}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize by sum of weights\n",
    "        if interpolation_weights_sum > 0:\n",
    "            interpolated_coordinates /= interpolation_weights_sum\n",
    "            interpolated_wrist_to_nose /= interpolation_weights_sum\n",
    "            # Calculate confidence based on weights and frame distribution\n",
    "            has_frames_on_both_sides = has_numbers_on_both_sides(frame_number, interpolation_frames)\n",
    "            \n",
    "            if has_frames_on_both_sides:\n",
    "                interpolation_confidence = interpolation_weights_sum / MAX_WEIGHT_SUM\n",
    "            else:\n",
    "                interpolation_confidence = (interpolation_weights_sum / MAX_WEIGHT_SUM) * 0.8\n",
    "            \n",
    "            # Update the file with interpolated data\n",
    "            def update_interp_scores(arr):\n",
    "                new_arr = arr.copy()\n",
    "                new_arr[0] = interpolation_confidence  # Index 0 for dominant hand\n",
    "                return new_arr\n",
    "            \n",
    "            def update_nose_to_wrist_scores(matrix):\n",
    "                new_matrix = matrix.copy()\n",
    "                new_matrix[0, :] = interpolated_wrist_to_nose\n",
    "                return new_matrix\n",
    "                \n",
    "            modifications = {\n",
    "                'dom_landmarks': interpolated_coordinates,\n",
    "                'interpolation_scores': update_interp_scores,\n",
    "                'nose_to_wrist_dist': update_nose_to_wrist_scores\n",
    "            }\n",
    "            \n",
    "\n",
    "            modify_npz_file(\n",
    "                file_path=os.path.join(directory_path, filepath),\n",
    "                modifications=modifications\n",
    "            )\n",
    "            \n",
    "            dom_interpolated_count += 1\n",
    "    \n",
    "    print(f\"Interpolated {dom_interpolated_count} dominant hand frames\")\n",
    "    print(f\"Total interpolated: {non_dom_interpolated_count + dom_interpolated_count} frames\")\n",
    "    \n",
    "    return non_dom_interpolated_count + dom_interpolated_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting interpolation for directory: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\n",
      "Processing frames range: 30 to 60\n",
      "Processing non-dominant hand failures...\n",
      "Frame 36: Interpolated with confidence 0.94\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\n",
      "Frame 39: Interpolated with confidence 0.62\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\n",
      "Frame 40: Interpolated with confidence 0.64\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms.npz\n",
      "Frame 54: Interpolated with confidence 1.00\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms.npz\n",
      "Interpolated 4 non-dominant hand frames\n",
      "Processing dominant hand failures...\n",
      "Interpolated 0 dominant hand frames\n",
      "Total interpolated: 4 frames\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolate_undetected_hand_landmarks(directory_path=\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = load_frame_data(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_valid_detection(frame_data, is_dominant_hand):\n",
    "    \"\"\"\n",
    "    Check if the frame has valid detection (not interpolated) for a specific hand.\n",
    "    \n",
    "    Args:\n",
    "        frame_data: The loaded frame data\n",
    "        is_dominant_hand: If True, check dominant hand; if False, check non-dominant hand;\n",
    "                         if None, check if either hand is detected\n",
    "    \n",
    "    Returns:\n",
    "        bool: Whether the specified hand(s) is/are detected\n",
    "    \"\"\"\n",
    "    detection_status = frame_data[4]\n",
    "    \n",
    "    if is_dominant_hand:\n",
    "        # Check specifically for dominant hand\n",
    "        return detection_status[0] == 1\n",
    "    else:\n",
    "        # Check specifically for non-dominant hand\n",
    "        return detection_status[1] == 1\n",
    "    \n",
    "\n",
    "\n",
    "def has_value(frame_data, is_dominant_hand):\n",
    "    \"\"\"Check if the frame exists and has any value (detection or interpolation)\"\"\"\n",
    "    detection_status = frame_data[4]\n",
    "    interpolation_scores = frame_data[3]\n",
    "    if is_dominant_hand:\n",
    "        return (detection_status[0]==1) or (interpolation_scores[0]>0)\n",
    "    else:\n",
    "        return (detection_status[1]==1) or (interpolation_scores[1]>0)\n",
    "    \n",
    "        \n",
    "\n",
    "def cartesian_to_spherical(velocities):\n",
    "    \"\"\"\n",
    "    Convert Cartesian velocities (ux, uy, uz) to spherical coordinate features.\n",
    "    \n",
    "    Args:\n",
    "        velocities: NumPy array of shape (20, 3) with Cartesian velocities\n",
    "        \n",
    "    Returns:\n",
    "        NumPy array of shape (20, 5) with spherical features:\n",
    "            [vmagnitude, ϕsin, ϕcos, θsin, θcos]\n",
    "    \"\"\"\n",
    "    num_landmarks = velocities.shape[0]\n",
    "    spherical_features = np.zeros((num_landmarks, 5))\n",
    "    \n",
    "    for i in range(num_landmarks):\n",
    "        ux, uy, uz = velocities[i]\n",
    "        \n",
    "        # Calculate velocity magnitude\n",
    "        vmagnitude = np.sqrt(ux**2 + uy**2 + uz**2)\n",
    "        spherical_features[i, 0] = vmagnitude\n",
    "        \n",
    "        # Handle edge cases to avoid division by zero\n",
    "        if vmagnitude == 0:\n",
    "            # If velocity is zero, set all angles to zero\n",
    "            spherical_features[i, 1:] = 0\n",
    "            continue\n",
    "        \n",
    "        # Calculate azimuth angle (ϕ)\n",
    "        phi = np.arctan2(uy, ux)\n",
    "        spherical_features[i, 1] = np.sin(phi)  # ϕsin\n",
    "        spherical_features[i, 2] = np.cos(phi)  # ϕcos\n",
    "        \n",
    "        # Calculate elevation angle (θ)\n",
    "        # Clamp uz/vmagnitude to range [-1, 1] to avoid numerical errors\n",
    "        cos_theta = np.clip(uz / vmagnitude, -1.0, 1.0)\n",
    "        theta = np.arccos(cos_theta)\n",
    "        spherical_features[i, 3] = np.sin(theta)  # θsin\n",
    "        spherical_features[i, 4] = cos_theta      # θcos (already calculated)\n",
    "    \n",
    "    return spherical_features\n",
    "\n",
    "\n",
    "\n",
    "def cartesian_to_polar_features(velocities):\n",
    "    \"\"\"\n",
    "    Convert Cartesian velocity coordinates to polar features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    velocities : numpy.ndarray\n",
    "        Array of shape (2, 2) where each row represents an object's [Ux, Uy]\n",
    "\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of shape (2, 3) with columns [magnitude, sin(direction), cos(direction)]\n",
    "    \"\"\"\n",
    "    # Calculate magnitude\n",
    "    magnitude = np.sqrt(np.sum(velocities**2, axis=1))\n",
    "    \n",
    "    # Initialize result array\n",
    "    result = np.zeros((velocities.shape[0], 3))\n",
    "    result[:, 0] = magnitude  # Set first column to magnitude\n",
    "    \n",
    "    # Create a mask for non-zero magnitudes\n",
    "    non_zero = magnitude > 0\n",
    "    \n",
    "    # For non-zero magnitudes, calculate direction components\n",
    "    if np.any(non_zero):\n",
    "        # Get direction for non-zero magnitudes\n",
    "        direction = np.arctan2(velocities[non_zero, 1], velocities[non_zero, 0])\n",
    "        \n",
    "        # Calculate sin and cos\n",
    "        result[non_zero, 1] = np.sin(direction)  # sin(direction)\n",
    "        result[non_zero, 2] = np.cos(direction)  # cos(direction)\n",
    "    \n",
    "    # Handle zero magnitudes \n",
    "    zero_indices = ~non_zero\n",
    "    if np.any(zero_indices):\n",
    "        result[zero_indices, 1] = 0.0\n",
    "        result[zero_indices, 2] = 0.0\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_landmark_velocities(directory_path):\n",
    "    \"\"\"\n",
    "    Compute velocity features for hand landmarks using central differencing with two window sizes,\n",
    "    and convert to spherical coordinates.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing frame NPZ files\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of frames processed\n",
    "    \"\"\"\n",
    "    # List all NPZ files in the directory\n",
    "    npz_files = sorted(glob.glob(os.path.join(directory_path, \"*.npz\")))\n",
    "    \n",
    "    # Skip if no files found\n",
    "    if not npz_files:\n",
    "        print(f\"No NPZ files found in {directory_path}\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Computing velocities for {len(npz_files)} files...\")\n",
    "    \n",
    "    # Create a mapping of frame indices to file paths\n",
    "    frame_to_file = {}\n",
    "    for file_path in npz_files:\n",
    "        frame_data = load_frame_data(file_path)\n",
    "        frame_idx = frame_data[8]  # Index for frame_idx\n",
    "        frame_to_file[frame_idx] = file_path\n",
    "    \n",
    "    frame_indices = sorted(frame_to_file.keys())\n",
    "    processed_count = 0\n",
    "    \n",
    "\n",
    "    min_frame = min(frame_indices)\n",
    "    max_frame = max(frame_indices)\n",
    "    safe_margin = 5  # Skip processing frames within 5 frames of the edge\n",
    "    \n",
    "    # Process each frame\n",
    "    for i, curr_idx in enumerate(frame_indices):\n",
    "        if curr_idx < min_frame + safe_margin or curr_idx > max_frame - safe_margin:\n",
    "            dom_velocity_small = np.zeros((20, 5))\n",
    "            dom_velocity_large = np.zeros((20, 5))\n",
    "            non_dom_velocity_small = np.zeros((20, 5))\n",
    "            non_dom_velocity_large = np.zeros((20, 5))\n",
    "            \n",
    "            wrist_velocity_small = np.zeros((2, 3))  \n",
    "            wrist_velocity_large = np.zeros((2, 3))\n",
    "            \n",
    "            # Create zero confidence arrays\n",
    "            velocity_confidence = np.zeros(2)\n",
    "            velocity_calculation_confidence = np.zeros(2)\n",
    "            \n",
    "            # Save these zero arrays\n",
    "            modifications = {\n",
    "                'dom_velocity_small': dom_velocity_small,\n",
    "                'dom_velocity_large': dom_velocity_large,\n",
    "                'non_dom_velocity_small': non_dom_velocity_small,\n",
    "                'non_dom_velocity_large': non_dom_velocity_large,\n",
    "                'velocity_confidence': velocity_confidence,\n",
    "                'velocity_calculation_confidence': velocity_calculation_confidence,\n",
    "                'wrist_velocity_small': wrist_velocity_small,\n",
    "                'wrist_velocity_large': wrist_velocity_large,\n",
    "            }\n",
    "            \n",
    "            # Get the file path for this frame\n",
    "            current_file_path = frame_to_file[curr_idx]\n",
    "            modify_npz_file(current_file_path, modifications)\n",
    "            processed_count += 1\n",
    "            \n",
    "            # Log that we're skipping calculation\n",
    "            print(f\"Frame {curr_idx} too close to video boundary - setting zero velocities\")\n",
    "            continue\n",
    "        # Load current frame\n",
    "        current_file_path = frame_to_file[curr_idx]\n",
    "        curr_frame_data = load_frame_data(current_file_path)\n",
    "        \n",
    "        # Store needed frames in a dictionary for easy access\n",
    "        frame_cache = {curr_idx: curr_frame_data}\n",
    "        \n",
    "        # Load all potentially needed frames in the -5 to +5 range\n",
    "        for offset in range(-5, 6):\n",
    "            if offset == 0:  # Skip current frame (already loaded)\n",
    "                continue\n",
    "            \n",
    "            check_idx = curr_idx + offset\n",
    "            if check_idx in frame_to_file:\n",
    "                frame_cache[check_idx] = load_frame_data(frame_to_file[check_idx])\n",
    "            else:\n",
    "                frame_cache[check_idx] = None  # Mark as not available\n",
    "        \n",
    "        # Extract dominant and non-dominant hand landmarks from current frame\n",
    "        dom_landmarks = curr_frame_data[0]\n",
    "        non_dom_landmarks = curr_frame_data[1]\n",
    "        \n",
    "        # Initialize velocity arrays in Cartesian coordinates\n",
    "        dom_velocity_small_cart = np.zeros_like(dom_landmarks)\n",
    "        dom_velocity_large_cart = np.zeros_like(dom_landmarks)\n",
    "        non_dom_velocity_small_cart = np.zeros_like(non_dom_landmarks)\n",
    "        non_dom_velocity_large_cart = np.zeros_like(non_dom_landmarks)\n",
    "        \n",
    "        wrist_velocity_small = np.zeros((2, 2))  # 2 hands × [x, y] coordinates\n",
    "        wrist_velocity_large = np.zeros((2, 2))  # 2 hands × [x, y] coordinates\n",
    "        \n",
    "        # Initialize confidence and method weight tracking\n",
    "        dom_small_conf = 0.0\n",
    "        dom_large_conf = 0.0\n",
    "        non_dom_small_conf = 0.0\n",
    "        non_dom_large_conf = 0.0\n",
    "        \n",
    "        dom_small_method_weight = 0.0\n",
    "        dom_large_method_weight = 0.0\n",
    "        non_dom_small_method_weight = 0.0\n",
    "        non_dom_large_method_weight = 0.0\n",
    "        \n",
    "        dom_small_source_quality = 0.0\n",
    "        dom_large_source_quality = 0.0\n",
    "        non_dom_small_source_quality = 0.0\n",
    "        non_dom_large_source_quality = 0.0\n",
    "        \n",
    "        # ===== DOMINANT HAND VELOCITY CALCULATION =====\n",
    "        \n",
    "        # Small window [-1, +1] velocity with fallbacks\n",
    "        if (curr_idx + 1 in frame_cache and frame_cache[curr_idx + 1] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 1], True) and \n",
    "            curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "            has_value(frame_cache[curr_idx - 1], True)):\n",
    "            # Ideal case: (t+1, t-1)\n",
    "            dom_velocity_small_cart = (frame_cache[curr_idx + 1][0] - frame_cache[curr_idx - 1][0]) / 2.0\n",
    "            wrist_velocity_small[0, :] = (frame_cache[curr_idx + 1][7][0, :] - frame_cache[curr_idx - 1][7][0, :]) / 2.0\n",
    "            dom_small_conf = min(frame_cache[curr_idx + 1][2][0], frame_cache[curr_idx - 1][2][0])  # Detection confidence of t+1 frame\n",
    "            dom_small_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor (average of interpolation confidences)\n",
    "            t_plus_1_interp = frame_cache[curr_idx + 1][3][0]\n",
    "            t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "            dom_small_source_quality = (t_plus_1_interp + t_minus_1_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "              is_valid_detection(frame_cache[curr_idx + 2], True) and \n",
    "              curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "              has_value(frame_cache[curr_idx - 2], True)):\n",
    "            # Fallback 1: (t+2, t-2)\n",
    "            dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - frame_cache[curr_idx - 2][0]) / 4.0\n",
    "            wrist_velocity_small[0, :] = (frame_cache[curr_idx + 2][7][0, :] - frame_cache[curr_idx - 2][7][0, :]) / 4.0\n",
    "            dom_small_conf = min(frame_cache[curr_idx + 2][2][0], frame_cache[curr_idx - 2][2][0])  # Detection confidence of t+2 frame\n",
    "            dom_small_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "            t_minus_2_interp = frame_cache[curr_idx - 2][3][0]\n",
    "            dom_small_source_quality = (t_plus_2_interp + t_minus_2_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "              is_valid_detection(frame_cache[curr_idx + 2], True)):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], True)):\n",
    "                # Fallback 2: (t+2, t-1)\n",
    "                dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - frame_cache[curr_idx - 1][0]) / 3.0\n",
    "                wrist_velocity_small[0, :] = (frame_cache[curr_idx + 2][7][0, :] - frame_cache[curr_idx - 1][7][0, :]) / 3.0\n",
    "                dom_small_conf = min(frame_cache[curr_idx + 2][2][0], frame_cache[curr_idx - 1][2][0])  # Detection confidence of t+2 frame\n",
    "                dom_small_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "                dom_small_source_quality = (t_plus_2_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif is_valid_detection(curr_frame_data, True):\n",
    "                # Fallback 3: (t+2, t)\n",
    "                dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - curr_frame_data[0]) / 2.0\n",
    "                wrist_velocity_small[0, :] = (frame_cache[curr_idx + 2][7][0, :] - curr_frame_data[7][0, :]) / 2.0\n",
    "                dom_small_conf = min(frame_cache[curr_idx + 2][2][0], curr_frame_data[2][0])\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                dom_small_source_quality = (t_plus_2_interp + t_interp) / 2.0\n",
    "                \n",
    "        elif is_valid_detection(curr_frame_data, True):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], True)):\n",
    "                # Fallback 4: (t, t-1)\n",
    "                dom_velocity_small_cart = (curr_frame_data[0] - frame_cache[curr_idx - 1][0])\n",
    "                wrist_velocity_small[0, :] = (curr_frame_data[7][0, :] - frame_cache[curr_idx - 1][7][0, :]) \n",
    "                dom_small_conf = min(curr_frame_data[2][0], frame_cache[curr_idx - 1][2][0])  # Detection confidence of current frame\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "                dom_small_source_quality = (t_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "                  has_value(frame_cache[curr_idx - 2], True)):\n",
    "                # Fallback 5: (t, t-2)\n",
    "                dom_velocity_small_cart = (curr_frame_data[0] - frame_cache[curr_idx - 2][0]) / 2.0\n",
    "                wrist_velocity_small[0, :] = (curr_frame_data[7][0, :] - frame_cache[curr_idx - 2][7][0, :]) / 2.0\n",
    "                dom_small_conf = min(curr_frame_data[2][0], frame_cache[curr_idx - 2][2][0])  # Detection confidence of current frame\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                t_minus_2_interp = frame_cache[curr_idx - 2][3][0]\n",
    "                dom_small_source_quality = (t_interp + t_minus_2_interp) / 2.0\n",
    "        \n",
    "        # Large window [-5, +5] velocity with fallbacks\n",
    "\n",
    "        if (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], True) and \n",
    "            curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "            has_value(frame_cache[curr_idx - 5], True)):\n",
    "            # Ideal case: (t+5, t-5)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 5][0]) / 10.0\n",
    "            wrist_velocity_large[0, :] = (frame_cache[curr_idx + 5][7][0, :] - frame_cache[curr_idx - 5][7][0, :]) / 10.0\n",
    "            dom_large_conf = min(frame_cache[curr_idx + 5][2][0], frame_cache[curr_idx - 5][2][0])  # Detection confidence of t+5 frame\n",
    "            dom_large_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "            t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "            dom_large_source_quality = (t_plus_5_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "        is_valid_detection(frame_cache[curr_idx + 4], True) and \n",
    "        curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "        has_value(frame_cache[curr_idx - 4], True)):\n",
    "        # Fallback 1: (t+4, t-4)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 4][0]) / 8.0\n",
    "            wrist_velocity_large[0, :] = (frame_cache[curr_idx + 4][7][0, :] - frame_cache[curr_idx - 4][7][0, :]) / 8.0\n",
    "            dom_large_conf = min(frame_cache[curr_idx + 4][2][0], frame_cache[curr_idx - 4][2][0]) # Detection confidence of t+4 frame\n",
    "            dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "            dom_large_source_quality = (t_plus_4_interp + t_minus_4_interp) / 2.0\n",
    "    \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], True) and \n",
    "            curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "            has_value(frame_cache[curr_idx - 3], True)):\n",
    "            # Fallback 2: (t+3, t-3)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 3][0]) / 6.0\n",
    "            wrist_velocity_large[0, :] = (frame_cache[curr_idx + 3][7][0, :] - frame_cache[curr_idx - 3][7][0, :]) / 6.0\n",
    "            dom_large_conf = min(frame_cache[curr_idx + 3][2][0], frame_cache[curr_idx - 3][2][0])  # Detection confidence of t+3 frame\n",
    "            dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "            t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "            dom_large_source_quality = (t_plus_3_interp + t_minus_3_interp) / 2.0\n",
    "            \n",
    "        # Asymmetric fallbacks for large window\n",
    "        elif (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], True)):\n",
    "            if (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4], True)):\n",
    "                # Fallback 3: (t+5, t-4)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 4][0]) / 9.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 5][7][0, :] - frame_cache[curr_idx - 4][7][0, :]) / 9.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 5][2][0], frame_cache[curr_idx - 4][2][0])  # Detection confidence of t+5 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "                dom_large_source_quality = (t_plus_5_interp + t_minus_4_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], True)):\n",
    "                # Fallback 4: (t+5, t-3)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 3][0]) / 8.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 5][7][0, :] - frame_cache[curr_idx - 3][7][0, :]) / 8.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 5][2][0], frame_cache[curr_idx - 3][2][0])  # Detection confidence of t+5 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "                dom_large_source_quality = (t_plus_5_interp + t_minus_3_interp) / 2.0\n",
    "        \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], True)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], True)):\n",
    "                # Fallback 5: (t+4, t-5)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 5][0]) / 9.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 4][7][0, :] - frame_cache[curr_idx - 5][7][0, :]) / 9.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 4][2][0], frame_cache[curr_idx - 5][2][0])  # Detection confidence of t+4 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "                dom_large_source_quality = (t_plus_4_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], True)):\n",
    "                # Fallback 6: (t+4, t-3)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 3][0]) / 7.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 4][7][0, :] - frame_cache[curr_idx - 3][7][0, :]) / 7.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 4][2][0], frame_cache[curr_idx - 3][2][0])  # Detection confidence of t+4 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "                dom_large_source_quality = (t_plus_4_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], True)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], True)):\n",
    "                # Fallback 7: (t+3, t-5)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 5][0]) / 8.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 3][7][0, :] - frame_cache[curr_idx - 5][7][0, :]) / 8.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 3][2][0], frame_cache[curr_idx - 5][2][0])  # Detection confidence of t+3 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "                dom_large_source_quality = (t_plus_3_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4], True)):\n",
    "                # Fallback 8: (t+3, t-4)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 4][0]) / 7.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 3][7][0, :] - frame_cache[curr_idx - 4][7][0, :]) / 7.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 3][2][0], frame_cache[curr_idx - 4][2][0])  # Detection confidence of t+3 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "                dom_large_source_quality = (t_plus_3_interp + t_minus_4_interp) / 2.0\n",
    "\n",
    "        # ===== NON-DOMINANT HAND VELOCITY CALCULATION =====\n",
    "\n",
    "    # Small window [-1, +1] velocity with fallbacks for non-dominant hand\n",
    "        if (curr_idx + 1 in frame_cache and frame_cache[curr_idx + 1] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 1], False) and \n",
    "            curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "            has_value(frame_cache[curr_idx - 1], False)):\n",
    "            # Ideal case: (t+1, t-1)\n",
    "            non_dom_velocity_small_cart = (frame_cache[curr_idx + 1][1] - frame_cache[curr_idx - 1][1]) / 2.0\n",
    "            wrist_velocity_small[1, :] = (frame_cache[curr_idx + 1][7][1, :] - frame_cache[curr_idx - 1][7][1, :]) / 2.0\n",
    "            non_dom_small_conf = min(frame_cache[curr_idx + 1][2][1], frame_cache[curr_idx - 1][2][1])  # Detection confidence of t+1 frame\n",
    "            non_dom_small_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_1_interp = frame_cache[curr_idx + 1][3][1]\n",
    "            t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "            non_dom_small_source_quality = (t_plus_1_interp + t_minus_1_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 2], False) and \n",
    "            curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "            has_value(frame_cache[curr_idx - 2], False)):\n",
    "            # Fallback 1: (t+2, t-2)\n",
    "            non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - frame_cache[curr_idx - 2][1]) / 4.0\n",
    "            wrist_velocity_small[1, :] = (frame_cache[curr_idx + 2][7][1, :] - frame_cache[curr_idx - 2][7][1, :]) / 4.0\n",
    "            non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], frame_cache[curr_idx - 2][2][1]) \n",
    "            non_dom_small_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "            t_minus_2_interp = frame_cache[curr_idx - 2][3][1]\n",
    "            non_dom_small_source_quality = (t_plus_2_interp + t_minus_2_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 2], False)):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], False)):\n",
    "                # Fallback 2: (t+2, t-1)\n",
    "                non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - frame_cache[curr_idx - 1][1]) / 3.0\n",
    "                wrist_velocity_small[1, :] = (frame_cache[curr_idx + 2][7][1, :] - frame_cache[curr_idx - 2][7][1, :]) / 3.0\n",
    "                non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], frame_cache[curr_idx - 1][2][1])  \n",
    "                non_dom_small_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "                non_dom_small_source_quality = (t_plus_2_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif is_valid_detection(curr_frame_data, False):\n",
    "                # Fallback 3: (t+2, t)\n",
    "                non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - curr_frame_data[1]) / 2.0\n",
    "                wrist_velocity_small[1, :] = (frame_cache[curr_idx + 2][7][1, :] - curr_frame_data[7][1, :]) / 2.0\n",
    "                non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], curr_frame_data[2][1])\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                non_dom_small_source_quality = (t_plus_2_interp + t_interp) / 2.0\n",
    "                \n",
    "        elif is_valid_detection(curr_frame_data, False):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], False)):\n",
    "                # Fallback 4: (t, t-1)\n",
    "                non_dom_velocity_small_cart = (curr_frame_data[1] - frame_cache[curr_idx - 1][1])\n",
    "                wrist_velocity_small[1, :] = (curr_frame_data[7][1, :] - frame_cache[curr_idx - 1][7][1, :]) \n",
    "                non_dom_small_conf = min(curr_frame_data[2][1], frame_cache[curr_idx - 1][2][1])  # Detection confidence of current frame\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "                non_dom_small_source_quality = (t_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "                has_value(frame_cache[curr_idx - 2], False)):\n",
    "                # Fallback 5: (t, t-2)\n",
    "                non_dom_velocity_small_cart = (curr_frame_data[1] - frame_cache[curr_idx - 2][1]) / 2.0\n",
    "                wrist_velocity_small[1, :] = (curr_frame_data[7][1, :] - frame_cache[curr_idx -21][7][1, :]) / 2.0\n",
    "                non_dom_small_conf = min(curr_frame_data[2][1], frame_cache[curr_idx -2][2][1])  # Detection confidence of current frame\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                t_minus_2_interp = frame_cache[curr_idx - 2][3][1]\n",
    "                non_dom_small_source_quality = (t_interp + t_minus_2_interp) / 2.0\n",
    "    \n",
    "        # Large window [-5, +5] velocity with fallbacks for non-dominant hand\n",
    "        if (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], False) and \n",
    "            curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "            has_value(frame_cache[curr_idx - 5], False)):\n",
    "            # Ideal case: (t+5, t-5)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 5][1]) / 10.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 5][7][1, :] - frame_cache[curr_idx - 5][7][1, :]) / 10.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 5][2][1], frame_cache[curr_idx - 5][2][1])  \n",
    "            non_dom_large_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "            t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_5_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], False) and \n",
    "            curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "            has_value(frame_cache[curr_idx - 4], False)):\n",
    "            # Fallback 1: (t+4, t-4)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 4][1]) / 8.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 4][7][1, :] - frame_cache[curr_idx - 4][7][1, :]) / 8.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 4][2][1], frame_cache[curr_idx - 4][2][1])  \n",
    "            non_dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_4_interp + t_minus_4_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], False) and \n",
    "            curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "            has_value(frame_cache[curr_idx - 3], False)):\n",
    "            # Fallback 2: (t+3, t-3)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 3][1]) / 6.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 3][7][1, :] - frame_cache[curr_idx - 3][7][1, :]) / 6.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 3][2][1], frame_cache[curr_idx - 3][2][1])  # Detection confidence of t+3 frame\n",
    "            non_dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "            t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_3_interp + t_minus_3_interp) / 2.0\n",
    "            \n",
    "        # Asymmetric fallbacks for large window\n",
    "        elif (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], False)):\n",
    "            if (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4], False)):\n",
    "                # Fallback 3: (t+5, t-4)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 4][1]) / 9.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 5][7][1, :] - frame_cache[curr_idx - 4][7][1, :]) / 9.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 5][2][1], frame_cache[curr_idx - 4][2][1])  # Detection confidence of t+5 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_5_interp + t_minus_4_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], False)):\n",
    "                # Fallback 4: (t+5, t-3)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 3][1]) / 8.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 5][7][1, :] - frame_cache[curr_idx - 3][7][1, :]) / 8.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 5][2][1], frame_cache[curr_idx - 3][2][1])  # Detection confidence of t+5 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_5_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], False)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], False)):\n",
    "                # Fallback 5: (t+4, t-5)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 5][1]) / 9.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 4][7][1, :] - frame_cache[curr_idx - 5][7][1, :]) / 9.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 4][2][1], frame_cache[curr_idx - 5][2][1])  # Detection confidence of t+4 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_4_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], False)):\n",
    "                # Fallback 6: (t+4, t-3)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 3][1]) / 7.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 4][7][1, :] - frame_cache[curr_idx - 3][7][1, :]) / 7.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 4][2][1], frame_cache[curr_idx - 3][2][1])  # Detection confidence of t+4 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_4_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], False)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], False)):\n",
    "                # Fallback 7: (t+3, t-5)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 5][1]) / 8.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 3][7][1, :] - frame_cache[curr_idx - 5][7][1, :]) / 8.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 3][2][1], frame_cache[curr_idx - 5][2][1])  # Detection confidence of t+3 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_3_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "            has_value(frame_cache[curr_idx - 4], False)):\n",
    "            # Fallback 8: (t+3, t-4)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 4][1]) / 7.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 3][7][1, :] - frame_cache[curr_idx - 4][7][1, :]) / 7.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 3][2][1], frame_cache[curr_idx - 4][2][1])  # Detection confidence of t+3 frame\n",
    "            non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_3_interp + t_minus_4_interp) / 2.0\n",
    "            \n",
    "\n",
    "        \n",
    "        # Convert Cartesian velocities to spherical/polar coordinates\n",
    "        dom_velocity_small = cartesian_to_spherical(dom_velocity_small_cart)\n",
    "        dom_velocity_large = cartesian_to_spherical(dom_velocity_large_cart)\n",
    "        non_dom_velocity_small = cartesian_to_spherical(non_dom_velocity_small_cart)\n",
    "        non_dom_velocity_large = cartesian_to_spherical(non_dom_velocity_large_cart)\n",
    "        \n",
    "        wrist_velocity_small_polar = cartesian_to_polar_features(wrist_velocity_small)\n",
    "        wrist_velocity_large_polar = cartesian_to_polar_features(wrist_velocity_large)\n",
    "        \n",
    "        # Calculate average confidence for each hand across both windows\n",
    "        dom_avg_conf = (dom_small_conf + dom_large_conf) / 2.0\n",
    "        non_dom_avg_conf = (non_dom_small_conf + non_dom_large_conf) / 2.0\n",
    "        \n",
    "        # Calculate velocityCalculationConfidence using method weight and source quality\n",
    "        dom_small_vel_calc_conf = dom_small_method_weight * dom_small_source_quality\n",
    "        dom_large_vel_calc_conf = dom_large_method_weight * dom_large_source_quality\n",
    "        non_dom_small_vel_calc_conf = non_dom_small_method_weight * non_dom_small_source_quality\n",
    "        non_dom_large_vel_calc_conf = non_dom_large_method_weight * non_dom_large_source_quality\n",
    "        \n",
    "        # Average across windows for each hand\n",
    "        dom_vel_calc_conf = (dom_small_vel_calc_conf + dom_large_vel_calc_conf) / 2.0\n",
    "        non_dom_vel_calc_conf = (non_dom_small_vel_calc_conf + non_dom_large_vel_calc_conf) / 2.0\n",
    "        \n",
    "        # Prepare arrays\n",
    "        velocity_confidence = np.array([dom_avg_conf, non_dom_avg_conf])\n",
    "        velocity_calculation_confidence = np.array([dom_vel_calc_conf, non_dom_vel_calc_conf])\n",
    "        \n",
    "        # Save back to the NPZ file\n",
    "        modifications = {\n",
    "            'dom_velocity_small': dom_velocity_small,\n",
    "            'dom_velocity_large': dom_velocity_large,\n",
    "            'non_dom_velocity_small': non_dom_velocity_small,\n",
    "            'non_dom_velocity_large': non_dom_velocity_large,\n",
    "            'velocity_confidence': velocity_confidence,\n",
    "            'velocity_calculation_confidence': velocity_calculation_confidence,\n",
    "            'wrist_velocity_small': wrist_velocity_small_polar,\n",
    "            'wrist_velocity_large': wrist_velocity_large_polar,\n",
    "            \n",
    "        }\n",
    "        \n",
    "        modify_npz_file(current_file_path, modifications)\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 100 == 0 or i == len(frame_indices) - 1:\n",
    "            print(f\"Processed {i+1}/{len(frame_indices)} frames\")\n",
    "    \n",
    "    print(f\"Velocity computation complete. Processed {processed_count} frames.\")\n",
    "    return processed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing velocities for 30 files...\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000030_00m30s000ms.npz\n",
      "Frame 30 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000031_00m31s000ms.npz\n",
      "Frame 31 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000032_00m32s000ms.npz\n",
      "Frame 32 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000033_00m33s000ms.npz\n",
      "Frame 33 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000034_00m34s000ms.npz\n",
      "Frame 34 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000035_00m35s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000037_00m37s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000038_00m38s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000041_00m41s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000042_00m42s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000043_00m43s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000044_00m44s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000045_00m45s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000046_00m46s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000047_00m47s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000048_00m48s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000049_00m49s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000050_00m50s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000051_00m51s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000052_00m52s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000053_00m53s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000055_00m55s000ms.npz\n",
      "Frame 55 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000056_00m56s000ms.npz\n",
      "Frame 56 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000057_00m57s000ms.npz\n",
      "Frame 57 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000058_00m58s000ms.npz\n",
      "Frame 58 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000059_00m59s000ms.npz\n",
      "Frame 59 too close to video boundary - setting zero velocities\n",
      "Velocity computation complete. Processed 30 frames.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_landmark_velocities(directory_path=\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data_with_velocities(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks = data['dom_landmarks']\n",
    "    non_dom_landmarks = data['non_dom_landmarks']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores = data['blendshape_scores']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist = data['nose_to_wrist_dist']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    dom_velocity_small = data['dom_velocity_small']\n",
    "    dom_velocity_large = data['dom_velocity_large']\n",
    "    non_dom_velocity_small = data['non_dom_velocity_small']\n",
    "    non_dom_velocity_large = data['non_dom_velocity_large']\n",
    "    velocity_confidence = data['velocity_confidence']\n",
    "    velocity_calculation_confidence = data['velocity_calculation_confidence']\n",
    "    nose_to_wrist_velocity_small = data['wrist_velocity_small']\n",
    "    nose_to_wrist_velocity_large = data['wrist_velocity_large']\n",
    "    \n",
    "    return (dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores, face_detected, \n",
    "            nose_to_wrist_dist, frame_idx, timestamp_ms, dom_velocity_small, dom_velocity_large, non_dom_velocity_small, non_dom_velocity_large, velocity_confidence, velocity_calculation_confidence, nose_to_wrist_velocity_small, nose_to_wrist_velocity_large)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_30 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000030_00m30s000ms.npz\")\n",
    "frame_37 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000037_00m37s000ms.npz\")\n",
    "frame_42 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000042_00m42s000ms.npz\")\n",
    "frame_32 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000032_00m32s000ms.npz\")\n",
    "frame_36 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\")\n",
    "frame_38 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000038_00m38s000ms.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_videos_df(directory_path):\n",
    "    \"\"\"\n",
    "    Process video files in a directory and return information in a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to the directory containing video files\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with video name, frame count, fps, Right/Left designation, and file path\n",
    "    \"\"\"\n",
    "    # Lists to store video information\n",
    "    data = []\n",
    "    \n",
    "    # Common video extensions\n",
    "    video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv']\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            # Check if the file is a video\n",
    "            _, ext = os.path.splitext(file)\n",
    "            if ext.lower() in video_extensions:\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Extract video name (without extension)\n",
    "                video_name = os.path.splitext(file)[0]\n",
    "                \n",
    "                # Determine if it's Right or Left\n",
    "                if video_name.endswith(\"_R\"):\n",
    "                    dom_hand = \"Right\"\n",
    "                elif video_name.endswith(\"_L\"):\n",
    "                    dom_hand = \"Left\"\n",
    "                else:\n",
    "                    dom_hand = None\n",
    "                \n",
    "                # Open the video file\n",
    "                try:\n",
    "                    cap = cv2.VideoCapture(file_path)\n",
    "                    \n",
    "                    # Get frames per second\n",
    "                    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "                    \n",
    "                    # Get total number of frames\n",
    "                    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                    \n",
    "                    # Release the video capture object\n",
    "                    cap.release()\n",
    "                    \n",
    "                    # Add row to data\n",
    "                    data.append({\n",
    "                        'Video Name': video_name,\n",
    "                        'Frame Count': frame_count,\n",
    "                        'FPS': fps,\n",
    "                        'dom_hand': dom_hand,\n",
    "                        'file_path': file_path  # Added file_path to the DataFrame\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = make_videos_df(directory_path=\"./videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video Name</th>\n",
       "      <th>Frame Count</th>\n",
       "      <th>FPS</th>\n",
       "      <th>dom_hand</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0636580316216-SHUT OUT_fps15_L</td>\n",
       "      <td>31</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Left</td>\n",
       "      <td>./videos/0636580316216-SHUT OUT_fps15_L.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7299129501965353e-7-seedSOUR_fps15_R</td>\n",
       "      <td>48</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Right</td>\n",
       "      <td>./videos/4.7299129501965353e-7-seedSOUR_fps15_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Video Name  Frame Count   FPS dom_hand  \\\n",
       "0          0636580316216-SHUT OUT_fps15_L           31  15.0     Left   \n",
       "1  4.7299129501965353e-7-seedSOUR_fps15_R           48  15.0    Right   \n",
       "\n",
       "                                           file_path  \n",
       "0        ./videos/0636580316216-SHUT OUT_fps15_L.mp4  \n",
       "1  ./videos/4.7299129501965353e-7-seedSOUR_fps15_...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_new_2(video_path, adaptive_detect_func=adaptive_detect, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                 min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5,\n",
    "                 min_face_detection_confidence=0.5, min_face_presence_confidence=0.5,\n",
    "                 num_hands=2, output_face_blendshapes=True,\n",
    "                 max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2, \n",
    "                 frame_step=1, start_time_seconds=0, end_time_seconds=None,\n",
    "                 save_failure_screenshots=False,\n",
    "                 num_workers=None,  # Added parameter for parallel processing\n",
    "                 batch_mode=False):  # Added parameter to indicate batch processing\n",
    "    \"\"\"\n",
    "    Process a video frame-by-frame using the adaptive_detect function and save results.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file\n",
    "        adaptive_detect_func: The adaptive detection function to use\n",
    "        min_hand_detection_confidence (float): Initial confidence threshold for hand detection\n",
    "        min_hand_presence_confidence (float): Initial confidence threshold for hand presence\n",
    "        min_face_detection_confidence (float): Initial confidence threshold for face detection\n",
    "        min_face_presence_confidence (float): Initial confidence threshold for face presence\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        output_face_blendshapes (bool): Whether to detect face blendshapes\n",
    "        max_attempts (int): Maximum detection attempts for adaptive detection\n",
    "        threshold_reduction_factor (float): Factor to reduce thresholds by\n",
    "        min_threshold (float): Minimum threshold limit\n",
    "        frame_step (int): Process every Nth frame (1 = all frames)\n",
    "        start_time_seconds (float): Time in seconds to start processing from\n",
    "        end_time_seconds (float): Time in seconds to end processing (None = process until end)\n",
    "        save_failure_screenshots (bool): Save screenshots for all frames with any detection failures\n",
    "        num_workers (int): Number of parallel workers to use (None = auto-detect based on CPU cores)\n",
    "        batch_mode (bool): Whether this is being run as part of a batch process\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the directory containing saved frame results\n",
    "    \"\"\"\n",
    "    # Import additional libraries for parallel processing\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    import threading\n",
    "    from queue import Queue\n",
    "    import multiprocessing\n",
    "    import gc  # For garbage collection\n",
    "\n",
    "    # Extract video name for directory creation\n",
    "    video_path = Path(video_path)\n",
    "    video_dir = video_path.parent\n",
    "    video_name = video_path.stem  # Get filename without extension\n",
    "    \n",
    "    # Extract dominant hand information from filename\n",
    "    if video_name.endswith(\"_R\"):\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "    elif video_name.endswith(\"_L\"):\n",
    "        extracted_dominant_hand = \"Left\"\n",
    "    else:\n",
    "        # Default if not specified in filename\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "        print(f\"Warning: Could not determine dominant hand from filename, using default: {extracted_dominant_hand}\")\n",
    "\n",
    "    # Use the extracted dominant hand instead of the parameter\n",
    "    dominand_hand = extracted_dominant_hand\n",
    "    if not batch_mode:\n",
    "        print(f\"Detected dominant hand from filename: {dominand_hand}\")\n",
    "    else:\n",
    "        print(f\"[{os.path.basename(str(video_path))}] Dominant hand: {dominand_hand}\")\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = video_dir / f\"{video_name}_landmarks\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create screenshots directory if screenshot option is enabled\n",
    "    screenshots_dir = None\n",
    "    if save_failure_screenshots:\n",
    "        screenshots_dir = output_dir / \"failure_screenshots\"\n",
    "        screenshots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a log file to track processing\n",
    "    log_file = output_dir / \"processing_log.txt\"\n",
    "    \n",
    "    # Create a detailed statistics file\n",
    "    stats_file = output_dir / \"detection_statistics.json\"\n",
    "\n",
    "    # Initialize statistics tracking\n",
    "    stats = {\n",
    "        \"video_info\": {\n",
    "            \"name\": video_name,\n",
    "            \"path\": str(video_path),\n",
    "            \"total_frames\": 0,\n",
    "            \"processed_frames\": 0,\n",
    "            \"fps\": 0,\n",
    "            \"duration_seconds\": 0,\n",
    "            \"start_time\": start_time_seconds,\n",
    "            \"end_time\": end_time_seconds,\n",
    "            \"dominant_hand\": dominand_hand,\n",
    "            \"processing_started\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"processing_completed\": None\n",
    "        },\n",
    "        \"detection_rates\": {\n",
    "            \"dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"non_dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"face\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"overall\": {\n",
    "                \"all_detected\": 0,\n",
    "                \"partial_detections\": 0,\n",
    "                \"no_detections\": 0,\n",
    "                \"success_rate\": 0\n",
    "            }\n",
    "        },\n",
    "        \"failed_frames\": {\n",
    "            \"dominant_hand_failures\": [],\n",
    "            \"non_dominant_hand_failures\": [],\n",
    "            \"face_failures\": [],\n",
    "            \"all_failures\": []\n",
    "        },\n",
    "        \"processing_performance\": {\n",
    "            \"average_processing_time_ms\": 0,\n",
    "            \"total_processing_time_seconds\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Setup logging\n",
    "    with open(log_file, \"w\") as log:\n",
    "        log.write(f\"Processing video: {video_path}\\n\")\n",
    "        log.write(f\"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        log.write(f\"Parameters:\\n\")\n",
    "        log.write(f\"  - frame_step: {frame_step}\\n\")\n",
    "        log.write(f\"  - start_time: {start_time_seconds} seconds\\n\")\n",
    "        if end_time_seconds is not None:\n",
    "            log.write(f\"  - end_time: {end_time_seconds} seconds\\n\")\n",
    "        log.write(f\"  - dominand_hand: {dominand_hand}\\n\")\n",
    "        log.write(f\"  - num_hands: {num_hands}\\n\")\n",
    "        log.write(f\"  - detection confidence thresholds: {min_hand_detection_confidence}, {min_face_detection_confidence}\\n\")\n",
    "        log.write(f\"  - batch_mode: {batch_mode}\\n\")\n",
    "        log.write(\"\\n--- Frame processing log ---\\n\")\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration_seconds = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    # Update stats with video info\n",
    "    stats[\"video_info\"][\"total_frames\"] = total_frames\n",
    "    stats[\"video_info\"][\"fps\"] = fps\n",
    "    stats[\"video_info\"][\"duration_seconds\"] = duration_seconds\n",
    "    if end_time_seconds==None:\n",
    "        stats[\"video_info\"][\"end_time\"] = duration_seconds\n",
    "    \n",
    "    # Convert time to frame indices\n",
    "    start_frame = int(max(0, start_time_seconds * fps))\n",
    "    \n",
    "    # Set end frame if specified\n",
    "    if end_time_seconds is not None:\n",
    "        end_frame = min(total_frames, int(end_time_seconds * fps))\n",
    "    else:\n",
    "        end_frame = total_frames\n",
    "    \n",
    "    # Set the number of worker threads if not specified\n",
    "    if num_workers is None:\n",
    "        # More conservative thread allocation to prevent system overload\n",
    "        # Use 50% of available cores (minimum 2, maximum 6) to leave resources for other processes\n",
    "        num_workers = max(2, min(multiprocessing.cpu_count() // 2, 6))\n",
    "    \n",
    "    # When in batch mode, conserve resources even more and limit output\n",
    "    if batch_mode:\n",
    "        # Further reduce threads in batch mode to ensure stability\n",
    "        num_workers = 8\n",
    "        print(f\"[{os.path.basename(str(video_path))}] Using {num_workers} worker threads\")\n",
    "    else:\n",
    "        print(f\"Video: {video_name}\")\n",
    "        print(f\"Total frames: {total_frames}\")\n",
    "        print(f\"FPS: {fps}\")\n",
    "        print(f\"Duration: {duration_seconds:.2f} seconds\")\n",
    "        print(f\"Processing frames {start_frame} to {end_frame} (time {start_time_seconds:.2f}s to {end_time_seconds if end_time_seconds is not None else duration_seconds:.2f}s)\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        print(f\"Using {num_workers} worker threads for parallel processing\")\n",
    "    \n",
    "    # Function to process a single frame\n",
    "    def process_single_frame(args):\n",
    "        frame, frame_idx, temp_dir = args\n",
    "        \n",
    "        # Get timestamp in milliseconds\n",
    "        timestamp_ms = int(frame_idx * 1000 / fps)\n",
    "        timestamp_formatted = f\"{timestamp_ms//60000:02d}m{(timestamp_ms//1000)%60:02d}s{timestamp_ms%1000:03d}ms\"\n",
    "        \n",
    "        # Temporary frame path\n",
    "        temp_frame_path = Path(temp_dir) / f\"temp_frame_{frame_idx}.jpg\"\n",
    "        \n",
    "        # Save the current frame as an image\n",
    "        cv2.imwrite(str(temp_frame_path), frame)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            with SuppressOutput():\n",
    "                # Use adaptive_detect on the frame\n",
    "                results = adaptive_detect_func(\n",
    "                    str(temp_frame_path), hand_model_path, face_model_path,\n",
    "                    min_hand_detection_confidence=min_hand_detection_confidence,\n",
    "                    min_hand_presence_confidence=min_hand_presence_confidence,\n",
    "                    min_face_detection_confidence=min_face_detection_confidence,\n",
    "                    min_face_presence_confidence=min_face_presence_confidence,\n",
    "                    num_hands=num_hands,\n",
    "                    dominand_hand=dominand_hand,\n",
    "                    visualize=False,\n",
    "                    output_face_blendshapes=output_face_blendshapes,\n",
    "                    max_attempts=max_attempts,\n",
    "                    threshold_reduction_factor=threshold_reduction_factor,\n",
    "                    min_threshold=min_threshold\n",
    "                )\n",
    "            \n",
    "            # Calculate processing time\n",
    "            proc_time = time.time() - start_time\n",
    "            \n",
    "            # Create output filename with frame info\n",
    "            output_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "            output_path = output_dir / output_filename\n",
    "            \n",
    "            # Unpack results\n",
    "            dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
    "            \n",
    "            # Save screenshot if any detection failed and screenshots are enabled\n",
    "            if save_failure_screenshots and (detection_status[0] != 1 or detection_status[1] != 1 or face_detected != 1):\n",
    "                # Create a detailed failure type description for the filename\n",
    "                failure_type = []\n",
    "                if detection_status[0] != 1:\n",
    "                    failure_type.append(\"DomHand\")\n",
    "                if detection_status[1] != 1:\n",
    "                    failure_type.append(\"NonDomHand\")\n",
    "                if face_detected != 1:\n",
    "                    failure_type.append(\"Face\")\n",
    "                \n",
    "                failure_str = \"_\".join(failure_type)\n",
    "                screenshot_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}_missing_{failure_str}.jpg\"\n",
    "                screenshot_path = screenshots_dir / screenshot_filename\n",
    "                \n",
    "                # Copy the frame to the screenshots directory\n",
    "                cv2.imwrite(str(screenshot_path), frame)\n",
    "            \n",
    "            # Save all results in a single .npz file\n",
    "            np.savez(\n",
    "                output_path,\n",
    "                dom_landmarks=dom_landmarks,\n",
    "                non_dom_landmarks=non_dom_landmarks,\n",
    "                confidence_scores=confidence_scores,\n",
    "                interpolation_scores=interpolation_scores,\n",
    "                detection_status=detection_status,\n",
    "                blendshape_scores=blendshape_scores,\n",
    "                face_detected=face_detected,\n",
    "                nose_to_wrist_dist=nose_to_wrist_dist,\n",
    "                frame_idx=np.array([frame_idx]),\n",
    "                timestamp_ms=np.array([timestamp_ms])\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"frame_idx\": frame_idx,\n",
    "                \"timestamp_ms\": timestamp_ms,\n",
    "                \"timestamp_formatted\": timestamp_formatted,\n",
    "                \"detection_status\": detection_status,\n",
    "                \"face_detected\": face_detected,\n",
    "                \"proc_time\": proc_time\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"frame_idx\": frame_idx,\n",
    "                \"timestamp_ms\": timestamp_ms,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "        finally:\n",
    "            # Clean up temporary frame file\n",
    "            if temp_frame_path.exists():\n",
    "                try:\n",
    "                    temp_frame_path.unlink()\n",
    "                except:\n",
    "                    pass  # Ignore errors during cleanup\n",
    "    \n",
    "    # Create a thread-safe lock for updating the progress bar to avoid output issues\n",
    "    progress_lock = threading.Lock()\n",
    "    \n",
    "    # Collect frames to process\n",
    "    frames_to_process = []\n",
    "    frame_idx = start_frame\n",
    "    \n",
    "    # Skip to start_frame\n",
    "    if start_frame > 0:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "    # Process frames\n",
    "    processed_count = 0\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        if not batch_mode:\n",
    "            print(\"Reading frames to process...\")\n",
    "        \n",
    "        # First phase: Read all frames to process\n",
    "        while frame_idx < end_frame:\n",
    "            # Read the next frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "            \n",
    "            # Only process every frame_step frames\n",
    "            if (frame_idx - start_frame) % frame_step == 0:\n",
    "                frames_to_process.append((frame.copy(), frame_idx, temp_dir))\n",
    "            \n",
    "            frame_idx += 1\n",
    "        \n",
    "        # Release the video capture to free resources\n",
    "        cap.release()\n",
    "        \n",
    "        if not batch_mode:\n",
    "            print(f\"Starting parallel processing of {len(frames_to_process)} frames with {num_workers} workers...\")\n",
    "        \n",
    "        try:\n",
    "            # Process frames in parallel using ThreadPoolExecutor\n",
    "            # Use a context manager to ensure all resources are properly released\n",
    "            with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                # Submit all frame processing tasks\n",
    "                futures = [executor.submit(process_single_frame, args) for args in frames_to_process]\n",
    "                \n",
    "                # Process results as they complete\n",
    "                for i, future in enumerate(as_completed(futures)):\n",
    "                    result = future.result()\n",
    "                    \n",
    "                    if result[\"success\"]:\n",
    "                        frame_idx = result[\"frame_idx\"]\n",
    "                        detection_status = result[\"detection_status\"]\n",
    "                        face_detected = result[\"face_detected\"]\n",
    "                        proc_time = result[\"proc_time\"]\n",
    "                        \n",
    "                        # Update detection statistics\n",
    "                        dom_hand_detected = detection_status[0] == 1\n",
    "                        non_dom_hand_detected = detection_status[1] == 1\n",
    "                        face_was_detected = face_detected == 1\n",
    "                        \n",
    "                        if dom_hand_detected:\n",
    "                            stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] += 1\n",
    "                        else:\n",
    "                            stats[\"detection_rates\"][\"dominant_hand\"][\"failed\"] += 1\n",
    "                            stats[\"failed_frames\"][\"dominant_hand_failures\"].append({\n",
    "                                \"frame\": frame_idx,\n",
    "                                \"timestamp_ms\": result[\"timestamp_ms\"],\n",
    "                                \"file\": f\"{video_name}_frame{frame_idx:06d}_{result['timestamp_formatted']}.npz\"\n",
    "                            })\n",
    "                        \n",
    "                        if non_dom_hand_detected:\n",
    "                            stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] += 1\n",
    "                        else:\n",
    "                            stats[\"detection_rates\"][\"non_dominant_hand\"][\"failed\"] += 1\n",
    "                            stats[\"failed_frames\"][\"non_dominant_hand_failures\"].append({\n",
    "                                \"frame\": frame_idx,\n",
    "                                \"timestamp_ms\": result[\"timestamp_ms\"],\n",
    "                                \"file\": f\"{video_name}_frame{frame_idx:06d}_{result['timestamp_formatted']}.npz\"\n",
    "                            })\n",
    "                        \n",
    "                        if face_was_detected:\n",
    "                            stats[\"detection_rates\"][\"face\"][\"detected\"] += 1\n",
    "                        else:\n",
    "                            stats[\"detection_rates\"][\"face\"][\"failed\"] += 1\n",
    "                            stats[\"failed_frames\"][\"face_failures\"].append({\n",
    "                                \"frame\": frame_idx,\n",
    "                                \"timestamp_ms\": result[\"timestamp_ms\"],\n",
    "                                \"file\": f\"{video_name}_frame{frame_idx:06d}_{result['timestamp_formatted']}.npz\"\n",
    "                            })\n",
    "                        \n",
    "                        # Track combined detection status\n",
    "                        detection_count = dom_hand_detected + non_dom_hand_detected + face_was_detected\n",
    "                        \n",
    "                        if detection_count == 3:\n",
    "                            stats[\"detection_rates\"][\"overall\"][\"all_detected\"] += 1\n",
    "                        elif detection_count == 0:\n",
    "                            stats[\"detection_rates\"][\"overall\"][\"no_detections\"] += 1\n",
    "                            stats[\"failed_frames\"][\"all_failures\"].append({\n",
    "                                \"frame\": frame_idx,\n",
    "                                \"timestamp_ms\": result[\"timestamp_ms\"],\n",
    "                                \"file\": f\"{video_name}_frame{frame_idx:06d}_{result['timestamp_formatted']}.npz\"\n",
    "                            })\n",
    "                        else:\n",
    "                            stats[\"detection_rates\"][\"overall\"][\"partial_detections\"] += 1\n",
    "                        \n",
    "                        # Update processing log\n",
    "                        detection_summary = f\"Dom: {detection_status[0]}, Non-dom: {detection_status[1]}, Face: {face_detected}\"\n",
    "                        log_entry = f\"Frame {frame_idx}: {detection_summary} (proc time: {proc_time:.2f}s)\\n\"\n",
    "                        \n",
    "                        with open(log_file, \"a\") as log:\n",
    "                            log.write(log_entry)\n",
    "                        \n",
    "                        total_processing_time += proc_time\n",
    "                        processed_count += 1\n",
    "                        \n",
    "                        # Update progress with thread safety, but only if not in batch mode\n",
    "                        # This prevents console output conflicts when running in batch\n",
    "                        if not batch_mode:\n",
    "                            with progress_lock:\n",
    "                                update_progress(frame_idx, total_frames, result[\"timestamp_formatted\"])\n",
    "                                \n",
    "                    else:\n",
    "                        # Handle error\n",
    "                        with open(log_file, \"a\") as log:\n",
    "                            log.write(f\"Error on frame {result['frame_idx']}: {result['error']}\\n\")\n",
    "                        \n",
    "                        # Update progress (even for errors), but only if not in batch mode\n",
    "                        if not batch_mode:\n",
    "                            with progress_lock:\n",
    "                                update_progress(result['frame_idx'], total_frames, \"ERROR\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during parallel processing: {str(e)}\")\n",
    "            # Continue with cleanup to ensure resources are released\n",
    "        finally:\n",
    "            # Clear the frames_to_process list to free memory\n",
    "            frames_to_process.clear()\n",
    "    \n",
    "    # Update final statistics\n",
    "    stats[\"video_info\"][\"processed_frames\"] = processed_count\n",
    "    stats[\"video_info\"][\"processing_completed\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Calculate detection rates\n",
    "    if processed_count > 0:\n",
    "        stats[\"detection_rates\"][\"dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"non_dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"face\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"face\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"overall\"][\"success_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"overall\"][\"all_detected\"] / processed_count * 100\n",
    "        )\n",
    "    \n",
    "    # Calculate processing performance\n",
    "    if processed_count > 0:\n",
    "        stats[\"processing_performance\"][\"average_processing_time_ms\"] = (\n",
    "            total_processing_time / processed_count * 1000\n",
    "        )\n",
    "    stats[\"processing_performance\"][\"total_processing_time_seconds\"] = total_processing_time\n",
    "    \n",
    "    # Save statistics to JSON file\n",
    "    with open(stats_file, \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # Add summary statistics to log file\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"\\n\\n===== PROCESSING SUMMARY =====\\n\")\n",
    "        log.write(f\"Completed at: {stats['video_info']['processing_completed']}\\n\")\n",
    "        log.write(f\"Frames processed: {processed_count} from {start_frame} to {min(end_frame, frame_idx-1)}\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION RATES:\\n\")\n",
    "        log.write(f\"  Dominant hand ({dominand_hand}): {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Non-dominant hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  All parts detected: {stats['detection_rates']['overall']['success_rate']:.1f}%\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION FAILURES:\\n\")\n",
    "        log.write(f\"  Frames with dominant hand failures: {len(stats['failed_frames']['dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with non-dominant hand failures: {len(stats['failed_frames']['non_dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with face failures: {len(stats['failed_frames']['face_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with all parts missing: {len(stats['failed_frames']['all_failures'])}\\n\\n\")\n",
    "        \n",
    "        log.write(\"PERFORMANCE:\\n\")\n",
    "        log.write(f\"  Average processing time per frame: {stats['processing_performance']['average_processing_time_ms']:.2f} ms\\n\")\n",
    "        log.write(f\"  Total processing time: {stats['processing_performance']['total_processing_time_seconds']:.2f} seconds\\n\")\n",
    "    \n",
    "    # Adjust summary output based on batch mode\n",
    "    if batch_mode:\n",
    "        print(f\"[{os.path.basename(str(video_path))}] Processed {processed_count} frames: \" +\n",
    "              f\"Dom: {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "              f\"Non-dom: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "              f\"Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\n===== PROCESSING SUMMARY =====\")\n",
    "        print(f\"Processed {processed_count} frames\")\n",
    "        print(f\"Detection rates: Dom hand: {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "              f\"Non-dom hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "              f\"Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\")\n",
    "        print(f\"All parts detected in {stats['detection_rates']['overall']['success_rate']:.1f}% of frames\")\n",
    "        print(f\"Full statistics saved to: {stats_file}\")\n",
    "        print(f\"Results saved to: {output_dir}\")\n",
    "        \n",
    "    # Force garbage collection to free memory\n",
    "    # This is especially important in batch processing\n",
    "    gc.collect()\n",
    "    \n",
    "    return str(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_call_process_video(video_path, start_time_seconds=0, end_time_seconds=None):\n",
    "    \"\"\"\n",
    "    Wrapper function to call process_video with optimized settings for batch processing\n",
    "    \n",
    "    This wrapper ensures that process_video runs efficiently when processing\n",
    "    multiple videos one after another.\n",
    "    \"\"\"\n",
    "    # Set batch_mode to True and use conservative worker count\n",
    "    return process_video_new_2(\n",
    "        video_path=video_path,\n",
    "        adaptive_detect_func=adaptive_detect,  # Use your actual adaptive_detect function\n",
    "        hand_model_path=hand_model_path,  # Use your actual path\n",
    "        face_model_path=face_model_path,  # Use your actual path\n",
    "        start_time_seconds=start_time_seconds,\n",
    "        end_time_seconds=end_time_seconds,\n",
    "        # Enable batch mode for reduced console output and resource usage\n",
    "        batch_mode=True,  \n",
    "        # Conservative worker count to prevent system overload\n",
    "        num_workers=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_videos(video_df, process_video_func, detection_threshold_dom, detection_threshold_non_dom, \n",
    "                         detection_threshold_dom_small_length, detection_threshold_non_dom_small_length, report_dir, \n",
    "                         start_time_seconds=0, end_time_seconds=None, delete_videos=False, \n",
    "                         start_from_row=0):\n",
    "    \"\"\"\n",
    "    Process multiple videos from a pandas dataframe with memory-efficient operation.\n",
    "    \n",
    "    This function processes videos one by one, storing detailed results on disk rather than in memory.\n",
    "    This approach prevents memory growth regardless of how many videos are processed, making it\n",
    "    suitable for processing thousands of videos in a single run.\n",
    "    \n",
    "\n",
    "    \n",
    "    Args:\n",
    "        video_df (pandas.DataFrame): DataFrame with 'file_path', 'FPS', and 'Frame Count' columns\n",
    "        process_video_func: The process_video function to use for processing each video\n",
    "        detection_threshold_dom (float): Minimum detection rate (%) for dominant hand\n",
    "        detection_threshold_non_dom (float): Minimum detection rate (%) for non-dominant hand\n",
    "        detection_threshold_dom_small_length (float): Threshold for short videos (dominant hand)\n",
    "        detection_threshold_non_dom_small_length (float): Threshold for short videos (non-dominant hand)\n",
    "        start_time_seconds (float): Start time for video processing\n",
    "        end_time_seconds (float): End time for video processing\n",
    "        delete_videos (bool): Whether to delete original videos after processing\n",
    "        start_from_row (int): Index to start processing from (for resuming previous runs)\n",
    "        report_dir (str): Directory to store reports and detailed results\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the generated report JSON file\n",
    "    \"\"\"\n",
    "    # Create report directory structure\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    details_dir = os.path.join(report_dir, \"video_details\")\n",
    "    os.makedirs(details_dir, exist_ok=True)\n",
    "    \n",
    "    # Define report file paths\n",
    "    current_report_path = os.path.join(report_dir, \"video_processing_report_current.json\")\n",
    "    temp_report_path = os.path.join(report_dir, \"video_processing_report_temp.json\")\n",
    "    \n",
    "    # Initialize statistics for logging\n",
    "    if start_from_row > 0 and os.path.exists(current_report_path):\n",
    "        # Load previous summary statistics if resuming\n",
    "        print(f\"Loading previous report from {current_report_path}\")\n",
    "        try:\n",
    "            with open(current_report_path, \"r\") as f:\n",
    "                stats = json.load(f)\n",
    "            \n",
    "            # Update resume information\n",
    "            stats[\"processing_info\"][\"resume_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            stats[\"processing_info\"][\"resumed_from_row\"] = start_from_row\n",
    "            \n",
    "            print(f\"Successfully loaded previous report summary\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading previous report: {e}\")\n",
    "            return\n",
    "\n",
    "    else:\n",
    "        stats = None\n",
    "        \n",
    "    # Create new statistics if not resuming or if loading failed\n",
    "    if stats is None:\n",
    "        stats = {\n",
    "            \"processing_info\": {\n",
    "                \"start_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"end_time\": None,\n",
    "                \"total_videos\": len(video_df),\n",
    "                \"videos_processed\": 0,\n",
    "                \"directories_deleted\": 0,\n",
    "                \"videos_deleted\": 0,\n",
    "                \"detection_threshold_dom\": detection_threshold_dom,\n",
    "                \"detection_threshold_non_dom\": detection_threshold_non_dom,\n",
    "                \"detection_threshold_dom_small_length\": detection_threshold_dom_small_length,\n",
    "                \"detection_threshold_non_dom_small_length\": detection_threshold_non_dom_small_length,\n",
    "                \"last_processed_row\": -1,\n",
    "            },\n",
    "            \"deleted_directories_summary\": {\n",
    "                \"count\": 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "    if start_from_row > 0:\n",
    "        last_processed_row = stats[\"processing_info\"].get(\"last_processed_row\", -1)\n",
    "        if last_processed_row >= 0 and start_from_row <= last_processed_row:\n",
    "            print(f\"Resuming from row {start_from_row} (last processed row was {last_processed_row})\")\n",
    "        elif last_processed_row >= 0 and start_from_row > last_processed_row + 1:\n",
    "            print(f\"Warning: Skipping rows {last_processed_row+1} to {start_from_row-1}\") \n",
    "    \n",
    "    # Calculate the number of videos to process\n",
    "    total_videos = len(video_df)\n",
    "    remaining_videos = total_videos - start_from_row\n",
    "    \n",
    "    print(f\"Starting batch processing from row {start_from_row} ({remaining_videos} videos remaining)\")\n",
    "    print(f\"Detection thresholds (normal): Dom={detection_threshold_dom}%, Non-Dom={detection_threshold_non_dom}%\")\n",
    "    print(f\"Detection thresholds (short videos): Dom={detection_threshold_dom_small_length}%, Non-Dom={detection_threshold_non_dom_small_length}%\")\n",
    "    print(f\"Using memory-efficient processing - detailed results stored in: {details_dir}\")\n",
    "    \n",
    "    # Helper function to save the current report using atomic operations\n",
    "    def save_current_report():\n",
    "        stats[\"processing_info\"][\"last_updated\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Write to temporary file first to avoid corruption\n",
    "        with open(temp_report_path, \"w\") as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        \n",
    "        # Atomic rename to ensure file integrity\n",
    "        if os.path.exists(temp_report_path):\n",
    "            os.rename(temp_report_path, current_report_path)\n",
    "    \n",
    "    # Save initial report\n",
    "    save_current_report()\n",
    "    \n",
    "    # Process each video starting from the specified row\n",
    "    for idx in range(start_from_row, total_videos):\n",
    "        row = video_df.iloc[idx]\n",
    "        video_path = row['file_path']\n",
    "        video_fps = row['FPS']\n",
    "        video_framecount = row['Frame Count']\n",
    "        video_length = video_framecount / video_fps\n",
    "        \n",
    "\n",
    "        # Skip if file doesn't exist\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Video not found: {video_path}\")\n",
    "            \n",
    "            # Create video detail file for skipped video\n",
    "            video_detail = {\n",
    "                \"video_path\": video_path,\n",
    "                \"status\": \"skipped\",\n",
    "                \"reason\": \"file_not_found\",\n",
    "                \"processed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"row_index\": idx\n",
    "            }\n",
    "            \n",
    "            # Save detail to separate file\n",
    "            path_hash = hashlib.md5(video_path.encode()).hexdigest()[:8]\n",
    "            detail_filename = f\"video_detail_{idx}_{path_hash}.json\"\n",
    "            with open(os.path.join(details_dir, detail_filename), 'w') as f:\n",
    "                json.dump(video_detail, f, indent=2)\n",
    "                \n",
    "            # Mark as processed\n",
    "            stats[\"processing_info\"][\"last_processed_row\"] = idx\n",
    "            \n",
    "            # Save summary report\n",
    "            save_current_report()\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing video {idx-start_from_row+1}/{remaining_videos} (overall: {idx+1}/{total_videos}): {video_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Determine output directory path (before actually processing)\n",
    "            video_dir = os.path.dirname(video_path)\n",
    "            video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "            output_dir = os.path.join(video_dir, f\"{video_name}_landmarks\")\n",
    "            \n",
    "            # Process the video\n",
    "            process_start_time = datetime.now()\n",
    "            process_video_func(video_path, start_time_seconds=start_time_seconds, end_time_seconds=end_time_seconds)\n",
    "            process_end_time = datetime.now()\n",
    "            process_duration = (process_end_time - process_start_time).total_seconds()\n",
    "            \n",
    "            stats[\"processing_info\"][\"videos_processed\"] += 1\n",
    "            \n",
    "            # Check detection statistics\n",
    "            stats_file = os.path.join(output_dir, \"detection_statistics.json\")\n",
    "            \n",
    "            if not os.path.exists(stats_file):\n",
    "                print(f\"Warning: Statistics file not found for {video_path}\")\n",
    "                \n",
    "                # Create video detail file for error\n",
    "                video_detail = {\n",
    "                    \"video_path\": video_path,\n",
    "                    \"status\": \"error\",\n",
    "                    \"reason\": \"statistics_file_not_found\",\n",
    "                    \"processed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"processing_time_seconds\": process_duration,\n",
    "                    \"row_index\": idx\n",
    "                }\n",
    "                \n",
    "                # Save detail to separate file\n",
    "                path_hash = hashlib.md5(video_path.encode()).hexdigest()[:8]\n",
    "                detail_filename = f\"video_detail_{idx}_{path_hash}.json\"\n",
    "                with open(os.path.join(details_dir, detail_filename), 'w') as f:\n",
    "                    json.dump(video_detail, f, indent=2)\n",
    "                    \n",
    "                \n",
    "                stats[\"processing_info\"][\"last_processed_row\"] = idx\n",
    "                \n",
    "                # Save summary report\n",
    "                save_current_report()\n",
    "                continue\n",
    "                \n",
    "            # Read detection statistics\n",
    "            with open(stats_file, \"r\") as f:\n",
    "                detection_stats = json.load(f)\n",
    "            \n",
    "            # Check detection rates\n",
    "            dom_hand_rate = detection_stats[\"detection_rates\"][\"dominant_hand\"][\"detection_rate\"]\n",
    "            non_dom_hand_rate = detection_stats[\"detection_rates\"][\"non_dominant_hand\"][\"detection_rate\"]\n",
    "            face_rate = detection_stats[\"detection_rates\"][\"face\"][\"detection_rate\"]\n",
    "            \n",
    "            # Create video details\n",
    "            video_detail = {\n",
    "                \"video_path\": video_path,\n",
    "                \"output_dir\": output_dir,\n",
    "                \"dominant_hand_rate\": dom_hand_rate,\n",
    "                \"non_dominant_hand_rate\": non_dom_hand_rate,\n",
    "                \"face_rate\": face_rate,\n",
    "                \"processed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"processing_time_seconds\": process_duration,\n",
    "                \"row_index\": idx,\n",
    "                \"video_length_seconds\": video_length\n",
    "            }\n",
    "            \n",
    "            # Apply appropriate thresholds based on video length\n",
    "            current_detection_threshold_dom = detection_threshold_dom\n",
    "            current_detection_threshold_non_dom = detection_threshold_non_dom\n",
    "            \n",
    "            if video_length < 10: \n",
    "                current_detection_threshold_dom = detection_threshold_dom_small_length\n",
    "                current_detection_threshold_non_dom = detection_threshold_non_dom_small_length\n",
    "                video_detail[\"thresholds_used\"] = \"short_video\"\n",
    "            else:\n",
    "                video_detail[\"thresholds_used\"] = \"normal\"\n",
    "                \n",
    "            # Check if detection rates are below threshold\n",
    "            if dom_hand_rate < current_detection_threshold_dom and non_dom_hand_rate < current_detection_threshold_non_dom:\n",
    "                print(f\"Low detection rate for {video_name}: Dom={dom_hand_rate:.1f}%, Non-Dom={non_dom_hand_rate:.1f}%\")\n",
    "                print(f\"Deleting directory: {output_dir}\")\n",
    "                \n",
    "                # Delete the directory\n",
    "                shutil.rmtree(output_dir)\n",
    "                \n",
    "                # Update statistics\n",
    "                stats[\"processing_info\"][\"directories_deleted\"] += 1\n",
    "                stats[\"deleted_directories_summary\"][\"count\"] += 1\n",
    "                \n",
    "                video_detail[\"status\"] = \"deleted\"\n",
    "                video_detail[\"reason\"] = \"low_detection_rate\"\n",
    "            else:\n",
    "                print(f\"Detection rates acceptable: Dom={dom_hand_rate:.1f}%, Non-Dom={non_dom_hand_rate:.1f}%\")\n",
    "                video_detail[\"status\"] = \"kept\"\n",
    "            \n",
    "            # Save detail to separate file\n",
    "            path_hash = hashlib.md5(video_path.encode()).hexdigest()[:8]\n",
    "            detail_filename = f\"video_detail_{idx}_{path_hash}.json\"\n",
    "            with open(os.path.join(details_dir, detail_filename), 'w') as f:\n",
    "                json.dump(video_detail, f, indent=2)\n",
    "            \n",
    "            stats[\"processing_info\"][\"last_processed_row\"] = idx\n",
    "            \n",
    "            # Delete original video if requested\n",
    "            if delete_videos:\n",
    "                os.remove(video_path)\n",
    "                stats[\"processing_info\"][\"videos_deleted\"] += 1\n",
    "                print(f\"Deleted original video: {video_path}\")\n",
    "            \n",
    "            # Save report after each video\n",
    "            save_current_report()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_path}: {str(e)}\")\n",
    "            \n",
    "            # Create video detail file for error\n",
    "            video_detail = {\n",
    "                \"video_path\": video_path,\n",
    "                \"status\": \"error\",\n",
    "                \"reason\": str(e),\n",
    "                \"processed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"row_index\": idx\n",
    "            }\n",
    "            \n",
    "            # Save detail to separate file\n",
    "            path_hash = hashlib.md5(video_path.encode()).hexdigest()[:8]\n",
    "            detail_filename = f\"video_detail_{idx}_{path_hash}.json\"\n",
    "            with open(os.path.join(details_dir, detail_filename), 'w') as f:\n",
    "                json.dump(video_detail, f, indent=2)\n",
    "                \n",
    "            \n",
    "            stats[\"processing_info\"][\"last_processed_row\"] = idx\n",
    "            \n",
    "            # Save report even when errors occur\n",
    "            save_current_report()\n",
    "    \n",
    "    # Complete statistics\n",
    "    stats[\"processing_info\"][\"end_time\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    \n",
    "    # Save final current report\n",
    "    save_current_report()\n",
    "    \n",
    "    # Also save a timestamped archive copy\n",
    "    archive_report_path = os.path.join(report_dir, f\"video_processing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    shutil.copy(current_report_path, archive_report_path)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n===== PROCESSING SUMMARY =====\")\n",
    "    print(f\"Total videos: {stats['processing_info']['total_videos']}\")\n",
    "    print(f\"Videos processed (this run + previous): {stats['processing_info']['videos_processed']}\")\n",
    "    print(f\"Directories deleted (low detection rate): {stats['processing_info']['directories_deleted']}\")\n",
    "    if delete_videos:\n",
    "        print(f\"Original videos deleted: {stats['processing_info']['videos_deleted']}\")\n",
    "    print(f\"Current report saved to: {current_report_path}\")\n",
    "    print(f\"Archive report saved to: {archive_report_path}\")\n",
    "    print(f\"Detailed results stored in: {details_dir}\")\n",
    "    \n",
    "    return archive_report_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_call_process_video_with_output(video_path, start_time_seconds=0, end_time_seconds=None):\n",
    "    \"\"\"Wrapper with explicit console output\"\"\"\n",
    "    import sys\n",
    "    \n",
    "    print(f\"\\nStarting processing of video: {os.path.basename(video_path)}\", flush=True)\n",
    "    \n",
    "    # Call the original function\n",
    "    result = batch_call_process_video(video_path, start_time_seconds, end_time_seconds)\n",
    "    \n",
    "    print(f\"Completed processing of video: {os.path.basename(video_path)}\", flush=True)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing from row 0 (2 videos remaining)\n",
      "Detection thresholds (normal): Dom=70%, Non-Dom=50%\n",
      "Detection thresholds (short videos): Dom=20%, Non-Dom=-1%\n",
      "Using memory-efficient processing - detailed results stored in: ./report/video_details\n",
      "\n",
      "Processing video 1/2 (overall: 1/2): ./videos/0636580316216-SHUT OUT_fps15_L.mp4\n",
      "\n",
      "Starting processing of video: 0636580316216-SHUT OUT_fps15_L.mp4\n",
      "[0636580316216-SHUT OUT_fps15_L.mp4] Dominant hand: Left\n",
      "[0636580316216-SHUT OUT_fps15_L.mp4] Using 8 worker threads\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: False (confidence: 0.000)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering dominant hand thresholds: 0.245, 0.245\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering dominant hand thresholds: 0.245, 0.245\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "No improvement in detection. Continuing to next attempt.No improvement in detection. Continuing to next attempt.No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: False (confidence: 0.000)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "\n",
      "--- Attempt 3/3 ---Face detected: True\n",
      "\n",
      "Total detection attempts: 3\n",
      "\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: False (confidence: 0.000)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743593991.048293 1195076 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743593991.148005 1195601 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 560.28.03), renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2\n",
      "W0000 00:00:1743593991.148263 1195076 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1743593991.152455 1195604 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743593991.158893 1195603 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1743593991.441263 1195079 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743593991.565030 1195615 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 560.28.03), renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2\n",
      "W0000 00:00:1743593991.588773 1195621 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743593991.602118 1195625 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1743593991.602434 1195074 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743593991.697823 1195629 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 560.28.03), renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2\n",
      "W0000 00:00:1743593991.698079 1195074 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1743593991.702415 1195632 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743593991.718096 1195638 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1743593991.719574 1195080 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743593991.814361 1195643 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 560.28.03), renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2\n",
      "W0000 00:00:1743593991.844462 1195646 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743593991.865848 1195649 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1743593991.990134 1195081 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743593992.131426 1195657 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 560.28.03), renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2\n",
      "W0000 00:00:1743593992.131722 1195081 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1743593992.135881 1195660 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743593992.151137 1195662 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1743593992.675036 1195080 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743593992.865359 1195671 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 560.28.03), renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2\n",
      "W0000 00:00:1743593992.865917 1195080 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1743593992.874861 1195674 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743593992.890555 1195684 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1743593992.892559 1195079 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743593992.997868 1195685 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 560.28.03), renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2\n",
      "W0000 00:00:1743593992.998115 1195079 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1743593993.002638 1195688 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743593993.027695 1195698 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./report/video_processing_report_20250402_144116.json'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_process_videos(\n",
    "    video_df=videos_df, \n",
    "    process_video_func=batch_call_process_video_with_output,\n",
    "    detection_threshold_dom=70, \n",
    "    detection_threshold_non_dom=50,\n",
    "    detection_threshold_dom_small_length=20,\n",
    "    detection_threshold_non_dom_small_length=-1,\n",
    "    start_from_row=0, report_dir=\"./report\", delete_videos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./report/video_processing_report_current.json\", \"r\") as f:\n",
    "    current_report = json.load(f)\n",
    "    last_row = current_report[\"processing_info\"][\"last_processed_row\"]\n",
    "    next_row = last_row + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./report/video_processing_report_20250402_144318.json'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_process_videos(\n",
    "    video_df=videos_df, \n",
    "    process_video_func=process_video,\n",
    "    detection_threshold_dom=70, \n",
    "    detection_threshold_non_dom=50,\n",
    "    detection_threshold_dom_small_length=20,\n",
    "    detection_threshold_non_dom_small_length=-1,\n",
    "    start_from_row=0, report_dir=\"./report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_frame = load_frame_data(\"./results_old/0636580316216-SHUT OUT_fps15_L_landmarks/0636580316216-SHUT OUT_fps15_L_frame000010_00m00s666ms.npz\")\n",
    "new_frame = load_frame_data(\"./results_new/0636580316216-SHUT OUT_fps15_L_landmarks/0636580316216-SHUT OUT_fps15_L_frame000010_00m00s666ms.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.26612747, -0.24335966, -0.02193258],\n",
       "        [-0.40496428, -0.67612321, -0.03732664],\n",
       "        [-0.5038505 , -1.00486118, -0.05108703],\n",
       "        [-0.68340494, -1.27349305, -0.06534011],\n",
       "        [ 0.01753619, -1.59687821, -0.02775248],\n",
       "        [ 0.20681654, -2.19980728, -0.04594909],\n",
       "        [ 0.38957515, -2.60826973, -0.06086953],\n",
       "        [ 0.57177159, -2.9460909 , -0.07134181],\n",
       "        [ 0.15579441, -1.56000904, -0.02811006],\n",
       "        [ 0.33664678, -2.18237038, -0.0434243 ],\n",
       "        [ 0.5306638 , -2.61662499, -0.05503937],\n",
       "        [ 0.74907653, -2.99776513, -0.06343113],\n",
       "        [ 0.25404104, -1.40736799, -0.03068523],\n",
       "        [ 0.44771482, -2.03086463, -0.044617  ],\n",
       "        [ 0.62650457, -2.42245784, -0.05291518],\n",
       "        [ 0.80730064, -2.77223252, -0.0584594 ],\n",
       "        [ 0.3268286 , -1.15569998, -0.03548307],\n",
       "        [ 0.48484357, -1.59980482, -0.04741046],\n",
       "        [ 0.6025303 , -1.89355485, -0.05242891],\n",
       "        [ 0.71105486, -2.15862052, -0.0557082 ]]),\n",
       " array([[ 5.39556221e-01, -6.32520416e-01,  8.57848208e-03],\n",
       "        [ 7.52211904e-01, -1.35080774e+00,  9.82018188e-03],\n",
       "        [ 9.97913714e-01, -1.80215834e+00,  5.96622797e-03],\n",
       "        [ 1.18196401e+00, -2.21770158e+00,  7.78851740e-04],\n",
       "        [ 1.75612153e-01, -2.25478186e+00,  1.06420573e-02],\n",
       "        [ 1.28363023e-01, -3.16402902e+00, -3.91567079e-03],\n",
       "        [ 9.45332495e-02, -3.84760754e+00, -1.89183075e-02],\n",
       "        [ 2.75584414e-02, -4.40514016e+00, -3.02670319e-02],\n",
       "        [ 1.61174973e-01, -2.16881322e+00, -4.79245652e-03],\n",
       "        [ 4.26899383e-02, -3.15950502e+00, -1.98168289e-02],\n",
       "        [ 1.51301285e-02, -3.87804802e+00, -3.63130346e-02],\n",
       "        [-1.75793942e-02, -4.53685944e+00, -4.93968427e-02],\n",
       "        [ 2.07546433e-01, -1.99713474e+00, -2.10617315e-02],\n",
       "        [ 1.16061571e-01, -3.02425942e+00, -3.52366865e-02],\n",
       "        [ 6.90313703e-02, -3.73896413e+00, -4.57902141e-02],\n",
       "        [ 1.53431931e-02, -4.38359913e+00, -5.35019785e-02],\n",
       "        [ 3.20227510e-01, -1.74078557e+00, -3.71669903e-02],\n",
       "        [ 2.58699532e-01, -2.61978773e+00, -4.93849851e-02],\n",
       "        [ 2.29599598e-01, -3.25450603e+00, -5.45377322e-02],\n",
       "        [ 1.99551233e-01, -3.80762567e+00, -5.84841520e-02]]),\n",
       " array([0.862849  , 0.97563595]),\n",
       " array([1., 1.]),\n",
       " array([1, 1], dtype=int32),\n",
       " array([2.61808100e-06, 3.99339758e-03, 9.43745300e-03, 2.34994590e-02,\n",
       "        3.40591699e-01, 1.42032906e-01, 9.56317854e-06, 2.88195224e-07,\n",
       "        6.36944719e-07, 1.61824971e-01, 1.31367743e-01, 3.42025399e-01,\n",
       "        3.07312399e-01, 1.25510767e-02, 2.83290595e-01, 2.77423412e-01,\n",
       "        1.08566480e-02, 2.09741928e-02, 2.15646103e-02, 2.27990270e-01,\n",
       "        3.23441029e-01, 3.86391021e-03, 5.73133910e-03, 4.46181475e-05,\n",
       "        1.51199711e-04, 2.30723090e-04, 1.57151117e-05, 4.83574477e-05,\n",
       "        9.04691499e-03, 4.23019053e-04, 5.53076202e-03, 8.89929198e-03,\n",
       "        3.29005881e-04, 6.56135147e-03, 4.44346260e-05, 6.04493107e-05,\n",
       "        9.72673520e-02, 1.60017274e-02, 4.86925617e-03, 1.75962399e-04,\n",
       "        9.34927433e-04, 4.75689943e-04, 1.86871126e-01, 2.40445405e-01,\n",
       "        1.64583847e-01, 9.56770703e-02, 5.07142767e-02, 1.25538260e-01,\n",
       "        2.88174581e-02, 3.67524698e-02, 7.91069760e-06, 1.05426182e-06]),\n",
       " 1,\n",
       " array([[ 3.92593283,  2.43501106],\n",
       "        [-2.88507467,  2.22415216]]),\n",
       " 10,\n",
       " 666)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_frame[1]==old_frame[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_single_video(video_info):\n",
    "    \"\"\"Helper function to resample a single video\"\"\"\n",
    "    video_path, desired_fps = video_info\n",
    "    try:\n",
    "        # Get original video properties\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            return {\"path\": video_path, \"status\": \"error\", \"message\": \"Could not open video\"}\n",
    "        \n",
    "        current_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = total_frames / current_fps\n",
    "        output_total_frames = int(duration * desired_fps)\n",
    "        \n",
    "        # Generate output path\n",
    "        video_dir = os.path.dirname(video_path)\n",
    "        video_filename = os.path.basename(video_path)\n",
    "        base_name, ext = os.path.splitext(video_filename)\n",
    "        output_path = os.path.join(video_dir, f\"{base_name}_fps{int(desired_fps)}{ext}\")\n",
    "        \n",
    "        # Create VideoWriter\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, desired_fps, (width, height))\n",
    "        \n",
    "        # Process frames\n",
    "        for i in range(output_total_frames):\n",
    "            original_frame_idx = round(i * current_fps / desired_fps)\n",
    "            if original_frame_idx >= total_frames:\n",
    "                break\n",
    "                \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, original_frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            out.write(frame)\n",
    "        \n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        \n",
    "        # Get new video metadata\n",
    "        new_cap = cv2.VideoCapture(output_path)\n",
    "        new_frame_count = int(new_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        new_fps = new_cap.get(cv2.CAP_PROP_FPS)\n",
    "        new_cap.release()\n",
    "        \n",
    "        return {\n",
    "            \"path\": video_path,\n",
    "            \"new_path\": output_path,\n",
    "            \"status\": \"success\",\n",
    "            \"frame_count\": new_frame_count,\n",
    "            \"fps\": new_fps\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"path\": video_path, \"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "def resample_videos_in_dataframe(df, desired_fps, save_checkpoint=False, checkpoint_file='resampling_progress.csv', file_path_col='file_path', \n",
    "                                delete_originals=False, inplace=False, \n",
    "                                max_workers=None, batch_size=20):\n",
    "    \"\"\"\n",
    "    Efficiently resamples videos in a dataframe with parallel processing and batching.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): Dataframe containing video paths\n",
    "        desired_fps (float): The desired FPS for the output videos\n",
    "        file_path_col (str): Name of the column containing file paths\n",
    "        delete_originals (bool): Whether to delete original videos after resampling\n",
    "        inplace (bool): Whether to modify the original dataframe or return a copy\n",
    "        max_workers (int): Max number of parallel processes (None = auto-detect)\n",
    "        batch_size (int): Size of batches for processing\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Updated dataframe with new file paths and metadata\n",
    "    \"\"\"\n",
    "    # Use original dataframe or create a copy\n",
    "    result_df = df if inplace else df.copy()\n",
    "    \n",
    "    # Check if metadata columns exist\n",
    "    has_frame_count_col = 'Frame Count' in result_df.columns\n",
    "    has_fps_col = 'FPS' in result_df.columns\n",
    "    \n",
    "    # Get all valid video paths\n",
    "    video_paths = []\n",
    "    for idx, path in enumerate(result_df[file_path_col]):\n",
    "        if isinstance(path, str) and os.path.exists(path):\n",
    "            video_paths.append((path, desired_fps))\n",
    "    \n",
    "    total_videos = len(video_paths)\n",
    "    print(f\"Found {total_videos} valid videos to process\")\n",
    "    \n",
    "    # Process in batches to manage memory\n",
    "    results = []\n",
    "    for i in range(0, len(video_paths), batch_size):\n",
    "        batch = video_paths[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(total_videos-1)//batch_size + 1} ({len(batch)} videos)\")\n",
    "        \n",
    "        # Process batch in parallel\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(resample_single_video, video_info) for video_info in batch]\n",
    "            \n",
    "            # Collect results with progress bar\n",
    "            for future in tqdm(as_completed(futures), total=len(batch), desc=\"Resampling\"):\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update dataframe as results come in\n",
    "                if result[\"status\"] == \"success\":\n",
    "                    # Find the corresponding row\n",
    "                    row_idx = result_df[result_df[file_path_col] == result[\"path\"]].index\n",
    "                    if len(row_idx) > 0:\n",
    "                        idx = row_idx[0]\n",
    "                        # Update file path\n",
    "                        result_df.at[idx, file_path_col] = result[\"new_path\"]\n",
    "                        \n",
    "                        # Update metadata if columns exist\n",
    "                        if has_frame_count_col:\n",
    "                            result_df.at[idx, 'Frame Count'] = result[\"frame_count\"]\n",
    "                        if has_fps_col:\n",
    "                            result_df.at[idx, 'FPS'] = result[\"fps\"]\n",
    "                        \n",
    "                        # Delete original if requested\n",
    "                        if delete_originals:\n",
    "                            try:\n",
    "                                os.remove(result[\"path\"])\n",
    "                            except Exception as e:\n",
    "                                print(f\"Warning: Could not delete {result['path']}: {e}\")\n",
    "        if save_checkpoint:\n",
    "            result_df.to_csv(checkpoint_file, index=False)\n",
    "            print(f\"Saved checkpoint to {checkpoint_file}\")\n",
    "    \n",
    "    # Summarize results\n",
    "    successful = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "    failed = sum(1 for r in results if r[\"status\"] == \"error\")\n",
    "    \n",
    "    print(f\"\\nResampling complete:\")\n",
    "    print(f\"  - Successfully processed: {successful}/{total_videos} videos\")\n",
    "    print(f\"  - Failed: {failed}/{total_videos} videos\")\n",
    "    \n",
    "    if failed > 0:\n",
    "        print(\"\\nFailed videos:\")\n",
    "        for r in results:\n",
    "            if r[\"status\"] == \"error\":\n",
    "                print(f\"  - {os.path.basename(r['path'])}: {r.get('message', 'Unknown error')}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video Name</th>\n",
       "      <th>Frame Count</th>\n",
       "      <th>FPS</th>\n",
       "      <th>dom_hand</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>836681859074-DISAGREEMENT</td>\n",
       "      <td>78</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>./836681859074-DISAGREEMENT.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>824925993024-NEAR</td>\n",
       "      <td>48</td>\n",
       "      <td>29.925187</td>\n",
       "      <td>None</td>\n",
       "      <td>./824925993024-NEAR.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0636580316216-SHUT OUT</td>\n",
       "      <td>62</td>\n",
       "      <td>29.799000</td>\n",
       "      <td>None</td>\n",
       "      <td>./0636580316216-SHUT OUT.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>701606421745-PERCEIVE</td>\n",
       "      <td>88</td>\n",
       "      <td>29.921795</td>\n",
       "      <td>None</td>\n",
       "      <td>./701606421745-PERCEIVE.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.7299129501965353e-7-seedSOUR</td>\n",
       "      <td>98</td>\n",
       "      <td>30.406000</td>\n",
       "      <td>None</td>\n",
       "      <td>./4.7299129501965353e-7-seedSOUR.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Video Name  Frame Count        FPS dom_hand  \\\n",
       "0       836681859074-DISAGREEMENT           78  30.000000     None   \n",
       "1               824925993024-NEAR           48  29.925187     None   \n",
       "2          0636580316216-SHUT OUT           62  29.799000     None   \n",
       "3           701606421745-PERCEIVE           88  29.921795     None   \n",
       "4  4.7299129501965353e-7-seedSOUR           98  30.406000     None   \n",
       "\n",
       "                              file_path  \n",
       "0       ./836681859074-DISAGREEMENT.mp4  \n",
       "1               ./824925993024-NEAR.mp4  \n",
       "2          ./0636580316216-SHUT OUT.mp4  \n",
       "3           ./701606421745-PERCEIVE.mp4  \n",
       "4  ./4.7299129501965353e-7-seedSOUR.mp4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 valid videos to process\n",
      "Processing batch 1/1 (5 videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling: 100%|██████████| 5/5 [00:01<00:00,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to resampling_progress.csv\n",
      "\n",
      "Resampling complete:\n",
      "  - Successfully processed: 5/5 videos\n",
      "  - Failed: 0/5 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_vid = resample_videos_in_dataframe(df=videos_df, desired_fps=12, save_checkpoint=True, checkpoint_file='resampling_progress.csv', file_path_col='file_path', \n",
    "                                delete_originals=True, inplace=False, \n",
    "                                max_workers=None, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video Name</th>\n",
       "      <th>Frame Count</th>\n",
       "      <th>FPS</th>\n",
       "      <th>dom_hand</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>836681859074-DISAGREEMENT</td>\n",
       "      <td>31</td>\n",
       "      <td>12.0</td>\n",
       "      <td>None</td>\n",
       "      <td>./836681859074-DISAGREEMENT_fps12.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>824925993024-NEAR</td>\n",
       "      <td>19</td>\n",
       "      <td>12.0</td>\n",
       "      <td>None</td>\n",
       "      <td>./824925993024-NEAR_fps12.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0636580316216-SHUT OUT</td>\n",
       "      <td>24</td>\n",
       "      <td>12.0</td>\n",
       "      <td>None</td>\n",
       "      <td>./0636580316216-SHUT OUT_fps12.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>701606421745-PERCEIVE</td>\n",
       "      <td>35</td>\n",
       "      <td>12.0</td>\n",
       "      <td>None</td>\n",
       "      <td>./701606421745-PERCEIVE_fps12.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.7299129501965353e-7-seedSOUR</td>\n",
       "      <td>38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>None</td>\n",
       "      <td>./4.7299129501965353e-7-seedSOUR_fps12.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Video Name  Frame Count   FPS dom_hand  \\\n",
       "0       836681859074-DISAGREEMENT           31  12.0     None   \n",
       "1               824925993024-NEAR           19  12.0     None   \n",
       "2          0636580316216-SHUT OUT           24  12.0     None   \n",
       "3           701606421745-PERCEIVE           35  12.0     None   \n",
       "4  4.7299129501965353e-7-seedSOUR           38  12.0     None   \n",
       "\n",
       "                                    file_path  \n",
       "0       ./836681859074-DISAGREEMENT_fps12.mp4  \n",
       "1               ./824925993024-NEAR_fps12.mp4  \n",
       "2          ./0636580316216-SHUT OUT_fps12.mp4  \n",
       "3           ./701606421745-PERCEIVE_fps12.mp4  \n",
       "4  ./4.7299129501965353e-7-seedSOUR_fps12.mp4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
