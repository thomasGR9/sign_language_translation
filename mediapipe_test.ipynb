{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"test_Im.png\"\n",
    "grimace_2_path = \"grimace_2.png\"\n",
    "grimace_3_path = \"grimace_3.png\"\n",
    "not_a_face_path = \"not_a_face.png\"\n",
    "without_face_path = \"without_face.png\"\n",
    "without_right_hand_path = \"without_right_hand.png\"\n",
    "without_left_hand_path = \"without_left_hand.png\"\n",
    "hand_model_path = \"hand_landmarker.task\"\n",
    "face_model_path = \"face_landmarker.task\"\n",
    "video_path = \"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1, 2, 3, 4, 5, 9, 10, 11, 12, 21, 22,23, 24, 25, 26, 27, 33, 39, 42, 43, 44, 45, 46, 47, 50, 51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=all, 1=INFO, 2=WARNING, 3=ERROR\n",
    "logging.getLogger(\"mediapipe\").setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
    "os.environ['GLOG_minloglevel'] = '3'      # Suppress Google logging (used by MediaPipe)\n",
    "os.environ['MEDIAPIPE_DISABLE_GPU'] = '1'  # Optional: Disable GPU logging messages\n",
    "logging.getLogger(\"mediapipe\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"absl\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def detect(image_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, min_face_detection_confidence=0.5, min_face_presence_confidence=0.5, num_hands=2, dominand_hand='Right', visualize=False, output_face_blendshapes=True, adaptive_threshold=True, max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Detects hands and face in an image, extracts hand landmark coordinates and face blendshapes.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        min_hand_detection_confidence (float): Confidence threshold for hand detection (0.0-1.0)\n",
    "        min_hand_presence_confidence (float): Confidence threshold for hand presence (0.0-1.0)\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        visualize (bool): Whether to visualize the results\n",
    "        output_face_blendshapes (bool): Whether to detect and extract face blendshapes\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (dom_landmarks, non_dom_landmarks, wrists, confidence_scores, detection_status, \n",
    "                blendshape_scores, face_landmark_5, face_detected)\n",
    "               - dom_landmarks: NumPy array of shape [20, 3] with coordinates of dominant hand landmarks\n",
    "               - non_dom_landmarks: NumPy array of shape [20, 3] with coordinates of non-dominant hand landmarks\n",
    "               - wrists: NumPy array of shape [2, 2] with coordinates of both wrists [x, y]\n",
    "               - confidence_scores: NumPy array of shape [2] with confidence scores [dominant_hand, non_dominant_hand]\n",
    "               - detection_status: NumPy array of shape [2] with binary detection status [dominant_hand, non_dominant_hand]\n",
    "               - blendshape_scores: NumPy array of shape [26] with selected face blendshape scores\n",
    "               - face_landmark_5: NumPy array of shape [2] with coordinates of the 5th face landmark [x, y]\n",
    "               - face_detected: Binary value (1 if face detected, 0 if not)\n",
    "    \"\"\"\n",
    "    # Initialize output arrays for face detection\n",
    "    blendshape_scores = np.zeros(52)\n",
    "    nose_landmark = np.zeros(2)\n",
    "    left_eye_landmark = np.zeros(2)\n",
    "    right_eye_landmark = np.zeros(2)\n",
    "    face_detected = 0\n",
    "    \n",
    "    # PART 1: HAND LANDMARK DETECTION\n",
    "    # 1.1: Configure the hand landmarker\n",
    "    hand_base_options = python.BaseOptions(\n",
    "        model_asset_path=hand_model_path\n",
    "    )\n",
    "\n",
    "    VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "    # Configure detection options\n",
    "    hand_options = vision.HandLandmarkerOptions(\n",
    "        base_options=hand_base_options,\n",
    "        num_hands=num_hands,                             \n",
    "        min_hand_detection_confidence=min_hand_detection_confidence,       \n",
    "        min_hand_presence_confidence=min_hand_presence_confidence,        \n",
    "        min_tracking_confidence=0.5,             \n",
    "        running_mode=VisionRunningMode.IMAGE\n",
    "    )\n",
    "\n",
    "    # Create the hand detector\n",
    "    hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "\n",
    "    # 1.2: Load the input image\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "\n",
    "    # 1.3: Detect hand landmarks\n",
    "    hand_detection_result = hand_detector.detect(image)\n",
    "    \n",
    "    # Initialize hand output arrays with zeros\n",
    "    dom_landmarks = np.zeros((20, 3))       # 20 landmarks (excluding wrist), [x,y,z]\n",
    "    non_dom_landmarks = np.zeros((20, 3))   # 20 landmarks (excluding wrist), [x,y,z]\n",
    "    wrists = np.zeros((2, 2))               # 2 wrists, [x,y]\n",
    "    confidence_scores = np.zeros(2)         # Confidence scores for [dominant, non-dominant]\n",
    "    interpolation_scores = np.zeros(2) #Interpolation scores for [dominant, non-dominant]. Used later.\n",
    "    detection_status = np.zeros(2, dtype=np.int32)  # Binary detection status [dominant, non-dominant]\n",
    "    nose_to_wrist_dist = np.zeros((2, 2))\n",
    "    \n",
    "    # 1.4: Process hand landmarks if hands are detected\n",
    "    if hand_detection_result.hand_landmarks and hand_detection_result.handedness:\n",
    "        dom_hand_found = False\n",
    "        non_dom_hand_found = False\n",
    "        \n",
    "        # First, find the dominant and non-dominant hands in detection results\n",
    "        for idx, handedness in enumerate(hand_detection_result.handedness):\n",
    "            hand_type = handedness[0].category_name  # 'Left' or 'Right'\n",
    "            hand_score = handedness[0].score  # Confidence score for the handedness classification\n",
    "            \n",
    "            if hand_type == dominand_hand:\n",
    "                # This is the dominant hand\n",
    "                dom_hand_found = True\n",
    "                detection_status[0] = 1  # Set detection status to 1 (detected)\n",
    "                confidence_scores[0] = hand_score  # Store confidence score\n",
    "                interpolation_scores[0] = 1\n",
    "                \n",
    "                # Store dominant hand wrist coordinates [x,y]\n",
    "                dom_hand_landmarks = hand_detection_result.hand_landmarks[idx]\n",
    "                wrists[0, 0] = dom_hand_landmarks[0].x\n",
    "                wrists[0, 1] = dom_hand_landmarks[0].y\n",
    "                \n",
    "                # Store all other dominant hand landmarks (excluding wrist)\n",
    "                for i in range(1, 21):  # Landmarks 1-20 (skipping wrist which is index 0)\n",
    "                    dom_landmarks[i-1, 0] = dom_hand_landmarks[i].x\n",
    "                    dom_landmarks[i-1, 1] = dom_hand_landmarks[i].y\n",
    "                    dom_landmarks[i-1, 2] = dom_hand_landmarks[i].z\n",
    "                    \n",
    "            elif hand_type != dominand_hand:\n",
    "                # This is the non-dominant hand\n",
    "                non_dom_hand_found = True\n",
    "                detection_status[1] = 1  # Set detection status to 1 (detected)\n",
    "                confidence_scores[1] = hand_score  # Store confidence score\n",
    "                interpolation_scores[1] = 1\n",
    "                \n",
    "                # Store non-dominant hand wrist coordinates [x,y]\n",
    "                non_dom_hand_landmarks = hand_detection_result.hand_landmarks[idx]\n",
    "                wrists[1, 0] = non_dom_hand_landmarks[0].x\n",
    "                wrists[1, 1] = non_dom_hand_landmarks[0].y\n",
    "                \n",
    "                # Store all other non-dominant hand landmarks (excluding wrist)\n",
    "                for i in range(1, 21):  # Landmarks 1-20 (skipping wrist)\n",
    "                    non_dom_landmarks[i-1, 0] = non_dom_hand_landmarks[i].x\n",
    "                    non_dom_landmarks[i-1, 1] = non_dom_hand_landmarks[i].y\n",
    "                    non_dom_landmarks[i-1, 2] = non_dom_hand_landmarks[i].z\n",
    "                    \n",
    "        # Log information about which hands were found\n",
    "        print(f\"Dominant hand ({dominand_hand}) detected: {dom_hand_found}\")\n",
    "        print(f\"Non-dominant hand detected: {non_dom_hand_found}\")\n",
    "    \n",
    "\n",
    "   # PART 2: FACE LANDMARK DETECTION (If requested)\n",
    "    if output_face_blendshapes:\n",
    "        try:\n",
    "            # 2.1: Configure the face landmarker\n",
    "            face_base_options = python.BaseOptions(\n",
    "                model_asset_path=face_model_path\n",
    "            )\n",
    "            \n",
    "            # Configure face detection options\n",
    "            face_options = vision.FaceLandmarkerOptions(\n",
    "                base_options=face_base_options,\n",
    "                min_face_detection_confidence=min_face_detection_confidence,\n",
    "                min_face_presence_confidence=min_face_presence_confidence,\n",
    "                output_face_blendshapes=True,\n",
    "                num_faces=1,\n",
    "                running_mode=VisionRunningMode.IMAGE\n",
    "            )\n",
    "            \n",
    "            # Create the face detector\n",
    "            face_detector = vision.FaceLandmarker.create_from_options(face_options)\n",
    "            \n",
    "            # 2.2: Detect face landmarks (reuse the same image)\n",
    "            face_detection_result = face_detector.detect(image)\n",
    "            \n",
    "            # 2.3: Process face blendshapes if face is detected\n",
    "            if (face_detection_result.face_blendshapes and len(face_detection_result.face_blendshapes) > 0 and\n",
    "                face_detection_result.face_landmarks and len(face_detection_result.face_landmarks) > 0):\n",
    "                \n",
    "                # Set face detected flag to 1\n",
    "                face_detected = 1\n",
    "                \n",
    "                # Get all blendshapes from the first face\n",
    "                all_blendshapes = face_detection_result.face_blendshapes[0]\n",
    "                \n",
    "                # Initialize blendshape_scores with the correct size to hold all blendshapes\n",
    "                # Assuming MediaPipe returns all 52 blendshapes\n",
    "                blendshape_scores = np.zeros(len(all_blendshapes))\n",
    "                \n",
    "                # Fill the blendshape_scores array with ALL scores\n",
    "                for i in range(len(all_blendshapes)):\n",
    "                    blendshape_scores[i] = all_blendshapes[i].score\n",
    "                \n",
    "                # Get nose coordinates\n",
    "                nose = face_detection_result.face_landmarks[0][4]\n",
    "                nose_landmark[0] = nose.x\n",
    "                nose_landmark[1] = nose.y\n",
    "    \n",
    "                # Get eye coordinates\n",
    "                left_eye = face_detection_result.face_landmarks[0][473]\n",
    "                left_eye_landmark[0] = left_eye.x\n",
    "                left_eye_landmark[1] = left_eye.y\n",
    "    \n",
    "                right_eye = face_detection_result.face_landmarks[0][468]\n",
    "                right_eye_landmark[0] = right_eye.x\n",
    "                right_eye_landmark[1] = right_eye.y\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during face detection: {e}\")\n",
    "            # Keep default zero values for face outputs if detection fails\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PART 3: VISUALIZATION\n",
    "    if visualize:\n",
    "        # Load the image with OpenCV for visualization\n",
    "        img_cv = cv2.imread(image_path)\n",
    "        img_height, img_width, _ = img_cv.shape\n",
    "\n",
    "        # 3.1: Draw hand landmarks if hands are detected\n",
    "        if hand_detection_result.hand_landmarks:\n",
    "            print(f\"Visualizing {len(hand_detection_result.hand_landmarks)} hands\")\n",
    "            \n",
    "            # Define connections between landmarks for hand skeleton\n",
    "            connections = [\n",
    "                # Thumb connections\n",
    "                (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "                # Index finger connections\n",
    "                (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "                # Middle finger connections\n",
    "                (0, 9), (9, 10), (10, 11), (11, 12),\n",
    "                # Ring finger connections\n",
    "                (0, 13), (13, 14), (14, 15), (15, 16),\n",
    "                # Pinky finger connections\n",
    "                (0, 17), (17, 18), (18, 19), (19, 20),\n",
    "                # Palm connections\n",
    "                (0, 5), (5, 9), (9, 13), (13, 17)\n",
    "            ]\n",
    "            \n",
    "            for idx, hand_landmarks in enumerate(hand_detection_result.hand_landmarks):\n",
    "                # Determine if this is the dominant hand\n",
    "                is_dominant = False\n",
    "                if hand_detection_result.handedness:\n",
    "                    hand_type = hand_detection_result.handedness[idx][0].category_name\n",
    "                    is_dominant = (hand_type == dominand_hand)\n",
    "                \n",
    "                # Use different colors for dominant vs non-dominant hand\n",
    "                hand_color = (0, 0, 255) if is_dominant else (255, 0, 0)  # Blue for dominant, Red for non-dominant\n",
    "                \n",
    "                # Draw all landmark points\n",
    "                for landmark in hand_landmarks:\n",
    "                    # Convert normalized coordinates to pixel coordinates\n",
    "                    x = int(landmark.x * img_width)\n",
    "                    y = int(landmark.y * img_height)\n",
    "                    \n",
    "                    # Draw the landmark point\n",
    "                    cv2.circle(img_cv, (x, y), 5, hand_color, -1)\n",
    "                \n",
    "                # Draw connections between landmarks (hand skeleton)\n",
    "                for connection in connections:\n",
    "                    start_idx, end_idx = connection\n",
    "                    \n",
    "                    if start_idx < len(hand_landmarks) and end_idx < len(hand_landmarks):\n",
    "                        start_point = hand_landmarks[start_idx]\n",
    "                        end_point = hand_landmarks[end_idx]\n",
    "                        \n",
    "                        # Convert normalized coordinates to pixel coordinates\n",
    "                        start_x = int(start_point.x * img_width)\n",
    "                        start_y = int(start_point.y * img_height)\n",
    "                        end_x = int(end_point.x * img_width)\n",
    "                        end_y = int(end_point.y * img_height)\n",
    "                        \n",
    "                        # Draw the connection line\n",
    "                        cv2.line(img_cv, (start_x, start_y), (end_x, end_y), hand_color, 2)\n",
    "                \n",
    "                # Add hand type label (Left/Right, Dominant/Non-dominant)\n",
    "                if hand_detection_result.handedness:\n",
    "                    handedness = hand_detection_result.handedness[idx]\n",
    "                    hand_type = handedness[0].category_name  # 'Left' or 'Right'\n",
    "                    hand_score = handedness[0].score\n",
    "                    dom_status = \"Dominant\" if hand_type == dominand_hand else \"Non-dominant\"\n",
    "                    cv2.putText(img_cv, f\"{hand_type} Hand - {dom_status} ({hand_score:.2f})\", \n",
    "                            (10, 30 + idx * 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                            0.8, hand_color, 2)\n",
    "                    \n",
    "                    # Calculate and draw a bounding box\n",
    "                    x_coords = [landmark.x for landmark in hand_landmarks]\n",
    "                    y_coords = [landmark.y for landmark in hand_landmarks]\n",
    "                    min_x, max_x = min(x_coords), max(x_coords)\n",
    "                    min_y, max_y = min(y_coords), max(y_coords)\n",
    "                    \n",
    "                    # Convert to pixel coordinates\n",
    "                    min_x, max_x = int(min_x * img_width), int(max_x * img_width)\n",
    "                    min_y, max_y = int(min_y * img_height), int(max_y * img_height)\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(img_cv, (min_x, min_y), (max_x, max_y), hand_color, 2)\n",
    "\n",
    "        # 3.2: Draw Nose if face was detected\n",
    "        if face_detected == 1:\n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            face_x = int(nose_landmark[0] * img_width)\n",
    "            face_y = int(nose_landmark[1] * img_height)\n",
    "            \n",
    "            # Draw the Nose with a distinctive color and size\n",
    "            cv2.circle(img_cv, (face_x, face_y), 8, (0, 255, 255), -1)  # Yellow circle\n",
    "            cv2.putText(img_cv, \"Nose\", (face_x + 10, face_y), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "            # Draw eyes\n",
    "            left_eye_x = int(left_eye_landmark[0] * img_width)\n",
    "            left_eye_y = int(left_eye_landmark[1] * img_height)\n",
    "            right_eye_x = int(right_eye_landmark[0] * img_width)\n",
    "            right_eye_y = int(right_eye_landmark[1] * img_height)\n",
    "            \n",
    "            cv2.circle(img_cv, (left_eye_x, left_eye_y), 6, (255, 255, 0), -1)  # Cyan circle\n",
    "            cv2.circle(img_cv, (right_eye_x, right_eye_y), 6, (255, 255, 0), -1)  # Cyan circle\n",
    "            cv2.line(img_cv, (left_eye_x, left_eye_y), (right_eye_x, right_eye_y), (255, 255, 0), 2)\n",
    "        # 3.3: Add detection status information to visualization\n",
    "        y_pos = img_height - 80\n",
    "        hand_status_text = f\"Hand Detection: Dom={detection_status[0]}, Non-Dom={detection_status[1]}\"\n",
    "        hand_conf_text = f\"Hand Confidence: Dom={confidence_scores[0]:.2f}, Non-Dom={confidence_scores[1]:.2f}\"\n",
    "        face_status_text = f\"Face Detection: {face_detected}\"\n",
    "        \n",
    "        cv2.putText(img_cv, hand_status_text, (10, y_pos), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img_cv, hand_conf_text, (10, y_pos + 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(img_cv, face_status_text, (10, y_pos + 60), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # 3.4: Display the result\n",
    "        cv2.imshow('Hand and Face Landmarks', img_cv)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    if face_detected==1:\n",
    "        #Calculate distance between the eyes\n",
    "        eyes_diff = right_eye_landmark-left_eye_landmark\n",
    "        eyes_distance = np.sqrt(eyes_diff.dot(eyes_diff))\n",
    "        if detection_status[0]==1 and detection_status[1]==1:\n",
    "            nose_to_wrist_dist = (wrists-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / eyes_distance\n",
    "        elif detection_status[0]==1 and detection_status[1]==0:\n",
    "            nose_to_wrist_dist[0, :] = (wrists[0, :]-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "        elif detection_status[0]==0 and detection_status[1]==1:\n",
    "            nose_to_wrist_dist[1,:] = (wrists[1,:]-nose_landmark) / eyes_distance\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the eye's distance\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[0, :]) / eyes_distance\n",
    "        \n",
    "    elif face_detected==0 and detection_status[0]==1:\n",
    "        #Calculate palm width distance as fallback scaling factor\n",
    "        palm_width_diff = dom_landmarks[5, :]- dom_landmarks[17, :]\n",
    "        palm_width_dist = np.sqrt(palm_width_diff.dot(palm_width_diff))\n",
    "        if detection_status[1]==1:\n",
    "            nose_to_wrist_dist = (wrists-nose_landmark) / palm_width_dist\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / palm_width_dist\n",
    "            non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / palm_width_dist\n",
    "        elif detection_status[1]==0:\n",
    "            nose_to_wrist_dist[0,:] = (wrists[0,:]-nose_landmark) / palm_width_dist\n",
    "            #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "            dom_landmarks[:, 0:2] = (dom_landmarks[:, 0:2] - wrists[0, :]) / palm_width_dist\n",
    "    elif face_detected==0 and detection_status[0]==0 and detection_status[1]==1:\n",
    "        #Calculate palm width distance as fallback scaling factor\n",
    "        palm_width_diff = non_dom_landmarks[5, :]- non_dom_landmarks[17, :]\n",
    "        palm_width_dist = np.sqrt(palm_width_diff.dot(palm_width_diff))\n",
    "        nose_to_wrist_dist[1,:] = (wrists[1,:]-nose_landmark) / palm_width_dist\n",
    "        #Make every hand's landmark potision relative to the wrist, and scaled by the palm width \n",
    "        non_dom_landmarks[:, 0:2] = (non_dom_landmarks[:, 0:2] - wrists[1, :]) / palm_width_dist\n",
    "    \n",
    "\n",
    "    \n",
    "    # Return all requested outputs\n",
    "    return dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824825.617509    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824825.620415  282728 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824825.856196  282729 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824825.881208  282730 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824825.948322    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824825.952441  282744 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824825.953249    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824825.961192  282746 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824825.988733  282749 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: True\n",
      "Non-dominant hand detected: True\n",
      "Visualizing 2 hands\n"
     ]
    }
   ],
   "source": [
    "lol = detect(grimace_2_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, num_hands=2, dominand_hand='Left', visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lol[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.22157899, -0.47131054, -0.03073101],\n",
       "        [-0.17301513, -0.94633667, -0.04778747],\n",
       "        [ 0.09016798, -1.17645313, -0.06066544],\n",
       "        [ 0.44656346, -1.2285489 , -0.07052706],\n",
       "        [ 0.75239831, -1.00154878, -0.03440262],\n",
       "        [ 0.74546371, -0.32252427, -0.05854694],\n",
       "        [ 0.52926255,  0.01389155, -0.07399104],\n",
       "        [ 0.3284734 ,  0.18489803, -0.08165359],\n",
       "        [ 0.96142811, -0.83551715, -0.02785412],\n",
       "        [ 0.733497  , -0.12437514, -0.05066952],\n",
       "        [ 0.38909197,  0.19143662, -0.05764463],\n",
       "        [ 0.1054417 ,  0.3583336 , -0.05927849],\n",
       "        [ 1.04701376, -0.64099421, -0.02452194],\n",
       "        [ 0.80774103, -0.04213003, -0.04881671],\n",
       "        [ 0.46820677,  0.20524576, -0.04839684],\n",
       "        [ 0.19587368,  0.31181122, -0.04229698],\n",
       "        [ 1.07308434, -0.42637636, -0.02360797],\n",
       "        [ 1.01975656, -0.01941465, -0.04110365],\n",
       "        [ 0.78549988,  0.19136538, -0.03911975],\n",
       "        [ 0.56846131,  0.24917491, -0.03242829]]),\n",
       " array([[ 0.43443525, -0.28010929, -0.01027886],\n",
       "        [ 0.80353707, -0.54089751, -0.0247014 ],\n",
       "        [ 0.97556812, -0.65599555, -0.03816538],\n",
       "        [ 1.01942988, -0.56354025, -0.05229027],\n",
       "        [ 0.86045716, -0.13682166, -0.03400054],\n",
       "        [ 1.31591499,  0.24062976, -0.04829271],\n",
       "        [ 1.60218625,  0.43870974, -0.05713674],\n",
       "        [ 1.81017121,  0.57159162, -0.06226961],\n",
       "        [ 0.61163023,  0.19540783, -0.03670957],\n",
       "        [ 1.09116728,  0.50590183, -0.04773441],\n",
       "        [ 1.4089959 ,  0.66185109, -0.05110947],\n",
       "        [ 1.64211591,  0.76527366, -0.05398791],\n",
       "        [ 0.37204199,  0.52394618, -0.03922421],\n",
       "        [ 0.87258241,  0.80436906, -0.0487699 ],\n",
       "        [ 1.172205  ,  0.94312534, -0.04679542],\n",
       "        [ 1.3891072 ,  1.00824313, -0.04479555],\n",
       "        [ 0.14446114,  0.84810335, -0.04231404],\n",
       "        [ 0.57520175,  1.08297814, -0.05030103],\n",
       "        [ 0.84108896,  1.15069684, -0.04695994],\n",
       "        [ 1.05146618,  1.16495436, -0.04323559]]),\n",
       " array([0.84230059, 0.72651029]),\n",
       " array([1., 1.]),\n",
       " array([1, 1], dtype=int32),\n",
       " array([5.13106784e-07, 2.02846597e-03, 3.18804756e-03, 5.41452050e-01,\n",
       "        7.69849420e-02, 3.81750800e-02, 1.87551632e-05, 3.96643820e-08,\n",
       "        2.66600040e-07, 3.10787767e-01, 3.27007324e-01, 3.97320539e-01,\n",
       "        3.99153769e-01, 1.49953105e-02, 5.87362051e-01, 5.51258922e-01,\n",
       "        2.45020236e-03, 3.55305187e-02, 3.04265637e-02, 4.82355088e-01,\n",
       "        5.29462278e-01, 4.21624212e-03, 2.38157995e-03, 6.42919476e-05,\n",
       "        1.46195479e-03, 3.30839418e-02, 7.51869346e-04, 9.70096048e-03,\n",
       "        8.69681120e-01, 8.20557177e-01, 7.09664860e-09, 2.41063027e-08,\n",
       "        2.43527279e-03, 4.74228486e-02, 2.91958713e-04, 1.99188435e-04,\n",
       "        3.98596115e-02, 1.40069559e-01, 1.56948388e-01, 7.12577777e-04,\n",
       "        1.05726868e-01, 7.86331594e-01, 1.80265668e-03, 3.23777436e-04,\n",
       "        1.01934398e-04, 2.63166294e-04, 7.19477441e-07, 1.26338258e-04,\n",
       "        9.95448863e-06, 5.62861487e-06, 3.39345775e-07, 4.37951542e-08]),\n",
       " 1,\n",
       " array([[ 3.81955315,  4.40232786],\n",
       "        [-3.99970874,  4.21392517]]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_detect(image_path, hand_model_path, face_model_path, min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, \n",
    "                   min_face_detection_confidence=0.5, min_face_presence_confidence=0.5, \n",
    "                   num_hands=2, dominand_hand='Right', visualize=False, output_face_blendshapes=True,\n",
    "                   max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Adaptively detects hands and face by progressively lowering detection thresholds\n",
    "    for undetected body parts.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file\n",
    "        min_hand_detection_confidence (float): Initial confidence threshold for hand detection\n",
    "        min_hand_presence_confidence (float): Initial confidence threshold for hand presence\n",
    "        min_face_detection_confidence (float): Initial confidence threshold for face detection\n",
    "        min_face_presence_confidence (float): Initial confidence threshold for face presence\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        visualize (bool): Whether to visualize the final results\n",
    "        output_face_blendshapes (bool): Whether to detect and extract face blendshapes\n",
    "        max_attempts (int): Maximum number of detection attempts with lowered thresholds\n",
    "        threshold_reduction_factor (float): Factor to multiply thresholds by on each attempt (0-1)\n",
    "        min_threshold (float): Minimum threshold to prevent excessive lowering\n",
    "        \n",
    "    Returns:\n",
    "        Same output as the detect() function\n",
    "    \"\"\"\n",
    "    # Import the original detect function\n",
    "    #from your_module import detect  # Replace with actual module name\n",
    "    \n",
    "    # Store original thresholds\n",
    "    orig_hand_detection_conf = min_hand_detection_confidence\n",
    "    orig_hand_presence_conf = min_hand_presence_confidence\n",
    "    orig_face_detection_conf = min_face_detection_confidence\n",
    "    orig_face_presence_conf = min_face_presence_confidence\n",
    "    \n",
    "    # Initialize best results and detection status\n",
    "    best_results = None\n",
    "    best_detection_status = [0, 0]  # [dom_hand, non_dom_hand]\n",
    "    best_face_detected = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    # Try detection with progressively lower thresholds\n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"\\n--- Attempt {attempt+1}/{max_attempts} ---\")\n",
    "        \n",
    "        # Calculate current thresholds\n",
    "        if attempt > 0:\n",
    "            # Only lower thresholds for undetected parts\n",
    "            # For hands\n",
    "            if best_detection_status[0] == 0:  # Dominant hand not detected\n",
    "                hand_detection_conf_dom = max(orig_hand_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                hand_presence_conf_dom = max(orig_hand_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering dominant hand thresholds: {hand_detection_conf_dom:.3f}, {hand_presence_conf_dom:.3f}\")\n",
    "            else:\n",
    "                hand_detection_conf_dom = orig_hand_detection_conf\n",
    "                hand_presence_conf_dom = orig_hand_presence_conf\n",
    "                \n",
    "            if best_detection_status[1] == 0:  # Non-dominant hand not detected\n",
    "                hand_detection_conf_non_dom = max(orig_hand_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                hand_presence_conf_non_dom = max(orig_hand_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering non-dominant hand thresholds: {hand_detection_conf_non_dom:.3f}, {hand_presence_conf_non_dom:.3f}\")\n",
    "            else:\n",
    "                hand_detection_conf_non_dom = orig_hand_detection_conf\n",
    "                hand_presence_conf_non_dom = orig_hand_presence_conf\n",
    "            \n",
    "            # Use the minimum of the two calculated thresholds (MediaPipe doesn't support per-hand thresholds)\n",
    "            current_hand_detection_conf = min(hand_detection_conf_dom, hand_detection_conf_non_dom)\n",
    "            current_hand_presence_conf = min(hand_presence_conf_dom, hand_presence_conf_non_dom)\n",
    "            \n",
    "            # For face\n",
    "            if output_face_blendshapes and best_face_detected == 0:  # Face not detected\n",
    "                current_face_detection_conf = max(orig_face_detection_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                current_face_presence_conf = max(orig_face_presence_conf * (threshold_reduction_factor ** attempt), min_threshold)\n",
    "                print(f\"Lowering face thresholds: {current_face_detection_conf:.3f}, {current_face_presence_conf:.3f}\")\n",
    "            else:\n",
    "                current_face_detection_conf = orig_face_detection_conf\n",
    "                current_face_presence_conf = orig_face_presence_conf\n",
    "        else:\n",
    "            # Use original thresholds for first attempt\n",
    "            current_hand_detection_conf = orig_hand_detection_conf\n",
    "            current_hand_presence_conf = orig_hand_presence_conf\n",
    "            current_face_detection_conf = orig_face_detection_conf\n",
    "            current_face_presence_conf = orig_face_presence_conf\n",
    "            print(f\"Using original thresholds: hands={current_hand_detection_conf}, face={current_face_detection_conf}\")\n",
    "        \n",
    "        # Call detect with current thresholds (don't visualize intermediate attempts)\n",
    "        results = detect(image_path,  hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                        min_hand_detection_confidence=current_hand_detection_conf,\n",
    "                        min_hand_presence_confidence=current_hand_presence_conf,\n",
    "                        min_face_detection_confidence=current_face_detection_conf,\n",
    "                        min_face_presence_confidence=current_face_presence_conf,\n",
    "                        num_hands=num_hands,\n",
    "                        dominand_hand=dominand_hand,\n",
    "                        visualize=False,\n",
    "                        output_face_blendshapes=output_face_blendshapes)\n",
    "        \n",
    "        # Unpack results\n",
    "        dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
    "        \n",
    "        # Compare with best results so far\n",
    "        current_detection_count = detection_status[0] + detection_status[1] + face_detected\n",
    "        best_detection_count = best_detection_status[0] + best_detection_status[1] + best_face_detected\n",
    "        \n",
    "        if best_results is None or current_detection_count > best_detection_count:\n",
    "            best_results = results\n",
    "            best_detection_status = [detection_status[0], detection_status[1]]\n",
    "            best_face_detected = face_detected\n",
    "            \n",
    "            print(f\"New best detection: dominant hand={detection_status[0]}, \"\n",
    "                  f\"non-dominant hand={detection_status[1]}, face={face_detected}\")\n",
    "            \n",
    "            # If everything is detected, we can stop early\n",
    "            if detection_status[0] == 1 and detection_status[1] == 1 and (face_detected == 1 or not output_face_blendshapes):\n",
    "                print(\"All body parts detected. Stopping early.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"No improvement in detection. Continuing to next attempt.\")\n",
    "    \n",
    "    # Run final detection with visualization if requested\n",
    "    if visualize:\n",
    "        print(\"\\n--- Visualizing final results ---\")\n",
    "        # Call detect one more time with the parameters that gave best results, but with visualize=True\n",
    "        # For simplicity, we'll just use the best thresholds we found\n",
    "        # This is slightly inefficient (one extra detection) but keeps the code clean\n",
    "        \n",
    "        # Determine which thresholds gave the best results\n",
    "        if best_detection_status[0] == 0:  # If dominant hand not detected in best result\n",
    "            hand_detection_conf = min_threshold\n",
    "            hand_presence_conf = min_threshold\n",
    "        else:\n",
    "            hand_detection_conf = orig_hand_detection_conf\n",
    "            hand_presence_conf = orig_hand_presence_conf\n",
    "            \n",
    "        if output_face_blendshapes and best_face_detected == 0:  # If face not detected in best result\n",
    "            face_detection_conf = min_threshold\n",
    "            face_presence_conf = min_threshold\n",
    "        else:\n",
    "            face_detection_conf = orig_face_detection_conf\n",
    "            face_presence_conf = orig_face_presence_conf\n",
    "        \n",
    "        # Run final detection with visualization\n",
    "        final_results = detect(image_path, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                              min_hand_detection_confidence=hand_detection_conf,\n",
    "                              min_hand_presence_confidence=hand_presence_conf, \n",
    "                              min_face_detection_confidence=face_detection_conf,\n",
    "                              min_face_presence_confidence=face_presence_conf,\n",
    "                              num_hands=num_hands,\n",
    "                              dominand_hand=dominand_hand,\n",
    "                              visualize=True,\n",
    "                              output_face_blendshapes=output_face_blendshapes)\n",
    "        \n",
    "        # Use these results if they're better than our best so far\n",
    "        dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = final_results\n",
    "        current_detection_count = detection_status[0] + detection_status[1] + face_detected\n",
    "        best_detection_count = best_detection_status[0] + best_detection_status[1] + best_face_detected\n",
    "        \n",
    "        if current_detection_count > best_detection_count:\n",
    "            best_results = final_results\n",
    "    \n",
    "    # Print final detection summary\n",
    "    print(\"\\n=== Detection Summary ===\")\n",
    "    dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = best_results\n",
    "    print(f\"Dominant hand detected: {detection_status[0] == 1} (confidence: {confidence_scores[0]:.3f})\")\n",
    "    print(f\"Non-dominant hand detected: {detection_status[1] == 1} (confidence: {confidence_scores[1]:.3f})\")\n",
    "    if output_face_blendshapes:\n",
    "        print(f\"Face detected: {face_detected == 1}\")\n",
    "    print(f\"Total detection attempts: {attempt+1}\")\n",
    "    return best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=0, non-dominant hand=1, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824986.936870    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824986.940818  284930 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.039888  284934 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.056971  284940 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.133708    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.135590  284946 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.136250    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.142800  284947 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.160368  284950 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.199556    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.202131  284962 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.238721  284964 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.252189  284966 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering dominant hand thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824987.304140    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.307504  284978 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.308167    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.314875  284980 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.507001  284979 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.543464    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.547164  284994 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.568495  284996 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.686856  285000 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Visualizing final results ---\n",
      "Dominant hand (Left) detected: False\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742824987.738748    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.741043  285010 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.741756    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.750388  285011 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.779803  285022 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.819273    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.822941  285026 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.853940  285027 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.874366  285035 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742824987.927914    4103 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742824987.931627  285064 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742824987.932330    4103 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742824987.944897  285072 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742824987.965721  285078 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing 1 hands\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: False (confidence: 0.000)\n",
      "Non-dominant hand detected: True (confidence: 0.948)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n"
     ]
    }
   ],
   "source": [
    "best_results = adaptive_detect(without_left_hand_path, hand_model_path=hand_model_path, face_model_path=face_model_path,  min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5, num_hands=2, dominand_hand='Left', visualize=True,output_face_blendshapes=True,max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dom_landmarks, non_dom_landmarks, confidence_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " array([[ 4.4302598 ,  5.79258936, -0.01529764],\n",
       "        [ 4.97843194,  5.60716573, -0.03023903],\n",
       "        [ 5.4227688 ,  5.474167  , -0.04531926],\n",
       "        [ 5.6396736 ,  5.29133956, -0.06161126],\n",
       "        [ 5.46952226,  5.95203357, -0.02546122],\n",
       "        [ 6.14746077,  5.96317265, -0.03837759],\n",
       "        [ 6.5033938 ,  5.98504281, -0.04724246],\n",
       "        [ 6.77242064,  5.99687042, -0.05398038],\n",
       "        [ 5.45199885,  6.19957023, -0.02953801],\n",
       "        [ 6.20066676,  6.22105431, -0.03755009],\n",
       "        [ 6.60851154,  6.23570749, -0.04424638],\n",
       "        [ 6.9013028 ,  6.24974116, -0.05119639],\n",
       "        [ 5.33187311,  6.40705239, -0.03495561],\n",
       "        [ 6.02315108,  6.45053573, -0.04479358],\n",
       "        [ 6.42136374,  6.46895391, -0.05380662],\n",
       "        [ 6.71558972,  6.47136295, -0.0608963 ],\n",
       "        [ 5.14666546,  6.58524524, -0.04154579],\n",
       "        [ 5.66482483,  6.65936167, -0.04902381],\n",
       "        [ 5.98583324,  6.67587705, -0.05188949],\n",
       "        [ 6.25919546,  6.67568786, -0.0540517 ]]),\n",
       " array([0.       , 0.9484275]),\n",
       " array([0., 1.]),\n",
       " array([0, 1], dtype=int32),\n",
       " array([2.92115374e-07, 1.33861089e-02, 1.24196718e-02, 2.88109779e-01,\n",
       "        5.75697683e-02, 3.07354219e-02, 1.32326995e-05, 1.58878031e-08,\n",
       "        8.72999664e-08, 3.63642573e-01, 4.11365360e-01, 6.62605762e-01,\n",
       "        6.69030905e-01, 2.73421267e-03, 5.51178098e-01, 5.63342750e-01,\n",
       "        1.03895655e-02, 1.23140477e-02, 8.84756818e-03, 2.24526033e-01,\n",
       "        2.50775576e-01, 4.66695800e-03, 1.89312676e-03, 3.73961666e-05,\n",
       "        1.19879853e-03, 7.60920672e-03, 4.57036338e-04, 1.92889536e-03,\n",
       "        8.89772832e-01, 8.59930277e-01, 8.38296987e-10, 1.49412671e-09,\n",
       "        1.53606525e-02, 1.33757144e-02, 1.26740182e-04, 8.43084490e-05,\n",
       "        7.94648603e-02, 9.92558002e-02, 8.17362487e-01, 1.14337606e-02,\n",
       "        1.67473480e-02, 3.32397074e-01, 3.43104475e-03, 1.09493383e-03,\n",
       "        1.44784761e-04, 2.09721227e-04, 8.35517653e-07, 5.79597008e-06,\n",
       "        3.15646721e-05, 2.39465880e-05, 5.11401367e-07, 3.26468843e-08]),\n",
       " 1,\n",
       " array([[ 0.        ,  0.        ],\n",
       "        [-1.39355698,  4.85907125]]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_video(video_path, adaptive_detect_func, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                 min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5,\n",
    "                 min_face_detection_confidence=0.5, min_face_presence_confidence=0.5,\n",
    "                 num_hands=2, output_face_blendshapes=True,\n",
    "                 max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2, \n",
    "                 frame_step=1, start_time_seconds=0, end_time_seconds=None,\n",
    "                 save_failure_screenshots=False):\n",
    "    \"\"\"\n",
    "    Process a video frame-by-frame using the adaptive_detect function and save results.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file\n",
    "        adaptive_detect_func: The adaptive detection function to use\n",
    "        min_hand_detection_confidence (float): Initial confidence threshold for hand detection\n",
    "        min_hand_presence_confidence (float): Initial confidence threshold for hand presence\n",
    "        min_face_detection_confidence (float): Initial confidence threshold for face detection\n",
    "        min_face_presence_confidence (float): Initial confidence threshold for face presence\n",
    "        num_hands (int): Maximum number of hands to detect\n",
    "        dominand_hand (str): Dominant hand preference ('Left' or 'Right')\n",
    "        output_face_blendshapes (bool): Whether to detect face blendshapes\n",
    "        max_attempts (int): Maximum detection attempts for adaptive detection\n",
    "        threshold_reduction_factor (float): Factor to reduce thresholds by\n",
    "        min_threshold (float): Minimum threshold limit\n",
    "        frame_step (int): Process every Nth frame (1 = all frames)\n",
    "        start_time_seconds (float): Time in seconds to start processing from\n",
    "        end_time_seconds (float): Time in seconds to end processing (None = process until end)\n",
    "        save_failure_screenshots (bool): Save screenshots for all frames with any detection failures\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the directory containing saved frame results\n",
    "    \"\"\"\n",
    "    # Extract video name for directory creation\n",
    "    video_path = Path(video_path)\n",
    "    video_name = video_path.stem  # Get filename without extension\n",
    "    \n",
    "    # Extract dominant hand information from filename\n",
    "    if video_name.endswith(\"_Right\"):\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "    elif video_name.endswith(\"_Left\"):\n",
    "        extracted_dominant_hand = \"Left\"\n",
    "    else:\n",
    "        # Default if not specified in filename\n",
    "        extracted_dominant_hand = \"Right\"\n",
    "        print(f\"Warning: Could not determine dominant hand from filename, using default: {dominand_hand}\")\n",
    "\n",
    "    # Use the extracted dominant hand instead of the parameter\n",
    "    dominand_hand = extracted_dominant_hand\n",
    "    print(f\"Detected dominant hand from filename: {dominand_hand}\")\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(f\"{video_name}_landmarks\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create screenshots directory if screenshot option is enabled\n",
    "    screenshots_dir = None\n",
    "    if save_failure_screenshots:\n",
    "        screenshots_dir = output_dir / \"failure_screenshots\"\n",
    "        screenshots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create a log file to track processing\n",
    "    log_file = output_dir / \"processing_log.txt\"\n",
    "    \n",
    "    # Create a detailed statistics file\n",
    "    stats_file = output_dir / \"detection_statistics.json\"\n",
    "    \n",
    "    # Initialize statistics tracking\n",
    "    stats = {\n",
    "        \"video_info\": {\n",
    "            \"name\": video_name,\n",
    "            \"path\": str(video_path),\n",
    "            \"total_frames\": 0,\n",
    "            \"processed_frames\": 0,\n",
    "            \"fps\": 0,\n",
    "            \"duration_seconds\": 0,\n",
    "            \"start_time\": start_time_seconds,\n",
    "            \"end_time\": end_time_seconds,\n",
    "            \"dominant_hand\": dominand_hand,\n",
    "            \"processing_started\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"processing_completed\": None\n",
    "        },\n",
    "        \"detection_rates\": {\n",
    "            \"dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"non_dominant_hand\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"face\": {\n",
    "                \"detected\": 0,\n",
    "                \"failed\": 0,\n",
    "                \"detection_rate\": 0\n",
    "            },\n",
    "            \"overall\": {\n",
    "                \"all_detected\": 0,\n",
    "                \"partial_detections\": 0,\n",
    "                \"no_detections\": 0,\n",
    "                \"success_rate\": 0\n",
    "            }\n",
    "        },\n",
    "        \"failed_frames\": {\n",
    "            \"dominant_hand_failures\": [],\n",
    "            \"non_dominant_hand_failures\": [],\n",
    "            \"face_failures\": [],\n",
    "            \"all_failures\": []\n",
    "        },\n",
    "        \"processing_performance\": {\n",
    "            \"average_processing_time_ms\": 0,\n",
    "            \"total_processing_time_seconds\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(log_file, \"w\") as log:\n",
    "        log.write(f\"Processing video: {video_path}\\n\")\n",
    "        log.write(f\"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        log.write(f\"Parameters:\\n\")\n",
    "        log.write(f\"  - frame_step: {frame_step}\\n\")\n",
    "        log.write(f\"  - start_time: {start_time_seconds} seconds\\n\")\n",
    "        if end_time_seconds is not None:\n",
    "            log.write(f\"  - end_time: {end_time_seconds} seconds\\n\")\n",
    "        log.write(f\"  - dominand_hand: {dominand_hand}\\n\")\n",
    "        log.write(f\"  - num_hands: {num_hands}\\n\")\n",
    "        log.write(f\"  - detection confidence thresholds: {min_hand_detection_confidence}, {min_face_detection_confidence}\\n\")\n",
    "        log.write(\"\\n--- Frame processing log ---\\n\")\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration_seconds = total_frames / fps if fps > 0 else 0\n",
    "    \n",
    "    # Update stats with video info\n",
    "    stats[\"video_info\"][\"total_frames\"] = total_frames\n",
    "    stats[\"video_info\"][\"fps\"] = fps\n",
    "    stats[\"video_info\"][\"duration_seconds\"] = duration_seconds\n",
    "    \n",
    "    # Convert time to frame indices\n",
    "    start_frame = int(max(0, start_time_seconds * fps))\n",
    "    \n",
    "    # Set end frame if specified\n",
    "    if end_time_seconds is not None:\n",
    "        end_frame = min(total_frames, int(end_time_seconds * fps))\n",
    "    else:\n",
    "        end_frame = total_frames\n",
    "    \n",
    "    print(f\"Video: {video_name}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"FPS: {fps}\")\n",
    "    print(f\"Duration: {duration_seconds:.2f} seconds\")\n",
    "    print(f\"Processing frames {start_frame} to {end_frame} (time {start_time_seconds:.2f}s to {end_time_seconds if end_time_seconds is not None else duration_seconds:.2f}s)\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Process frames\n",
    "    frame_idx = 0\n",
    "    processed_count = 0\n",
    "    total_processing_time = 0\n",
    "    \n",
    "    # Skip to start_frame\n",
    "    if start_frame > 0:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        frame_idx = start_frame\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        while frame_idx < end_frame:\n",
    "            # Read the next frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "            \n",
    "            # Only process every frame_step frames\n",
    "            if (frame_idx - start_frame) % frame_step != 0:\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "                \n",
    "            # Get timestamp in milliseconds\n",
    "            timestamp_ms = int(frame_idx * 1000 / fps)\n",
    "            timestamp_formatted = f\"{timestamp_ms//60000:02d}m{(timestamp_ms//1000)%60:02d}s{timestamp_ms%1000:03d}ms\"\n",
    "            \n",
    "            # Temporary frame path\n",
    "            temp_frame_path = Path(temp_dir) / f\"temp_frame_{frame_idx}.jpg\"\n",
    "            \n",
    "            # Save the current frame as an image\n",
    "            cv2.imwrite(str(temp_frame_path), frame)\n",
    "            \n",
    "            # Process the frame with adaptive_detect\n",
    "            print(f\"Processing frame {frame_idx}/{total_frames} (timestamp: {timestamp_formatted})\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                # Use adaptive_detect on the frame\n",
    "                results = adaptive_detect_func(\n",
    "                    str(temp_frame_path), hand_model_path, face_model_path,\n",
    "                    min_hand_detection_confidence=min_hand_detection_confidence,\n",
    "                    min_hand_presence_confidence=min_hand_presence_confidence,\n",
    "                    min_face_detection_confidence=min_face_detection_confidence,\n",
    "                    min_face_presence_confidence=min_face_presence_confidence,\n",
    "                    num_hands=num_hands,\n",
    "                    dominand_hand=dominand_hand,\n",
    "                    visualize=False,\n",
    "                    output_face_blendshapes=output_face_blendshapes,\n",
    "                    max_attempts=max_attempts,\n",
    "                    threshold_reduction_factor=threshold_reduction_factor,\n",
    "                    min_threshold=min_threshold\n",
    "                )\n",
    "                \n",
    "                # Calculate processing time\n",
    "                proc_time = time.time() - start_time\n",
    "                total_processing_time += proc_time\n",
    "                \n",
    "                # Unpack results\n",
    "                dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist = results\n",
    "                \n",
    "                # Update detection statistics\n",
    "                dom_hand_detected = detection_status[0] == 1\n",
    "                non_dom_hand_detected = detection_status[1] == 1\n",
    "                face_was_detected = face_detected == 1\n",
    "                \n",
    "                if dom_hand_detected:\n",
    "                    stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"dominant_hand\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"dominant_hand_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                if non_dom_hand_detected:\n",
    "                    stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"non_dominant_hand\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"non_dominant_hand_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                if face_was_detected:\n",
    "                    stats[\"detection_rates\"][\"face\"][\"detected\"] += 1\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"face\"][\"failed\"] += 1\n",
    "                    stats[\"failed_frames\"][\"face_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                \n",
    "                # Track combined detection status\n",
    "                detection_count = dom_hand_detected + non_dom_hand_detected + face_was_detected\n",
    "                \n",
    "                if detection_count == 3:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"all_detected\"] += 1\n",
    "                elif detection_count == 0:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"no_detections\"] += 1\n",
    "                    stats[\"failed_frames\"][\"all_failures\"].append({\n",
    "                        \"frame\": frame_idx,\n",
    "                        \"timestamp_ms\": timestamp_ms,\n",
    "                        \"file\": f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                    })\n",
    "                else:\n",
    "                    stats[\"detection_rates\"][\"overall\"][\"partial_detections\"] += 1\n",
    "                \n",
    "                # Save screenshot if any detection failed and screenshots are enabled\n",
    "                if save_failure_screenshots and (not dom_hand_detected or not non_dom_hand_detected or not face_was_detected):\n",
    "                    # Create a detailed failure type description for the filename\n",
    "                    failure_type = []\n",
    "                    if not dom_hand_detected:\n",
    "                        failure_type.append(\"DomHand\")\n",
    "                    if not non_dom_hand_detected:\n",
    "                        failure_type.append(\"NonDomHand\")\n",
    "                    if not face_was_detected:\n",
    "                        failure_type.append(\"Face\")\n",
    "                    \n",
    "                    failure_str = \"_\".join(failure_type)\n",
    "                    screenshot_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}_missing_{failure_str}.jpg\"\n",
    "                    screenshot_path = screenshots_dir / screenshot_filename\n",
    "                    \n",
    "                    # Copy the frame to the screenshots directory\n",
    "                    cv2.imwrite(str(screenshot_path), frame)\n",
    "                    print(f\"Saved failure screenshot: {screenshot_filename}\")\n",
    "                \n",
    "                # Create output filename with frame info\n",
    "                output_filename = f\"{video_name}_frame{frame_idx:06d}_{timestamp_formatted}.npz\"\n",
    "                output_path = output_dir / output_filename\n",
    "                \n",
    "                # Save all results in a single .npz file\n",
    "                np.savez(\n",
    "                    output_path,\n",
    "                    dom_landmarks=dom_landmarks,\n",
    "                    non_dom_landmarks=non_dom_landmarks,\n",
    "                    confidence_scores=confidence_scores,\n",
    "                    interpolation_scores=interpolation_scores,\n",
    "                    detection_status=detection_status,\n",
    "                    blendshape_scores=blendshape_scores,\n",
    "                    face_detected=face_detected,\n",
    "                    nose_to_wrist_dist=nose_to_wrist_dist,\n",
    "                    frame_idx=np.array([frame_idx]),\n",
    "                    timestamp_ms=np.array([timestamp_ms])\n",
    "                )\n",
    "                \n",
    "                # Update processing log\n",
    "                detection_summary = f\"Dom: {detection_status[0]}, Non-dom: {detection_status[1]}, Face: {face_detected}\"\n",
    "                log_entry = f\"Frame {frame_idx}: {detection_summary} (proc time: {proc_time:.2f}s)\\n\"\n",
    "                \n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(log_entry)\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {frame_idx}: {e}\")\n",
    "                with open(log_file, \"a\") as log:\n",
    "                    log.write(f\"Error on frame {frame_idx}: {str(e)}\\n\")\n",
    "            \n",
    "            # Clean up temporary frame file\n",
    "            if temp_frame_path.exists():\n",
    "                temp_frame_path.unlink()\n",
    "                \n",
    "            frame_idx += 1\n",
    "    \n",
    "    # Close the video file\n",
    "    cap.release()\n",
    "    \n",
    "    # Update final statistics\n",
    "    stats[\"video_info\"][\"processed_frames\"] = processed_count\n",
    "    stats[\"video_info\"][\"processing_completed\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Calculate detection rates\n",
    "    if processed_count > 0:\n",
    "        stats[\"detection_rates\"][\"dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"non_dominant_hand\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"non_dominant_hand\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"face\"][\"detection_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"face\"][\"detected\"] / processed_count * 100\n",
    "        )\n",
    "        stats[\"detection_rates\"][\"overall\"][\"success_rate\"] = (\n",
    "            stats[\"detection_rates\"][\"overall\"][\"all_detected\"] / processed_count * 100\n",
    "        )\n",
    "    \n",
    "    # Calculate processing performance\n",
    "    if processed_count > 0:\n",
    "        stats[\"processing_performance\"][\"average_processing_time_ms\"] = (\n",
    "            total_processing_time / processed_count * 1000\n",
    "        )\n",
    "    stats[\"processing_performance\"][\"total_processing_time_seconds\"] = total_processing_time\n",
    "    \n",
    "    # Save statistics to JSON file\n",
    "    with open(stats_file, \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    # Add summary statistics to log file\n",
    "    with open(log_file, \"a\") as log:\n",
    "        log.write(f\"\\n\\n===== PROCESSING SUMMARY =====\\n\")\n",
    "        log.write(f\"Completed at: {stats['video_info']['processing_completed']}\\n\")\n",
    "        log.write(f\"Frames processed: {processed_count} from {start_frame} to {min(end_frame, frame_idx-1)}\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION RATES:\\n\")\n",
    "        log.write(f\"  Dominant hand ({dominand_hand}): {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Non-dominant hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\\n\")\n",
    "        log.write(f\"  All parts detected: {stats['detection_rates']['overall']['success_rate']:.1f}%\\n\\n\")\n",
    "        \n",
    "        log.write(\"DETECTION FAILURES:\\n\")\n",
    "        log.write(f\"  Frames with dominant hand failures: {len(stats['failed_frames']['dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with non-dominant hand failures: {len(stats['failed_frames']['non_dominant_hand_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with face failures: {len(stats['failed_frames']['face_failures'])}\\n\")\n",
    "        log.write(f\"  Frames with all parts missing: {len(stats['failed_frames']['all_failures'])}\\n\\n\")\n",
    "        \n",
    "        log.write(\"PERFORMANCE:\\n\")\n",
    "        log.write(f\"  Average processing time per frame: {stats['processing_performance']['average_processing_time_ms']:.2f} ms\\n\")\n",
    "        log.write(f\"  Total processing time: {stats['processing_performance']['total_processing_time_seconds']:.2f} seconds\\n\")\n",
    "    \n",
    "    print(f\"\\n===== PROCESSING SUMMARY =====\")\n",
    "    print(f\"Processed {processed_count} frames\")\n",
    "    print(f\"Detection rates: Dom hand: {stats['detection_rates']['dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "          f\"Non-dom hand: {stats['detection_rates']['non_dominant_hand']['detection_rate']:.1f}%, \" +\n",
    "          f\"Face: {stats['detection_rates']['face']['detection_rate']:.1f}%\")\n",
    "    print(f\"All parts detected in {stats['detection_rates']['overall']['success_rate']:.1f}% of frames\")\n",
    "    print(f\"Full statistics saved to: {stats_file}\")\n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "    \n",
    "    return str(output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks = data['dom_landmarks']\n",
    "    non_dom_landmarks = data['non_dom_landmarks']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores = data['blendshape_scores']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist = data['nose_to_wrist_dist']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    \n",
    "    return (dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores, face_detected, \n",
    "            nose_to_wrist_dist, frame_idx, timestamp_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected dominant hand from filename: Right\n",
      "Video: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right\n",
      "Total frames: 65\n",
      "FPS: 1.0\n",
      "Duration: 65.00 seconds\n",
      "Processing frames 30 to 60 (time 30.20s to 60.40s)\n",
      "Output directory: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\n",
      "Processing frame 30/65 (timestamp: 00m30s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991256.197975    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991256.206808  233077 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991256.259148  233086 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991256.352388  233085 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991256.422769    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991256.425440  233093 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991256.658781    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991256.673208  233098 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991256.701054  233100 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991256.755408    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991256.758383  233109 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991256.788850  233110 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991256.817769  233114 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.974)\n",
      "Non-dominant hand detected: True (confidence: 0.996)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 31/65 (timestamp: 00m31s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991256.926562    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991256.934042  233125 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991256.934801    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991256.939976  233130 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991256.969035  233126 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991257.023034    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991257.026314  233141 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991257.055791  233149 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991257.079317  233146 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.980)\n",
      "Non-dominant hand detected: True (confidence: 0.998)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 32/65 (timestamp: 00m32s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991257.205162    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991257.208943  233157 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991257.209687    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991257.217147  233161 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991257.252528  233159 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991257.306939    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991257.310097  233173 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991257.340285  233176 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991257.367206  233179 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991257.454786    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991257.457669  233189 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991257.458184    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991257.466373  233191 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991257.494678  233193 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.966)\n",
      "Non-dominant hand detected: True (confidence: 0.970)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 33/65 (timestamp: 00m33s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991257.572350    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991257.575448  233205 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991257.604560  233208 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991257.629347  233207 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991257.758184    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991257.761910  233247 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991257.762608    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991257.770188  233256 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991257.805761  233249 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991257.850817    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991257.853809  233263 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991257.888565  233267 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991257.917054  233273 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991258.027214    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.030939  233279 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.031789    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991258.036422  233288 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.051364  233285 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991258.079167    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.082628  233295 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.101465  233297 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.116196  233304 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering face thresholds: 0.245, 0.245\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991258.196391    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.199056  233311 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.199559    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991258.204370  233318 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.226842  233317 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991258.281927    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.284653  233327 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.306304  233332 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.321813  233330 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991258.384584    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.386715  233343 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.387088    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991258.392083  233347 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.419654  233354 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.997)\n",
      "Non-dominant hand detected: True (confidence: 0.997)\n",
      "Face detected: False\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000033_00m33s000ms_missing_Face.jpg\n",
      "Processing frame 34/65 (timestamp: 00m34s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: False\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=0, non-dominant hand=1, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991258.460462    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.464619  233359 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.490533  233360 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.505861  233366 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991258.599290    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.601307  233375 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.601772    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991258.606769  233379 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.623503  233384 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.949)\n",
      "Non-dominant hand detected: True (confidence: 0.993)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 35/65 (timestamp: 00m35s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991258.682114    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.685994  233391 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.704529  233395 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.720966  233393 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991258.801089    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.803100  233407 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.803622    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991258.809156  233409 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.826555  233419 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991258.880220    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991258.883054  233423 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991258.905892  233427 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991258.923778  233426 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.905)\n",
      "Non-dominant hand detected: True (confidence: 0.993)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 36/65 (timestamp: 00m36s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991259.006097    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.009861  233439 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.010675    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991259.016991  233442 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.033121  233449 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991259.065323    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.067800  233455 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.086932  233456 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.101391  233465 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991259.178000    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.181123  233471 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.181726    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991259.186758  233474 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.201301  233472 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991259.237338    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.241571  233487 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.275210  233489 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.292656  233495 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991259.403049    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.406005  233503 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.406459    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991259.411669  233507 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.436192  233511 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991259.489492    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.491881  233519 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.516852  233522 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.535043  233525 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991259.635132    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.968)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms_missing_NonDomHand.jpg\n",
      "Processing frame 37/65 (timestamp: 00m37s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991259.638934  233535 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.639674    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991259.644684  233536 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.658953  233542 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991259.681035    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.683584  233551 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.704756  233557 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.733536  233559 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991259.821189    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.824289  233580 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.824674    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991259.832308  233585 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.850596  233586 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.949)\n",
      "Non-dominant hand detected: True (confidence: 0.998)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 38/65 (timestamp: 00m38s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.953)\n",
      "Non-dominant hand detected: True (confidence: 0.997)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 39/65 (timestamp: 00m39s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991259.894180    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.898128  233608 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.919851  233618 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991259.935488  233609 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991259.991567    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991259.994848  233624 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991259.995862    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991260.005226  233628 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.020224  233636 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991260.077015    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.081323  233640 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.100318  233644 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.118302  233643 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991260.167127    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.169421  233656 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.169915    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991260.174352  233663 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.191764  233658 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991260.226937    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.230307  233672 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.253144  233673 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.268458  233684 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991260.326379    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.330323  233688 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.331201    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991260.336272  233695 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.351457  233697 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991260.388185    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.392241  233704 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.417308  233706 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.432721  233707 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991260.495511    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.498225  233720 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.498710    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991260.504007  233725 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.526579  233721 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991260.576324    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.579911  233736 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.599784  233737 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.618448  233742 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.993)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms_missing_NonDomHand.jpg\n",
      "Processing frame 40/65 (timestamp: 00m40s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991260.707553    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.709583  233752 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.710176    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991260.715471  233756 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.731189  233761 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991260.775879    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.779122  233768 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.799713  233772 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.812197  233769 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991260.900810    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.902872  233784 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.903272    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991260.908356  233785 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991260.935077  233791 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991260.974742    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991260.977604  233800 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991260.996399  233808 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991261.009370  233810 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991261.098864    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991261.100874  233816 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991261.101270    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991261.106811  233817 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991261.131989  233823 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991261.204889    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.928)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms_missing_NonDomHand.jpg\n",
      "Processing frame 41/65 (timestamp: 00m41s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991261.208910  233832 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991261.225916  233838 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991261.244697  233842 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991261.349763    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991261.353326  233848 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991261.353788    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991261.358950  233850 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991261.384528  233853 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.992)\n",
      "Non-dominant hand detected: True (confidence: 0.997)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 42/65 (timestamp: 00m42s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991261.448935    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991261.453235  233864 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991261.481275  233865 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991261.497120  233873 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991261.591866    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991261.594645  233880 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991261.595080    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991261.600137  233884 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991261.624138  233888 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991261.673204    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991261.675669  233896 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991261.693247  233897 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991261.709519  233898 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.991)\n",
      "Non-dominant hand detected: True (confidence: 0.990)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 43/65 (timestamp: 00m43s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991261.801044    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991261.804782  233912 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991261.805359    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991261.810587  233917 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991261.835279  233915 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991261.878988    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991261.883434  233928 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991261.904356  233930 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991261.919648  233936 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.990)\n",
      "Non-dominant hand detected: True (confidence: 0.992)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 44/65 (timestamp: 00m44s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991262.012828    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.015966  233971 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.016487    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991262.020516  233974 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.040949  233980 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991262.091845    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.095148  233987 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.117946  233988 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.133409  233995 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991262.205118    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.207545  234003 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.208060    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991262.215091  234004 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.228527  234011 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.988)\n",
      "Non-dominant hand detected: True (confidence: 0.988)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 45/65 (timestamp: 00m45s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.992)\n",
      "Non-dominant hand detected: True (confidence: 0.990)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 46/65 (timestamp: 00m46s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.988)\n",
      "Non-dominant hand detected: True (confidence: 0.985)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 47/65 (timestamp: 00m47s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991262.288557    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.292697  234019 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.316588  234026 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.332491  234024 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991262.396182    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.400229  234035 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.400599    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991262.406564  234037 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.424415  234038 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991262.495516    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.500669  234051 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.521225  234052 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.545262  234059 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering face thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991262.643057    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.647260  234067 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.648406    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991262.653535  234069 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.668881  234071 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991262.685244    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.687441  234083 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.709654  234090 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.725398  234092 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991262.790917    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.795055  234099 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.796175    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991262.805235  234103 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.822300  234108 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991262.862325    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.867069  234115 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.887450  234123 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991262.907438  234120 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991262.991417    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991262.994163  234131 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991262.994635    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991262.999646  234132 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.015659  234137 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991263.059402    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991263.062197  234147 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991263.098664  234149 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.115432  234152 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.978)\n",
      "Non-dominant hand detected: True (confidence: 0.975)\n",
      "Face detected: False\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000047_00m47s000ms_missing_Face.jpg\n",
      "Processing frame 48/65 (timestamp: 00m48s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991263.177809    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991263.181275  234163 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991263.181876    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991263.186785  234165 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.202247  234169 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991263.259268    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991263.262614  234179 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991263.284711  234183 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.308676  234187 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.966)\n",
      "Non-dominant hand detected: True (confidence: 0.993)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 49/65 (timestamp: 00m49s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991263.401322    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991263.404313  234195 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991263.404915    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991263.411097  234196 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.434992  234203 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991263.482371    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991263.485114  234211 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991263.504282  234218 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.520622  234213 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.970)\n",
      "Non-dominant hand detected: True (confidence: 0.994)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 50/65 (timestamp: 00m50s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991263.621126    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991263.623835  234227 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991263.624337    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991263.632106  234228 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.653635  234239 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991263.707010    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991263.709516  234243 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991263.728189  234244 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.741428  234253 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991263.842880    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991263.847251  234259 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991263.847609    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991263.852311  234263 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.875647  234260 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.962)\n",
      "Non-dominant hand detected: True (confidence: 0.996)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 51/65 (timestamp: 00m51s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991263.919945    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991263.923516  234275 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991263.946266  234285 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991263.964494  234277 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991264.026773    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.030002  234292 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.031188    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991264.035499  234295 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.052238  234299 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991264.086112    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.088963  234322 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.114488  234325 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.128470  234328 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.988)\n",
      "Non-dominant hand detected: True (confidence: 0.556)\n",
      "Face detected: True\n",
      "Total detection attempts: 3\n",
      "Processing frame 52/65 (timestamp: 00m52s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991264.185795    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.188305  234349 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.188813    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991264.194900  234350 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.217772  234357 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991264.262059    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.265437  234365 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.284766  234374 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.300751  234370 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991264.356476    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.359623  234381 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.360161    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991264.365362  234383 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.379120  234386 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991264.419241    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.422176  234397 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.447535  234398 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.463455  234402 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.909)\n",
      "Non-dominant hand detected: True (confidence: 0.983)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 53/65 (timestamp: 00m53s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991264.556159    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.559493  234413 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.559936    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991264.566220  234415 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.582926  234416 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991264.636693    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.640248  234432 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.666142  234434 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.682798  234441 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991264.786933    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.789224  234448 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.789768    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991264.795231  234451 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.818872  234456 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.980)\n",
      "Non-dominant hand detected: True (confidence: 0.992)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "Processing frame 54/65 (timestamp: 00m54s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991264.837523    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.840496  234464 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.872132  234468 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.891505  234473 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991264.953478    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991264.956974  234480 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991264.957759    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991264.967401  234487 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991264.982704  234486 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991265.041186    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991265.046089  234496 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991265.070226  234503 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991265.084523  234499 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991265.186709    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991265.190863  234512 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991265.191715    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991265.201151  234514 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991265.224611  234513 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991265.260035    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "New best detection: dominant hand=1, non-dominant hand=0, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.350, 0.350\n",
      "Lowering face thresholds: 0.350, 0.350\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991265.265042  234528 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991265.295395  234532 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991265.315023  234535 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991265.428801    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991265.433576  234544 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991265.434344    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991265.444528  234548 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991265.466440  234553 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991265.503833    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991265.507347  234560 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991265.532258  234566 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991265.547124  234561 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering non-dominant hand thresholds: 0.245, 0.245\n",
      "Lowering face thresholds: 0.245, 0.245\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: False\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.967)\n",
      "Non-dominant hand detected: False (confidence: 0.000)\n",
      "Face detected: False\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms_missing_NonDomHand_Face.jpg\n",
      "Processing frame 55/65 (timestamp: 00m55s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991265.627613    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991265.631934  234576 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991265.632503    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991265.636374  234577 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991265.651164  234585 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991265.700256    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991265.702425  234592 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991265.723497  234596 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991265.741660  234593 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.934)\n",
      "Non-dominant hand detected: True (confidence: 0.984)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 56/65 (timestamp: 00m56s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991265.841365    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991265.844477  234608 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991265.845135    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991265.850255  234611 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991265.873866  234615 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991265.924095    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991265.926269  234624 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991265.951737  234626 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991265.968514  234632 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991266.042126    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.046316  234640 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.047016    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991266.052321  234647 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.073726  234652 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.949)\n",
      "Non-dominant hand detected: True (confidence: 0.983)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n",
      "Processing frame 57/65 (timestamp: 00m57s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.955)\n",
      "Non-dominant hand detected: True (confidence: 0.972)\n",
      "Face detected: True\n",
      "Total detection attempts: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991266.133065    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.135590  234657 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.158160  234661 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.182683  234662 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991266.260218    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.263909  234698 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.264779    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991266.269290  234701 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.284710  234705 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991266.331865    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.334203  234714 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.355608  234716 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.372828  234720 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 58/65 (timestamp: 00m58s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=0\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering face thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991266.450671    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.453280  234730 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.453715    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991266.458392  234734 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.475747  234735 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991266.505295    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.509585  234746 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.534066  234747 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.553471  234750 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991266.631780    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.633739  234762 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.634198    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991266.639492  234765 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.656136  234768 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "--- Attempt 3/3 ---\n",
      "Lowering face thresholds: 0.245, 0.245\n",
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991266.684266    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.686652  234778 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.706310  234782 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.729787  234785 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991266.827936    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.830986  234794 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.831523    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991266.835484  234802 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.851044  234799 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991266.906391    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991266.909740  234810 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991266.929778  234815 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991266.943036  234813 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in detection. Continuing to next attempt.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.947)\n",
      "Non-dominant hand detected: True (confidence: 0.712)\n",
      "Face detected: False\n",
      "Total detection attempts: 3\n",
      "Saved failure screenshot: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000058_00m58s000ms_missing_Face.jpg\n",
      "Processing frame 59/65 (timestamp: 00m59s000ms)\n",
      "\n",
      "--- Attempt 1/3 ---\n",
      "Using original thresholds: hands=0.5, face=0.5\n",
      "Dominant hand (Right) detected: False\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=0, non-dominant hand=1, face=1\n",
      "\n",
      "--- Attempt 2/3 ---\n",
      "Lowering dominant hand thresholds: 0.350, 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742991267.023653    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991267.028099  234826 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991267.028776    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991267.033412  234833 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991267.052916  234829 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991267.083058    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991267.085729  234842 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991267.104948  234843 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991267.119590  234853 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1742991267.228336    6302 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1742991267.232283  234858 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.3~git2407250600.76ae27~oibaf~j (git-76ae27e 2024-07-25 jammy-oibaf-ppa)), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 15.0.7, DRM 3.42, 5.15.0-131-generic)\n",
      "W0000 00:00:1742991267.232637    6302 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1742991267.237420  234865 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742991267.254589  234859 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant hand (Right) detected: True\n",
      "Non-dominant hand detected: True\n",
      "New best detection: dominant hand=1, non-dominant hand=1, face=1\n",
      "All body parts detected. Stopping early.\n",
      "\n",
      "=== Detection Summary ===\n",
      "Dominant hand detected: True (confidence: 0.979)\n",
      "Non-dominant hand detected: True (confidence: 0.992)\n",
      "Face detected: True\n",
      "Total detection attempts: 2\n",
      "\n",
      "===== PROCESSING SUMMARY =====\n",
      "Processed 30 frames\n",
      "Detection rates: Dom hand: 100.0%, Non-dom hand: 86.7%, Face: 86.7%\n",
      "All parts detected in 0.0% of frames\n",
      "Full statistics saved to: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/detection_statistics.json\n",
      "Results saved to: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_video(video_path=video_path, adaptive_detect_func=adaptive_detect, hand_model_path=hand_model_path, face_model_path=face_model_path,\n",
    "                 min_hand_detection_confidence=0.5, min_hand_presence_confidence=0.5,\n",
    "                 min_face_detection_confidence=0.5, min_face_presence_confidence=0.5,\n",
    "                 num_hands=2, output_face_blendshapes=True,\n",
    "                 max_attempts=3, threshold_reduction_factor=0.7, min_threshold=0.2, \n",
    "                 frame_step=1, start_time_seconds=30.2, end_time_seconds=60.4,\n",
    "                 save_failure_screenshots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = load_frame_data(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[7][1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_interpolation_frames(x, nums_list):\n",
    "    \"\"\"\n",
    "    Returns integers in the range [x-5, x+5] that are not equal to x\n",
    "    and are not in nums_list.\n",
    "    \n",
    "    Args:\n",
    "        x (int): The reference integer\n",
    "        nums_list (list): A list of integers\n",
    "        \n",
    "    Returns:\n",
    "        list: Integers in [x-5, x+5] excluding x and elements in nums_list\n",
    "    \"\"\"\n",
    "    # Create the set of all integers in the range [x-5, x+5]\n",
    "    all_range = set(range(x-5, x+6))  # +6 because range is exclusive at upper bound\n",
    "    \n",
    "    # Remove x itself\n",
    "    all_range.discard(x)\n",
    "    \n",
    "    # Remove numbers that are in the input list\n",
    "    result = all_range - set(nums_list)\n",
    "    \n",
    "    # Convert back to a list and return\n",
    "    return sorted(list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "def find_file_with_partial_name(partial_name, search_dir='.', recursive=False):\n",
    "    \"\"\"\n",
    "    Find files that start with the given partial name.\n",
    "    \n",
    "    Args:\n",
    "        partial_name (str): Partial file name to match\n",
    "        search_dir (str): Directory to search in (default: current directory)\n",
    "        recursive (bool): Whether to search in subdirectories\n",
    "        \n",
    "    Returns:\n",
    "        list: Complete paths of all matching files\n",
    "    \"\"\"\n",
    "    # Create a search pattern for files starting with the partial name\n",
    "    search_pattern = os.path.join(search_dir, f\"{partial_name}*\")\n",
    "    \n",
    "    # Use recursive glob if requested\n",
    "    if recursive:\n",
    "        matches = []\n",
    "        for root, _, _ in os.walk(search_dir):\n",
    "            matches.extend(glob.glob(os.path.join(root, f\"{os.path.basename(partial_name)}*\")))\n",
    "        return matches\n",
    "    else:\n",
    "        return glob.glob(search_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers_on_both_sides(x, missing_numbers):\n",
    "    \"\"\"\n",
    "    Checks if the list of missing numbers has at least one number smaller than x\n",
    "    AND at least one number larger than x.\n",
    "    \n",
    "    Args:\n",
    "        x (int): The reference integer\n",
    "        missing_numbers (list): Output from find_missing_numbers(x, nums_list)\n",
    "        \n",
    "    Returns:\n",
    "        bool: False if all numbers are either all smaller or all larger than x.\n",
    "              True if there's at least one smaller and one larger number.\n",
    "    \"\"\"\n",
    "    has_smaller = False\n",
    "    has_larger = False\n",
    "    \n",
    "    for num in missing_numbers:\n",
    "        if num < x:\n",
    "            has_smaller = True\n",
    "        elif num > x:\n",
    "            has_larger = True\n",
    "            \n",
    "        # Early exit if we found both smaller and larger numbers\n",
    "        if has_smaller and has_larger:\n",
    "            return True\n",
    "    \n",
    "    # If we get here, we didn't find both smaller and larger numbers\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_npz_file(file_path, modifications):\n",
    "    \"\"\"\n",
    "    Load a .npz file, modify existing arrays and add new ones, then save it back.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .npz file\n",
    "        modifications (dict): Dictionary with keys as array names and values as new arrays\n",
    "                             or functions that take the original array and return a modified version\n",
    "    \"\"\"\n",
    "    # Load the npz file\n",
    "    with np.load(file_path) as data:\n",
    "        # Create a copy of all arrays\n",
    "        arrays = {name: data[name] for name in data.files}\n",
    "    \n",
    "    # Apply modifications and add new arrays\n",
    "    for name, modification in modifications.items():\n",
    "        if name in arrays:\n",
    "            if callable(modification):\n",
    "                # If the modification is a function, apply it to the original array\n",
    "                arrays[name] = modification(arrays[name])\n",
    "            else:\n",
    "                # Otherwise, replace the array\n",
    "                arrays[name] = modification\n",
    "        else:\n",
    "            # Add new array\n",
    "            arrays[name] = modification\n",
    "            print(f\"Adding new array '{name}' to the file\")\n",
    "    \n",
    "    # Save back to the file with same format\n",
    "    np.savez(file_path, **arrays)\n",
    "    \n",
    "    print(f\"Successfully modified/added {len(modifications)} arrays in {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_undetected_hand_landmarks(directory_path):  \n",
    "    \"\"\"\n",
    "    Interpolate landmarks for frames where hand detection failed.\n",
    "    \"\"\"\n",
    "    print(f\"Starting interpolation for directory: {directory_path}\")\n",
    "    \n",
    "    # Load detection statistics JSON\n",
    "    with open(os.path.join(directory_path, 'detection_statistics.json')) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    first_frame_number = round(data['video_info']['fps'] * data['video_info']['start_time'])\n",
    "    final_frame_number = round(data['video_info']['fps'] * data['video_info']['end_time'])\n",
    "    \n",
    "    print(f\"Processing frames range: {first_frame_number} to {final_frame_number}\")\n",
    "    \n",
    "    # Maximum possible sum of weights for normalization (when all 10 frames are available)\n",
    "    MAX_WEIGHT_SUM = 2.92722222\n",
    "    \n",
    "    # Process non-dominant hand failures\n",
    "    print(\"Processing non-dominant hand failures...\")\n",
    "    missing_non_dominant_frame_list = [frame['frame'] for frame in data['failed_frames']['non_dominant_hand_failures']]\n",
    "    \n",
    "    non_dom_interpolated_count = 0\n",
    "    \n",
    "    for missing_frame in data['failed_frames']['non_dominant_hand_failures']:\n",
    "        frame_number = missing_frame['frame']\n",
    "        filepath = missing_frame['file']\n",
    "        \n",
    "        # Only interpolate frames not at the edges of the video\n",
    "        if (frame_number - 5) <= first_frame_number or (frame_number + 5) >= final_frame_number:\n",
    "            print(f\"Skipping frame {frame_number} - too close to video boundary\")\n",
    "            continue\n",
    "        \n",
    "        # Find frames with valid detections for interpolation\n",
    "        interpolation_frames = find_interpolation_frames(frame_number, missing_non_dominant_frame_list)\n",
    "        \n",
    "        if not interpolation_frames:\n",
    "            print(f\"No valid frames found for interpolating frame {frame_number}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate interpolated landmarks\n",
    "        interpolation_weights_sum = 0\n",
    "        interpolated_coordinates = np.zeros(shape=(20, 3))\n",
    "        interpolated_wrist_to_nose = np.zeros(2)\n",
    "        \n",
    "        for interp_frame in interpolation_frames:\n",
    "            weight = 1 / ((frame_number - interp_frame) ** 2)\n",
    "            interpolation_weights_sum += weight\n",
    "            \n",
    "            # Find and load the reference frame\n",
    "            interp_partial_filename = data['video_info']['name'] + f\"_frame{interp_frame:06d}\"\n",
    "            try:\n",
    "                interp_files = find_file_with_partial_name(\n",
    "                    interp_partial_filename, \n",
    "                    search_dir=directory_path, \n",
    "                    recursive=False\n",
    "                )\n",
    "                \n",
    "                if not interp_files:\n",
    "                    print(f\"Warning: Could not find file for frame {interp_frame}\")\n",
    "                    continue\n",
    "                    \n",
    "                interp_filepath = interp_files[0]\n",
    "                \n",
    "                # Load the frame data - index 1 for non-dominant hand landmarks\n",
    "                frame_data = load_frame_data(interp_filepath)\n",
    "                non_dom_landmarks = frame_data[1]  # Correct index for non-dominant hand\n",
    "                nose_to_wrist_non_dom = frame_data[7][1, :]\n",
    "                \n",
    "                \n",
    "                # Add weighted contribution\n",
    "                interpolated_coordinates += weight * non_dom_landmarks\n",
    "                interpolated_wrist_to_nose += weight * nose_to_wrist_non_dom\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {interp_frame}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize by sum of weights (crucial step!)\n",
    "        if interpolation_weights_sum > 0:\n",
    "            interpolated_coordinates /= interpolation_weights_sum\n",
    "            interpolated_wrist_to_nose /= interpolation_weights_sum\n",
    "            \n",
    "            # Calculate confidence based on weights and frame distribution\n",
    "            has_frames_on_both_sides = has_numbers_on_both_sides(frame_number, interpolation_frames)\n",
    "            \n",
    "            if has_frames_on_both_sides:\n",
    "                interpolation_confidence = interpolation_weights_sum / MAX_WEIGHT_SUM\n",
    "            else:\n",
    "                interpolation_confidence = (interpolation_weights_sum / MAX_WEIGHT_SUM) * 0.8\n",
    "                \n",
    "            print(f\"Frame {frame_number}: Interpolated with confidence {interpolation_confidence:.2f}\")\n",
    "            \n",
    "            # Update the file with interpolated data\n",
    "            def update_interp_scores(arr):\n",
    "                new_arr = arr.copy()\n",
    "                new_arr[1] = interpolation_confidence  # Index 1 for non-dominant hand\n",
    "                return new_arr\n",
    "            \n",
    "            def update_nose_to_wrist_scores(matrix):\n",
    "                new_matrix = matrix.copy()\n",
    "                new_matrix[1, :] = interpolated_wrist_to_nose\n",
    "                return new_matrix\n",
    "                \n",
    "            modifications = {\n",
    "                'non_dom_landmarks': interpolated_coordinates,\n",
    "                'interpolation_scores': update_interp_scores,\n",
    "                'nose_to_wrist_dist': update_nose_to_wrist_scores\n",
    "            }\n",
    "            \n",
    "            modify_npz_file(\n",
    "                file_path=os.path.join(directory_path, filepath),\n",
    "                modifications=modifications\n",
    "            )\n",
    "            \n",
    "            non_dom_interpolated_count += 1\n",
    "    \n",
    "    # Process dominant hand failures\n",
    "    print(f\"Interpolated {non_dom_interpolated_count} non-dominant hand frames\")\n",
    "    print(\"Processing dominant hand failures...\")\n",
    "    \n",
    "    missing_dominant_frame_list = [frame['frame'] for frame in data['failed_frames']['dominant_hand_failures']]\n",
    "    \n",
    "    dom_interpolated_count = 0\n",
    "    \n",
    "    for missing_frame in data['failed_frames']['dominant_hand_failures']:\n",
    "        frame_number = missing_frame['frame']\n",
    "        filepath = missing_frame['file']\n",
    "        \n",
    "        # Only interpolate frames not at the edges of the video\n",
    "        if (frame_number - 5) <= first_frame_number or (frame_number + 5) >= final_frame_number:\n",
    "            continue\n",
    "        \n",
    "        # Find frames with valid detections for interpolation\n",
    "        interpolation_frames = find_interpolation_frames(frame_number, missing_dominant_frame_list)\n",
    "        \n",
    "        if not interpolation_frames:\n",
    "            continue\n",
    "        \n",
    "        # Calculate interpolated landmarks\n",
    "        interpolation_weights_sum = 0\n",
    "        interpolated_coordinates = np.zeros(shape=(20, 3))\n",
    "        interpolated_wrist_to_nose = np.zeros(2)\n",
    "        \n",
    "        for interp_frame in interpolation_frames:\n",
    "            weight = 1 / ((frame_number - interp_frame) ** 2)\n",
    "            interpolation_weights_sum += weight\n",
    "            \n",
    "            # Find and load the reference frame\n",
    "            interp_partial_filename = data['video_info']['name'] + f\"_frame{interp_frame:06d}\"\n",
    "            try:\n",
    "                interp_files = find_file_with_partial_name(\n",
    "                    interp_partial_filename, \n",
    "                    search_dir=directory_path, \n",
    "                    recursive=False\n",
    "                )\n",
    "                \n",
    "                if not interp_files:\n",
    "                    continue\n",
    "                    \n",
    "                interp_filepath = interp_files[0]\n",
    "                \n",
    "                # Load the frame data - index 0 for dominant hand landmarks\n",
    "                frame_data = load_frame_data(interp_filepath)\n",
    "                dom_landmarks = frame_data[0]  # Correct index for dominant hand\n",
    "                nose_to_wrist_dom = frame_data[7][0, :]\n",
    "                \n",
    "                # Add weighted contribution\n",
    "                interpolated_coordinates += weight * dom_landmarks\n",
    "                interpolated_wrist_to_nose += weight * nose_to_wrist_non_dom\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {interp_frame}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Normalize by sum of weights\n",
    "        if interpolation_weights_sum > 0:\n",
    "            interpolated_coordinates /= interpolation_weights_sum\n",
    "            interpolated_wrist_to_nose /= interpolation_weights_sum\n",
    "            # Calculate confidence based on weights and frame distribution\n",
    "            has_frames_on_both_sides = has_numbers_on_both_sides(frame_number, interpolation_frames)\n",
    "            \n",
    "            if has_frames_on_both_sides:\n",
    "                interpolation_confidence = interpolation_weights_sum / MAX_WEIGHT_SUM\n",
    "            else:\n",
    "                interpolation_confidence = (interpolation_weights_sum / MAX_WEIGHT_SUM) * 0.8\n",
    "            \n",
    "            # Update the file with interpolated data\n",
    "            def update_interp_scores(arr):\n",
    "                new_arr = arr.copy()\n",
    "                new_arr[0] = interpolation_confidence  # Index 0 for dominant hand\n",
    "                return new_arr\n",
    "            \n",
    "            def update_nose_to_wrist_scores(matrix):\n",
    "                new_matrix = matrix.copy()\n",
    "                new_matrix[0, :] = interpolated_wrist_to_nose\n",
    "                return new_matrix\n",
    "                \n",
    "            modifications = {\n",
    "                'dom_landmarks': interpolated_coordinates,\n",
    "                'interpolation_scores': update_interp_scores,\n",
    "                'nose_to_wrist_dist': update_nose_to_wrist_scores\n",
    "            }\n",
    "            \n",
    "\n",
    "            modify_npz_file(\n",
    "                file_path=os.path.join(directory_path, filepath),\n",
    "                modifications=modifications\n",
    "            )\n",
    "            \n",
    "            dom_interpolated_count += 1\n",
    "    \n",
    "    print(f\"Interpolated {dom_interpolated_count} dominant hand frames\")\n",
    "    print(f\"Total interpolated: {non_dom_interpolated_count + dom_interpolated_count} frames\")\n",
    "    \n",
    "    return non_dom_interpolated_count + dom_interpolated_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting interpolation for directory: youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\n",
      "Processing frames range: 30 to 60\n",
      "Processing non-dominant hand failures...\n",
      "Frame 36: Interpolated with confidence 0.94\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\n",
      "Frame 39: Interpolated with confidence 0.62\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\n",
      "Frame 40: Interpolated with confidence 0.64\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms.npz\n",
      "Frame 54: Interpolated with confidence 1.00\n",
      "Successfully modified/added 3 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms.npz\n",
      "Interpolated 4 non-dominant hand frames\n",
      "Processing dominant hand failures...\n",
      "Interpolated 0 dominant hand frames\n",
      "Total interpolated: 4 frames\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolate_undetected_hand_landmarks(directory_path=\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = load_frame_data(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores, detection_status, blendshape_scores, face_detected, nose_to_wrist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_valid_detection(frame_data, is_dominant_hand):\n",
    "    \"\"\"\n",
    "    Check if the frame has valid detection (not interpolated) for a specific hand.\n",
    "    \n",
    "    Args:\n",
    "        frame_data: The loaded frame data\n",
    "        is_dominant_hand: If True, check dominant hand; if False, check non-dominant hand;\n",
    "                         if None, check if either hand is detected\n",
    "    \n",
    "    Returns:\n",
    "        bool: Whether the specified hand(s) is/are detected\n",
    "    \"\"\"\n",
    "    detection_status = frame_data[4]\n",
    "    \n",
    "    if is_dominant_hand:\n",
    "        # Check specifically for dominant hand\n",
    "        return detection_status[0] == 1\n",
    "    else:\n",
    "        # Check specifically for non-dominant hand\n",
    "        return detection_status[1] == 1\n",
    "    \n",
    "\n",
    "\n",
    "def has_value(frame_data, is_dominant_hand):\n",
    "    \"\"\"Check if the frame exists and has any value (detection or interpolation)\"\"\"\n",
    "    detection_status = frame_data[4]\n",
    "    interpolation_scores = frame_data[3]\n",
    "    if is_dominant_hand:\n",
    "        return (detection_status[0]==1) or (interpolation_scores[0]>0)\n",
    "    else:\n",
    "        return (detection_status[1]==1) or (interpolation_scores[1]>0)\n",
    "    \n",
    "        \n",
    "\n",
    "def cartesian_to_spherical(velocities):\n",
    "    \"\"\"\n",
    "    Convert Cartesian velocities (ux, uy, uz) to spherical coordinate features.\n",
    "    \n",
    "    Args:\n",
    "        velocities: NumPy array of shape (20, 3) with Cartesian velocities\n",
    "        \n",
    "    Returns:\n",
    "        NumPy array of shape (20, 5) with spherical features:\n",
    "            [vmagnitude, sin, cos, sin, cos]\n",
    "    \"\"\"\n",
    "    num_landmarks = velocities.shape[0]\n",
    "    spherical_features = np.zeros((num_landmarks, 5))\n",
    "    \n",
    "    for i in range(num_landmarks):\n",
    "        ux, uy, uz = velocities[i]\n",
    "        \n",
    "        # Calculate velocity magnitude\n",
    "        vmagnitude = np.sqrt(ux**2 + uy**2 + uz**2)\n",
    "        spherical_features[i, 0] = vmagnitude\n",
    "        \n",
    "        # Handle edge cases to avoid division by zero\n",
    "        if vmagnitude == 0:\n",
    "            # If velocity is zero, set all angles to zero\n",
    "            spherical_features[i, 1:] = 0\n",
    "            continue\n",
    "        \n",
    "        # Calculate azimuth angle ()\n",
    "        phi = np.arctan2(uy, ux)\n",
    "        spherical_features[i, 1] = np.sin(phi)  # sin\n",
    "        spherical_features[i, 2] = np.cos(phi)  # cos\n",
    "        \n",
    "        # Calculate elevation angle ()\n",
    "        # Clamp uz/vmagnitude to range [-1, 1] to avoid numerical errors\n",
    "        cos_theta = np.clip(uz / vmagnitude, -1.0, 1.0)\n",
    "        theta = np.arccos(cos_theta)\n",
    "        spherical_features[i, 3] = np.sin(theta)  # sin\n",
    "        spherical_features[i, 4] = cos_theta      # cos (already calculated)\n",
    "    \n",
    "    return spherical_features\n",
    "\n",
    "\n",
    "\n",
    "def cartesian_to_polar_features(velocities):\n",
    "    \"\"\"\n",
    "    Convert Cartesian velocity coordinates to polar features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    velocities : numpy.ndarray\n",
    "        Array of shape (2, 2) where each row represents an object's [Ux, Uy]\n",
    "\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Array of shape (2, 3) with columns [magnitude, sin(direction), cos(direction)]\n",
    "    \"\"\"\n",
    "    # Calculate magnitude\n",
    "    magnitude = np.sqrt(np.sum(velocities**2, axis=1))\n",
    "    \n",
    "    # Initialize result array\n",
    "    result = np.zeros((velocities.shape[0], 3))\n",
    "    result[:, 0] = magnitude  # Set first column to magnitude\n",
    "    \n",
    "    # Create a mask for non-zero magnitudes\n",
    "    non_zero = magnitude > 0\n",
    "    \n",
    "    # For non-zero magnitudes, calculate direction components\n",
    "    if np.any(non_zero):\n",
    "        # Get direction for non-zero magnitudes\n",
    "        direction = np.arctan2(velocities[non_zero, 1], velocities[non_zero, 0])\n",
    "        \n",
    "        # Calculate sin and cos\n",
    "        result[non_zero, 1] = np.sin(direction)  # sin(direction)\n",
    "        result[non_zero, 2] = np.cos(direction)  # cos(direction)\n",
    "    \n",
    "    # Handle zero magnitudes \n",
    "    zero_indices = ~non_zero\n",
    "    if np.any(zero_indices):\n",
    "        result[zero_indices, 1] = 0.0\n",
    "        result[zero_indices, 2] = 0.0\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_landmark_velocities(directory_path):\n",
    "    \"\"\"\n",
    "    Compute velocity features for hand landmarks using central differencing with two window sizes,\n",
    "    and convert to spherical coordinates.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing frame NPZ files\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of frames processed\n",
    "    \"\"\"\n",
    "    # List all NPZ files in the directory\n",
    "    npz_files = sorted(glob.glob(os.path.join(directory_path, \"*.npz\")))\n",
    "    \n",
    "    # Skip if no files found\n",
    "    if not npz_files:\n",
    "        print(f\"No NPZ files found in {directory_path}\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Computing velocities for {len(npz_files)} files...\")\n",
    "    \n",
    "    # Create a mapping of frame indices to file paths\n",
    "    frame_to_file = {}\n",
    "    for file_path in npz_files:\n",
    "        frame_data = load_frame_data(file_path)\n",
    "        frame_idx = frame_data[8]  # Index for frame_idx\n",
    "        frame_to_file[frame_idx] = file_path\n",
    "    \n",
    "    frame_indices = sorted(frame_to_file.keys())\n",
    "    processed_count = 0\n",
    "    \n",
    "\n",
    "    min_frame = min(frame_indices)\n",
    "    max_frame = max(frame_indices)\n",
    "    safe_margin = 5  # Skip processing frames within 5 frames of the edge\n",
    "    \n",
    "    # Process each frame\n",
    "    for i, curr_idx in enumerate(frame_indices):\n",
    "        if curr_idx < min_frame + safe_margin or curr_idx > max_frame - safe_margin:\n",
    "            dom_velocity_small = np.zeros((20, 5))\n",
    "            dom_velocity_large = np.zeros((20, 5))\n",
    "            non_dom_velocity_small = np.zeros((20, 5))\n",
    "            non_dom_velocity_large = np.zeros((20, 5))\n",
    "            \n",
    "            wrist_velocity_small = np.zeros((2, 3))  \n",
    "            wrist_velocity_large = np.zeros((2, 3))\n",
    "            \n",
    "            # Create zero confidence arrays\n",
    "            velocity_confidence = np.zeros(2)\n",
    "            velocity_calculation_confidence = np.zeros(2)\n",
    "            \n",
    "            # Save these zero arrays\n",
    "            modifications = {\n",
    "                'dom_velocity_small': dom_velocity_small,\n",
    "                'dom_velocity_large': dom_velocity_large,\n",
    "                'non_dom_velocity_small': non_dom_velocity_small,\n",
    "                'non_dom_velocity_large': non_dom_velocity_large,\n",
    "                'velocity_confidence': velocity_confidence,\n",
    "                'velocity_calculation_confidence': velocity_calculation_confidence,\n",
    "                'wrist_velocity_small': wrist_velocity_small,\n",
    "                'wrist_velocity_large': wrist_velocity_large,\n",
    "            }\n",
    "            \n",
    "            # Get the file path for this frame\n",
    "            current_file_path = frame_to_file[curr_idx]\n",
    "            modify_npz_file(current_file_path, modifications)\n",
    "            processed_count += 1\n",
    "            \n",
    "            # Log that we're skipping calculation\n",
    "            print(f\"Frame {curr_idx} too close to video boundary - setting zero velocities\")\n",
    "            continue\n",
    "        # Load current frame\n",
    "        current_file_path = frame_to_file[curr_idx]\n",
    "        curr_frame_data = load_frame_data(current_file_path)\n",
    "        \n",
    "        # Store needed frames in a dictionary for easy access\n",
    "        frame_cache = {curr_idx: curr_frame_data}\n",
    "        \n",
    "        # Load all potentially needed frames in the -5 to +5 range\n",
    "        for offset in range(-5, 6):\n",
    "            if offset == 0:  # Skip current frame (already loaded)\n",
    "                continue\n",
    "            \n",
    "            check_idx = curr_idx + offset\n",
    "            if check_idx in frame_to_file:\n",
    "                frame_cache[check_idx] = load_frame_data(frame_to_file[check_idx])\n",
    "            else:\n",
    "                frame_cache[check_idx] = None  # Mark as not available\n",
    "        \n",
    "        # Extract dominant and non-dominant hand landmarks from current frame\n",
    "        dom_landmarks = curr_frame_data[0]\n",
    "        non_dom_landmarks = curr_frame_data[1]\n",
    "        \n",
    "        # Initialize velocity arrays in Cartesian coordinates\n",
    "        dom_velocity_small_cart = np.zeros_like(dom_landmarks)\n",
    "        dom_velocity_large_cart = np.zeros_like(dom_landmarks)\n",
    "        non_dom_velocity_small_cart = np.zeros_like(non_dom_landmarks)\n",
    "        non_dom_velocity_large_cart = np.zeros_like(non_dom_landmarks)\n",
    "        \n",
    "        wrist_velocity_small = np.zeros((2, 2))  # 2 hands  [x, y] coordinates\n",
    "        wrist_velocity_large = np.zeros((2, 2))  # 2 hands  [x, y] coordinates\n",
    "        \n",
    "        # Initialize confidence and method weight tracking\n",
    "        dom_small_conf = 0.0\n",
    "        dom_large_conf = 0.0\n",
    "        non_dom_small_conf = 0.0\n",
    "        non_dom_large_conf = 0.0\n",
    "        \n",
    "        dom_small_method_weight = 0.0\n",
    "        dom_large_method_weight = 0.0\n",
    "        non_dom_small_method_weight = 0.0\n",
    "        non_dom_large_method_weight = 0.0\n",
    "        \n",
    "        dom_small_source_quality = 0.0\n",
    "        dom_large_source_quality = 0.0\n",
    "        non_dom_small_source_quality = 0.0\n",
    "        non_dom_large_source_quality = 0.0\n",
    "        \n",
    "        # ===== DOMINANT HAND VELOCITY CALCULATION =====\n",
    "        \n",
    "        # Small window [-1, +1] velocity with fallbacks\n",
    "        if (curr_idx + 1 in frame_cache and frame_cache[curr_idx + 1] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 1], True) and \n",
    "            curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "            has_value(frame_cache[curr_idx - 1], True)):\n",
    "            # Ideal case: (t+1, t-1)\n",
    "            dom_velocity_small_cart = (frame_cache[curr_idx + 1][0] - frame_cache[curr_idx - 1][0]) / 2.0\n",
    "            wrist_velocity_small[0, :] = (frame_cache[curr_idx + 1][7][0, :] - frame_cache[curr_idx - 1][7][0, :]) / 2.0\n",
    "            dom_small_conf = min(frame_cache[curr_idx + 1][2][0], frame_cache[curr_idx - 1][2][0])  # Detection confidence of t+1 frame\n",
    "            dom_small_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor (average of interpolation confidences)\n",
    "            t_plus_1_interp = frame_cache[curr_idx + 1][3][0]\n",
    "            t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "            dom_small_source_quality = (t_plus_1_interp + t_minus_1_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "              is_valid_detection(frame_cache[curr_idx + 2], True) and \n",
    "              curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "              has_value(frame_cache[curr_idx - 2], True)):\n",
    "            # Fallback 1: (t+2, t-2)\n",
    "            dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - frame_cache[curr_idx - 2][0]) / 4.0\n",
    "            wrist_velocity_small[0, :] = (frame_cache[curr_idx + 2][7][0, :] - frame_cache[curr_idx - 2][7][0, :]) / 4.0\n",
    "            dom_small_conf = min(frame_cache[curr_idx + 2][2][0], frame_cache[curr_idx - 2][2][0])  # Detection confidence of t+2 frame\n",
    "            dom_small_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "            t_minus_2_interp = frame_cache[curr_idx - 2][3][0]\n",
    "            dom_small_source_quality = (t_plus_2_interp + t_minus_2_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "              is_valid_detection(frame_cache[curr_idx + 2], True)):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], True)):\n",
    "                # Fallback 2: (t+2, t-1)\n",
    "                dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - frame_cache[curr_idx - 1][0]) / 3.0\n",
    "                wrist_velocity_small[0, :] = (frame_cache[curr_idx + 2][7][0, :] - frame_cache[curr_idx - 1][7][0, :]) / 3.0\n",
    "                dom_small_conf = min(frame_cache[curr_idx + 2][2][0], frame_cache[curr_idx - 1][2][0])  # Detection confidence of t+2 frame\n",
    "                dom_small_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "                dom_small_source_quality = (t_plus_2_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif is_valid_detection(curr_frame_data, True):\n",
    "                # Fallback 3: (t+2, t)\n",
    "                dom_velocity_small_cart = (frame_cache[curr_idx + 2][0] - curr_frame_data[0]) / 2.0\n",
    "                wrist_velocity_small[0, :] = (frame_cache[curr_idx + 2][7][0, :] - curr_frame_data[7][0, :]) / 2.0\n",
    "                dom_small_conf = min(frame_cache[curr_idx + 2][2][0], curr_frame_data[2][0])\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][0]\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                dom_small_source_quality = (t_plus_2_interp + t_interp) / 2.0\n",
    "                \n",
    "        elif is_valid_detection(curr_frame_data, True):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], True)):\n",
    "                # Fallback 4: (t, t-1)\n",
    "                dom_velocity_small_cart = (curr_frame_data[0] - frame_cache[curr_idx - 1][0])\n",
    "                wrist_velocity_small[0, :] = (curr_frame_data[7][0, :] - frame_cache[curr_idx - 1][7][0, :]) \n",
    "                dom_small_conf = min(curr_frame_data[2][0], frame_cache[curr_idx - 1][2][0])  # Detection confidence of current frame\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][0]\n",
    "                dom_small_source_quality = (t_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "                  has_value(frame_cache[curr_idx - 2], True)):\n",
    "                # Fallback 5: (t, t-2)\n",
    "                dom_velocity_small_cart = (curr_frame_data[0] - frame_cache[curr_idx - 2][0]) / 2.0\n",
    "                wrist_velocity_small[0, :] = (curr_frame_data[7][0, :] - frame_cache[curr_idx - 2][7][0, :]) / 2.0\n",
    "                dom_small_conf = min(curr_frame_data[2][0], frame_cache[curr_idx - 2][2][0])  # Detection confidence of current frame\n",
    "                dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][0]\n",
    "                t_minus_2_interp = frame_cache[curr_idx - 2][3][0]\n",
    "                dom_small_source_quality = (t_interp + t_minus_2_interp) / 2.0\n",
    "        \n",
    "        # Large window [-5, +5] velocity with fallbacks\n",
    "\n",
    "        if (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], True) and \n",
    "            curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "            has_value(frame_cache[curr_idx - 5], True)):\n",
    "            # Ideal case: (t+5, t-5)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 5][0]) / 10.0\n",
    "            wrist_velocity_large[0, :] = (frame_cache[curr_idx + 5][7][0, :] - frame_cache[curr_idx - 5][7][0, :]) / 10.0\n",
    "            dom_large_conf = min(frame_cache[curr_idx + 5][2][0], frame_cache[curr_idx - 5][2][0])  # Detection confidence of t+5 frame\n",
    "            dom_large_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "            t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "            dom_large_source_quality = (t_plus_5_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "        is_valid_detection(frame_cache[curr_idx + 4], True) and \n",
    "        curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "        has_value(frame_cache[curr_idx - 4], True)):\n",
    "        # Fallback 1: (t+4, t-4)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 4][0]) / 8.0\n",
    "            wrist_velocity_large[0, :] = (frame_cache[curr_idx + 4][7][0, :] - frame_cache[curr_idx - 4][7][0, :]) / 8.0\n",
    "            dom_large_conf = min(frame_cache[curr_idx + 4][2][0], frame_cache[curr_idx - 4][2][0]) # Detection confidence of t+4 frame\n",
    "            dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "            dom_large_source_quality = (t_plus_4_interp + t_minus_4_interp) / 2.0\n",
    "    \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], True) and \n",
    "            curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "            has_value(frame_cache[curr_idx - 3], True)):\n",
    "            # Fallback 2: (t+3, t-3)\n",
    "            dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 3][0]) / 6.0\n",
    "            wrist_velocity_large[0, :] = (frame_cache[curr_idx + 3][7][0, :] - frame_cache[curr_idx - 3][7][0, :]) / 6.0\n",
    "            dom_large_conf = min(frame_cache[curr_idx + 3][2][0], frame_cache[curr_idx - 3][2][0])  # Detection confidence of t+3 frame\n",
    "            dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "            t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "            dom_large_source_quality = (t_plus_3_interp + t_minus_3_interp) / 2.0\n",
    "            \n",
    "        # Asymmetric fallbacks for large window\n",
    "        elif (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], True)):\n",
    "            if (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4], True)):\n",
    "                # Fallback 3: (t+5, t-4)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 4][0]) / 9.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 5][7][0, :] - frame_cache[curr_idx - 4][7][0, :]) / 9.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 5][2][0], frame_cache[curr_idx - 4][2][0])  # Detection confidence of t+5 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "                dom_large_source_quality = (t_plus_5_interp + t_minus_4_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], True)):\n",
    "                # Fallback 4: (t+5, t-3)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 5][0] - frame_cache[curr_idx - 3][0]) / 8.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 5][7][0, :] - frame_cache[curr_idx - 3][7][0, :]) / 8.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 5][2][0], frame_cache[curr_idx - 3][2][0])  # Detection confidence of t+5 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][0]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "                dom_large_source_quality = (t_plus_5_interp + t_minus_3_interp) / 2.0\n",
    "        \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], True)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], True)):\n",
    "                # Fallback 5: (t+4, t-5)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 5][0]) / 9.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 4][7][0, :] - frame_cache[curr_idx - 5][7][0, :]) / 9.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 4][2][0], frame_cache[curr_idx - 5][2][0])  # Detection confidence of t+4 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "                dom_large_source_quality = (t_plus_4_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], True)):\n",
    "                # Fallback 6: (t+4, t-3)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 4][0] - frame_cache[curr_idx - 3][0]) / 7.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 4][7][0, :] - frame_cache[curr_idx - 3][7][0, :]) / 7.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 4][2][0], frame_cache[curr_idx - 3][2][0])  # Detection confidence of t+4 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][0]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][0]\n",
    "                dom_large_source_quality = (t_plus_4_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], True)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], True)):\n",
    "                # Fallback 7: (t+3, t-5)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 5][0]) / 8.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 3][7][0, :] - frame_cache[curr_idx - 5][7][0, :]) / 8.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 3][2][0], frame_cache[curr_idx - 5][2][0])  # Detection confidence of t+3 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][0]\n",
    "                dom_large_source_quality = (t_plus_3_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4], True)):\n",
    "                # Fallback 8: (t+3, t-4)\n",
    "                dom_velocity_large_cart = (frame_cache[curr_idx + 3][0] - frame_cache[curr_idx - 4][0]) / 7.0\n",
    "                wrist_velocity_large[0, :] = (frame_cache[curr_idx + 3][7][0, :] - frame_cache[curr_idx - 4][7][0, :]) / 7.0\n",
    "                dom_large_conf = min(frame_cache[curr_idx + 3][2][0], frame_cache[curr_idx - 4][2][0])  # Detection confidence of t+3 frame\n",
    "                dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][0]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][0]\n",
    "                dom_large_source_quality = (t_plus_3_interp + t_minus_4_interp) / 2.0\n",
    "\n",
    "        # ===== NON-DOMINANT HAND VELOCITY CALCULATION =====\n",
    "\n",
    "    # Small window [-1, +1] velocity with fallbacks for non-dominant hand\n",
    "        if (curr_idx + 1 in frame_cache and frame_cache[curr_idx + 1] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 1], False) and \n",
    "            curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "            has_value(frame_cache[curr_idx - 1], False)):\n",
    "            # Ideal case: (t+1, t-1)\n",
    "            non_dom_velocity_small_cart = (frame_cache[curr_idx + 1][1] - frame_cache[curr_idx - 1][1]) / 2.0\n",
    "            wrist_velocity_small[1, :] = (frame_cache[curr_idx + 1][7][1, :] - frame_cache[curr_idx - 1][7][1, :]) / 2.0\n",
    "            non_dom_small_conf = min(frame_cache[curr_idx + 1][2][1], frame_cache[curr_idx - 1][2][1])  # Detection confidence of t+1 frame\n",
    "            non_dom_small_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_1_interp = frame_cache[curr_idx + 1][3][1]\n",
    "            t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "            non_dom_small_source_quality = (t_plus_1_interp + t_minus_1_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 2], False) and \n",
    "            curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "            has_value(frame_cache[curr_idx - 2], False)):\n",
    "            # Fallback 1: (t+2, t-2)\n",
    "            non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - frame_cache[curr_idx - 2][1]) / 4.0\n",
    "            wrist_velocity_small[1, :] = (frame_cache[curr_idx + 2][7][1, :] - frame_cache[curr_idx - 2][7][1, :]) / 4.0\n",
    "            non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], frame_cache[curr_idx - 2][2][1]) \n",
    "            non_dom_small_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "            t_minus_2_interp = frame_cache[curr_idx - 2][3][1]\n",
    "            non_dom_small_source_quality = (t_plus_2_interp + t_minus_2_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 2 in frame_cache and frame_cache[curr_idx + 2] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 2], False)):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], False)):\n",
    "                # Fallback 2: (t+2, t-1)\n",
    "                non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - frame_cache[curr_idx - 1][1]) / 3.0\n",
    "                wrist_velocity_small[1, :] = (frame_cache[curr_idx + 2][7][1, :] - frame_cache[curr_idx - 2][7][1, :]) / 3.0\n",
    "                non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], frame_cache[curr_idx - 1][2][1])  \n",
    "                non_dom_small_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "                non_dom_small_source_quality = (t_plus_2_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif is_valid_detection(curr_frame_data, False):\n",
    "                # Fallback 3: (t+2, t)\n",
    "                non_dom_velocity_small_cart = (frame_cache[curr_idx + 2][1] - curr_frame_data[1]) / 2.0\n",
    "                wrist_velocity_small[1, :] = (frame_cache[curr_idx + 2][7][1, :] - curr_frame_data[7][1, :]) / 2.0\n",
    "                non_dom_small_conf = min(frame_cache[curr_idx + 2][2][1], curr_frame_data[2][1])\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_plus_2_interp = frame_cache[curr_idx + 2][3][1]\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                non_dom_small_source_quality = (t_plus_2_interp + t_interp) / 2.0\n",
    "                \n",
    "        elif is_valid_detection(curr_frame_data, False):\n",
    "            if (curr_idx - 1 in frame_cache and frame_cache[curr_idx - 1] is not None and \n",
    "                has_value(frame_cache[curr_idx - 1], False)):\n",
    "                # Fallback 4: (t, t-1)\n",
    "                non_dom_velocity_small_cart = (curr_frame_data[1] - frame_cache[curr_idx - 1][1])\n",
    "                wrist_velocity_small[1, :] = (curr_frame_data[7][1, :] - frame_cache[curr_idx - 1][7][1, :]) \n",
    "                non_dom_small_conf = min(curr_frame_data[2][1], frame_cache[curr_idx - 1][2][1])  # Detection confidence of current frame\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                t_minus_1_interp = frame_cache[curr_idx - 1][3][1]\n",
    "                non_dom_small_source_quality = (t_interp + t_minus_1_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 2 in frame_cache and frame_cache[curr_idx - 2] is not None and \n",
    "                has_value(frame_cache[curr_idx - 2], False)):\n",
    "                # Fallback 5: (t, t-2)\n",
    "                non_dom_velocity_small_cart = (curr_frame_data[1] - frame_cache[curr_idx - 2][1]) / 2.0\n",
    "                wrist_velocity_small[1, :] = (curr_frame_data[7][1, :] - frame_cache[curr_idx -21][7][1, :]) / 2.0\n",
    "                non_dom_small_conf = min(curr_frame_data[2][1], frame_cache[curr_idx -2][2][1])  # Detection confidence of current frame\n",
    "                non_dom_small_method_weight = 0.4  # One-sided derivative\n",
    "                # Calculate source quality factor\n",
    "                t_interp = curr_frame_data[3][1]\n",
    "                t_minus_2_interp = frame_cache[curr_idx - 2][3][1]\n",
    "                non_dom_small_source_quality = (t_interp + t_minus_2_interp) / 2.0\n",
    "    \n",
    "        # Large window [-5, +5] velocity with fallbacks for non-dominant hand\n",
    "        if (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], False) and \n",
    "            curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "            has_value(frame_cache[curr_idx - 5], False)):\n",
    "            # Ideal case: (t+5, t-5)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 5][1]) / 10.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 5][7][1, :] - frame_cache[curr_idx - 5][7][1, :]) / 10.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 5][2][1], frame_cache[curr_idx - 5][2][1])  \n",
    "            non_dom_large_method_weight = 1.0  # Ideal frames\n",
    "            # Calculate source quality factor\n",
    "            t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "            t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_5_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], False) and \n",
    "            curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "            has_value(frame_cache[curr_idx - 4], False)):\n",
    "            # Fallback 1: (t+4, t-4)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 4][1]) / 8.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 4][7][1, :] - frame_cache[curr_idx - 4][7][1, :]) / 8.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 4][2][1], frame_cache[curr_idx - 4][2][1])  \n",
    "            non_dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_4_interp + t_minus_4_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], False) and \n",
    "            curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "            has_value(frame_cache[curr_idx - 3], False)):\n",
    "            # Fallback 2: (t+3, t-3)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 3][1]) / 6.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 3][7][1, :] - frame_cache[curr_idx - 3][7][1, :]) / 6.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 3][2][1], frame_cache[curr_idx - 3][2][1])  # Detection confidence of t+3 frame\n",
    "            non_dom_large_method_weight = 0.8  # Wider symmetric window\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "            t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_3_interp + t_minus_3_interp) / 2.0\n",
    "            \n",
    "        # Asymmetric fallbacks for large window\n",
    "        elif (curr_idx + 5 in frame_cache and frame_cache[curr_idx + 5] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 5], False)):\n",
    "            if (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "                has_value(frame_cache[curr_idx - 4], False)):\n",
    "                # Fallback 3: (t+5, t-4)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 4][1]) / 9.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 5][7][1, :] - frame_cache[curr_idx - 4][7][1, :]) / 9.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 5][2][1], frame_cache[curr_idx - 4][2][1])  # Detection confidence of t+5 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "                t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_5_interp + t_minus_4_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], False)):\n",
    "                # Fallback 4: (t+5, t-3)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 5][1] - frame_cache[curr_idx - 3][1]) / 8.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 5][7][1, :] - frame_cache[curr_idx - 3][7][1, :]) / 8.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 5][2][1], frame_cache[curr_idx - 3][2][1])  # Detection confidence of t+5 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_5_interp = frame_cache[curr_idx + 5][3][1]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_5_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 4 in frame_cache and frame_cache[curr_idx + 4] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 4], False)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], False)):\n",
    "                # Fallback 5: (t+4, t-5)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 5][1]) / 9.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 4][7][1, :] - frame_cache[curr_idx - 5][7][1, :]) / 9.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 4][2][1], frame_cache[curr_idx - 5][2][1])  # Detection confidence of t+4 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_4_interp + t_minus_5_interp) / 2.0\n",
    "                \n",
    "            elif (curr_idx - 3 in frame_cache and frame_cache[curr_idx - 3] is not None and \n",
    "                has_value(frame_cache[curr_idx - 3], False)):\n",
    "                # Fallback 6: (t+4, t-3)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 4][1] - frame_cache[curr_idx - 3][1]) / 7.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 4][7][1, :] - frame_cache[curr_idx - 3][7][1, :]) / 7.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 4][2][1], frame_cache[curr_idx - 3][2][1])  # Detection confidence of t+4 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_4_interp = frame_cache[curr_idx + 4][3][1]\n",
    "                t_minus_3_interp = frame_cache[curr_idx - 3][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_4_interp + t_minus_3_interp) / 2.0\n",
    "                \n",
    "        elif (curr_idx + 3 in frame_cache and frame_cache[curr_idx + 3] is not None and \n",
    "            is_valid_detection(frame_cache[curr_idx + 3], False)):\n",
    "            if (curr_idx - 5 in frame_cache and frame_cache[curr_idx - 5] is not None and \n",
    "                has_value(frame_cache[curr_idx - 5], False)):\n",
    "                # Fallback 7: (t+3, t-5)\n",
    "                non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 5][1]) / 8.0\n",
    "                wrist_velocity_large[1, :] = (frame_cache[curr_idx + 3][7][1, :] - frame_cache[curr_idx - 5][7][1, :]) / 8.0\n",
    "                non_dom_large_conf = min(frame_cache[curr_idx + 3][2][1], frame_cache[curr_idx - 5][2][1])  # Detection confidence of t+3 frame\n",
    "                non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "                # Calculate source quality factor\n",
    "                t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "                t_minus_5_interp = frame_cache[curr_idx - 5][3][1]\n",
    "                non_dom_large_source_quality = (t_plus_3_interp + t_minus_5_interp) / 2.0\n",
    "            \n",
    "        elif (curr_idx - 4 in frame_cache and frame_cache[curr_idx - 4] is not None and \n",
    "            has_value(frame_cache[curr_idx - 4], False)):\n",
    "            # Fallback 8: (t+3, t-4)\n",
    "            non_dom_velocity_large_cart = (frame_cache[curr_idx + 3][1] - frame_cache[curr_idx - 4][1]) / 7.0\n",
    "            wrist_velocity_large[1, :] = (frame_cache[curr_idx + 3][7][1, :] - frame_cache[curr_idx - 4][7][1, :]) / 7.0\n",
    "            non_dom_large_conf = min(frame_cache[curr_idx + 3][2][1], frame_cache[curr_idx - 4][2][1])  # Detection confidence of t+3 frame\n",
    "            non_dom_large_method_weight = 0.6  # Asymmetric window maintaining center point\n",
    "            # Calculate source quality factor\n",
    "            t_plus_3_interp = frame_cache[curr_idx + 3][3][1]\n",
    "            t_minus_4_interp = frame_cache[curr_idx - 4][3][1]\n",
    "            non_dom_large_source_quality = (t_plus_3_interp + t_minus_4_interp) / 2.0\n",
    "            \n",
    "\n",
    "        \n",
    "        # Convert Cartesian velocities to spherical/polar coordinates\n",
    "        dom_velocity_small = cartesian_to_spherical(dom_velocity_small_cart)\n",
    "        dom_velocity_large = cartesian_to_spherical(dom_velocity_large_cart)\n",
    "        non_dom_velocity_small = cartesian_to_spherical(non_dom_velocity_small_cart)\n",
    "        non_dom_velocity_large = cartesian_to_spherical(non_dom_velocity_large_cart)\n",
    "        \n",
    "        wrist_velocity_small_polar = cartesian_to_polar_features(wrist_velocity_small)\n",
    "        wrist_velocity_large_polar = cartesian_to_polar_features(wrist_velocity_large)\n",
    "        \n",
    "        # Calculate average confidence for each hand across both windows\n",
    "        dom_avg_conf = (dom_small_conf + dom_large_conf) / 2.0\n",
    "        non_dom_avg_conf = (non_dom_small_conf + non_dom_large_conf) / 2.0\n",
    "        \n",
    "        # Calculate velocityCalculationConfidence using method weight and source quality\n",
    "        dom_small_vel_calc_conf = dom_small_method_weight * dom_small_source_quality\n",
    "        dom_large_vel_calc_conf = dom_large_method_weight * dom_large_source_quality\n",
    "        non_dom_small_vel_calc_conf = non_dom_small_method_weight * non_dom_small_source_quality\n",
    "        non_dom_large_vel_calc_conf = non_dom_large_method_weight * non_dom_large_source_quality\n",
    "        \n",
    "        # Average across windows for each hand\n",
    "        dom_vel_calc_conf = (dom_small_vel_calc_conf + dom_large_vel_calc_conf) / 2.0\n",
    "        non_dom_vel_calc_conf = (non_dom_small_vel_calc_conf + non_dom_large_vel_calc_conf) / 2.0\n",
    "        \n",
    "        # Prepare arrays\n",
    "        velocity_confidence = np.array([dom_avg_conf, non_dom_avg_conf])\n",
    "        velocity_calculation_confidence = np.array([dom_vel_calc_conf, non_dom_vel_calc_conf])\n",
    "        \n",
    "        # Save back to the NPZ file\n",
    "        modifications = {\n",
    "            'dom_velocity_small': dom_velocity_small,\n",
    "            'dom_velocity_large': dom_velocity_large,\n",
    "            'non_dom_velocity_small': non_dom_velocity_small,\n",
    "            'non_dom_velocity_large': non_dom_velocity_large,\n",
    "            'velocity_confidence': velocity_confidence,\n",
    "            'velocity_calculation_confidence': velocity_calculation_confidence,\n",
    "            'wrist_velocity_small': wrist_velocity_small_polar,\n",
    "            'wrist_velocity_large': wrist_velocity_large_polar,\n",
    "            \n",
    "        }\n",
    "        \n",
    "        modify_npz_file(current_file_path, modifications)\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 100 == 0 or i == len(frame_indices) - 1:\n",
    "            print(f\"Processed {i+1}/{len(frame_indices)} frames\")\n",
    "    \n",
    "    print(f\"Velocity computation complete. Processed {processed_count} frames.\")\n",
    "    return processed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing velocities for 30 files...\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000030_00m30s000ms.npz\n",
      "Frame 30 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000031_00m31s000ms.npz\n",
      "Frame 31 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000032_00m32s000ms.npz\n",
      "Frame 32 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000033_00m33s000ms.npz\n",
      "Frame 33 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000034_00m34s000ms.npz\n",
      "Frame 34 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000035_00m35s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000037_00m37s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000038_00m38s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000039_00m39s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000040_00m40s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000041_00m41s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000042_00m42s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000043_00m43s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000044_00m44s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000045_00m45s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000046_00m46s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000047_00m47s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000048_00m48s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000049_00m49s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000050_00m50s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000051_00m51s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000052_00m52s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000053_00m53s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000054_00m54s000ms.npz\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000055_00m55s000ms.npz\n",
      "Frame 55 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000056_00m56s000ms.npz\n",
      "Frame 56 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000057_00m57s000ms.npz\n",
      "Frame 57 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000058_00m58s000ms.npz\n",
      "Frame 58 too close to video boundary - setting zero velocities\n",
      "Adding new array 'dom_velocity_small' to the file\n",
      "Adding new array 'dom_velocity_large' to the file\n",
      "Adding new array 'non_dom_velocity_small' to the file\n",
      "Adding new array 'non_dom_velocity_large' to the file\n",
      "Adding new array 'velocity_confidence' to the file\n",
      "Adding new array 'velocity_calculation_confidence' to the file\n",
      "Adding new array 'wrist_velocity_small' to the file\n",
      "Adding new array 'wrist_velocity_large' to the file\n",
      "Successfully modified/added 8 arrays in youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000059_00m59s000ms.npz\n",
      "Frame 59 too close to video boundary - setting zero velocities\n",
      "Velocity computation complete. Processed 30 frames.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_landmark_velocities(directory_path=\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_data_with_velocities(npz_path):\n",
    "    \"\"\"\n",
    "    Load saved frame data from an NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        npz_path (str): Path to the saved .npz file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: All the detection results for the frame\n",
    "    \"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    # Extract all arrays from the npz file\n",
    "    dom_landmarks = data['dom_landmarks']\n",
    "    non_dom_landmarks = data['non_dom_landmarks']\n",
    "    confidence_scores = data['confidence_scores']\n",
    "    interpolation_scores = data['interpolation_scores']\n",
    "    detection_status = data['detection_status']\n",
    "    blendshape_scores = data['blendshape_scores']\n",
    "    face_detected = data['face_detected'].item()  # Convert 0-d array to scalar\n",
    "    nose_to_wrist_dist = data['nose_to_wrist_dist']\n",
    "    frame_idx = data['frame_idx'].item()\n",
    "    timestamp_ms = data['timestamp_ms'].item()\n",
    "    dom_velocity_small = data['dom_velocity_small']\n",
    "    dom_velocity_large = data['dom_velocity_large']\n",
    "    non_dom_velocity_small = data['non_dom_velocity_small']\n",
    "    non_dom_velocity_large = data['non_dom_velocity_large']\n",
    "    velocity_confidence = data['velocity_confidence']\n",
    "    velocity_calculation_confidence = data['velocity_calculation_confidence']\n",
    "    nose_to_wrist_velocity_small = data['wrist_velocity_small']\n",
    "    nose_to_wrist_velocity_large = data['wrist_velocity_large']\n",
    "    \n",
    "    return (dom_landmarks, non_dom_landmarks, confidence_scores, interpolation_scores,\n",
    "            detection_status, blendshape_scores, face_detected, \n",
    "            nose_to_wrist_dist, frame_idx, timestamp_ms, dom_velocity_small, dom_velocity_large, non_dom_velocity_small, non_dom_velocity_large, velocity_confidence, velocity_calculation_confidence, nose_to_wrist_velocity_small, nose_to_wrist_velocity_large)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_30 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000030_00m30s000ms.npz\")\n",
    "frame_37 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000037_00m37s000ms.npz\")\n",
    "frame_42 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000042_00m42s000ms.npz\")\n",
    "frame_32 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000032_00m32s000ms.npz\")\n",
    "frame_36 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000036_00m36s000ms.npz\")\n",
    "frame_38 = load_frame_data_with_velocities(\"youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_landmarks/youtube_DNViaspA8hM_1920x1080_h264_fps10_fps1_Right_frame000038_00m38s000ms.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do, add velocity features of wrists too!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
